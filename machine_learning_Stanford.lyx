#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning (Stanford)
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 10/5/2015 - 12/27/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Subsection*
Supervised learning
\end_layout

\begin_layout Itemize
Say you have data which gives the price and square footage of several houses.
\end_layout

\begin_layout Itemize
You want to predict the price given some square footage as an input.
\end_layout

\begin_layout Itemize
You can fit any type of function to the data: linear, quadratic, etc.
\end_layout

\begin_layout Itemize
Supervised learning: using a dataset where the 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 answers are known.
\end_layout

\begin_layout Itemize
Regression problem: predict a continuous valued output.
\end_layout

\begin_layout Itemize
Classification problem: predict for a problem with a discrete output.
\end_layout

\begin_deeper
\begin_layout Itemize
Also want to estimate the probability associated with the prediction.
\end_layout

\end_deeper
\begin_layout Itemize
Most interesting machine learning algorithms can deal with an infinite number
 of features!
\end_layout

\begin_deeper
\begin_layout Itemize
Requires a support vector machine - we will talk about this later in the
 course.
\end_layout

\end_deeper
\begin_layout Subsection*
Unsupervised learning
\end_layout

\begin_layout Itemize
We are given data where we don't know the 
\begin_inset Quotes eld
\end_inset

right answer.
\begin_inset Quotes erd
\end_inset

 The actual data is not classified as true or false.
\end_layout

\begin_layout Itemize
The question is: here is the dataset, can you find some structure in the
 data?
\end_layout

\begin_layout Itemize
Example: clustering algorithm.
 Used in Google news to group similar stories together.
\end_layout

\begin_deeper
\begin_layout Itemize
In this method, the algorithm assigns group labels to different clusters.
\end_layout

\end_deeper
\begin_layout Itemize
Other examples: social network analysis, organization of computer clusters,
 market segmentation, astronomical data analysis.
\end_layout

\begin_layout Itemize
Example: cocktail party problem - useful for separating highly correlated
 audio tracks into independent tracks.
\end_layout

\begin_deeper
\begin_layout Itemize
Cocktail party problem algorithm: 
\family typewriter
[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
\family default
 (singular value decomposition)
\end_layout

\end_deeper
\begin_layout Section*
Linear regression with one variable
\end_layout

\begin_layout Subsection*
Model and cost function
\end_layout

\begin_layout Itemize
Regression problem: predict a (continuous output).
\end_layout

\begin_layout Itemize
General notation for this course:
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
m
\emph default
: number of examples in the training dataset.
\end_layout

\begin_layout Itemize

\emph on
x
\emph default
: input variables/features.
\end_layout

\begin_layout Itemize

\emph on
y
\emph default
: output variable/target.
\end_layout

\begin_layout Itemize

\emph on
(x,y)
\emph default
: denotes a single training example.
\end_layout

\end_deeper
\begin_layout Itemize
General flow:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set -> learning algorithm -> hypothesis function.
\end_layout

\begin_layout Itemize
The hypothesis function takes input (
\emph on
x
\emph default
) and predicts the outcome (
\emph on
y
\emph default
).
 It maps from 
\emph on
x
\emph default
's to 
\emph on
y
\emph default
's.
\end_layout

\end_deeper
\begin_layout Itemize
How do we represent the hypothesis 
\emph on
h
\emph default
?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$
\end_inset

 for univariate linear regression, or linear regression with one variable.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\theta$
\end_inset

's are the parameters of the model.
\end_layout

\end_deeper
\begin_layout Itemize
How do we choose the parameters of the model?
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 such that we minimize the sum of the squared errors: 
\begin_inset Formula $\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Factor of 
\begin_inset Formula $\frac{1}{2m}$
\end_inset

 doesn't affect parameter values and will make later math a bit easier.
\end_layout

\begin_layout Itemize
This also defined as the cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
There are other cost functions that may work as well, but the squared error
 cost function is the most commonly used one for linear regression.
\end_layout

\end_deeper
\begin_layout Itemize
Can be useful to plot cost function in terms of the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
If we assume an intercept of 0 (
\begin_inset Formula $\theta_{0}=0$
\end_inset

), the value of 
\begin_inset Formula $\theta_{1}$
\end_inset

 that minimizes the cost function is 
\begin_inset Formula $\theta_{1}=\frac{\sum_{i}x^{\left(i\right)}y^{\left(i\right)}}{\sum_{i}x^{2\left(i\right)}}$
\end_inset


\end_layout

\begin_layout Itemize
For two parameters,
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{1}=\frac{\sum_{i=1}^{m}x^{\left(i\right)}y^{\left(i\right)}-\bar{x}\bar{y}}{\sum_{i=1}^{m}x^{\left(i\right)2}-\bar{x}^{2}}$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{0}=\bar{y}-\theta_{1}\bar{x}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Gradient descent
\end_layout

\begin_layout Itemize
Have some cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 that we want to minimize (find 
\begin_inset Formula $\underset{\theta_{0,}\theta_{1}}{\text{min}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Outline:
\end_layout

\begin_deeper
\begin_layout Itemize
Start with some 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
Keep changing 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

 to reduce 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 until we hopefully end up at a minimum.
\end_layout

\end_deeper
\begin_layout Itemize
Definition of gradient descent algorithm: 
\family typewriter
repeat until convergence { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 }.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $:=$
\end_inset

: assignment operator.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

: learning rate.
 Controls step size.
\end_layout

\end_deeper
\begin_layout Itemize
When actually programming this, make sure you change the parameters at the
 same time and then update them at the same time! Otherwise you may incorrectly
 use new values of the parameters to calculate the cost function.
\end_layout

\end_deeper
\begin_layout Itemize
Don't need to adjust 
\begin_inset Formula $\alpha$
\end_inset

 over time because the derivative term will get smaller as it approaches
 a local minimum.
\end_layout

\begin_layout Itemize
Applying gradient descent to our simple cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)=\frac{\partial}{\partial\theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}\left(\theta_{0}+\theta_{1}x^{\left(i\right)}-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=0$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{0}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=1$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{1}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Batch gradient descent: used in machine learning, each step of gradient
 descent uses all of the training examples.
\end_layout

\begin_layout Section*
Linear regression with multiple variables
\end_layout

\begin_layout Itemize
Also called multivariate linear regression.
\end_layout

\begin_layout Itemize
We now have multiple features (or predictors) that we want to use to develop
 a hypothesis function and make a prediction.
\end_layout

\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

: number of features.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: input (features) of 
\begin_inset Formula $i$
\end_inset

th training example.
 This is a vector of features in the 
\begin_inset Formula $i$
\end_inset

th observation.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{j}^{\left(i\right)}$
\end_inset

: value of feature 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $i$
\end_inset

th training example.
\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis function now has a new form:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$
\end_inset


\end_layout

\begin_layout Itemize
For convenience of notation, we can define 
\begin_inset Formula $x_{0}^{\left(i\right)}=1$
\end_inset

 so that 
\begin_inset Formula $h_{\theta}\left(x\right)=\sum_{i=1}^{n}\theta_{i}x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
We can also use linear algebra:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x=\left[x_{0},x_{1},...,x_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{0},\theta_{1},...,\theta_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta x^{T}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Gradient descent with multiple variables
\end_layout

\begin_layout Itemize
Write set of parameters as 
\begin_inset Formula $\theta$
\end_inset

 and cost function as 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Gradient descent: 
\family typewriter
repeat { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

 } 
\family default
(simultaneously update for each 
\begin_inset Formula $j=0,1,...,n$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Tips for gradient descent:
\end_layout

\begin_deeper
\begin_layout Itemize
Feature scaling - make sure that different features taken on similar ranges
 of values.
\end_layout

\begin_deeper
\begin_layout Itemize
Ideally, all features will be in the range 
\begin_inset Formula $-1\leq x\leq1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Mean normalization: normalize a feature to have a mean of zero.
 When doing this, you just replace a feature 
\begin_inset Formula $x_{i}$
\end_inset

 with 
\begin_inset Formula $x_{i}-\mu_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Do not apply this to 
\begin_inset Formula $x_{0}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Can also normalize by standard deviation: 
\begin_inset Formula $x_{i}\rightarrow\frac{x_{i}-\mu_{i}}{s_{i}}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Making sure that gradient descent is working properly:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot 
\begin_inset Formula $\underset{\theta}{\text{min}}J\left(\theta\right)$
\end_inset

 vs.
 number of iterations.
 It should decrease after every iteration for sufficiently small 
\begin_inset Formula $\alpha$
\end_inset

 (can be shown mathematically).
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too large, you may continue overshooting the minimum and never be able
 to actually reach it.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too small, convergence will happen, but very slowly.
\end_layout

\end_deeper
\begin_layout Itemize
Automatic convergence tests: algorithms that tell you whether gradient descent
 has converged:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: declare convergence if 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 decreases by less than some value (like 
\begin_inset Formula $10^{-3}$
\end_inset

) in one iteration.
\end_layout

\end_deeper
\begin_layout Subsection*
Polynomial regression
\end_layout

\begin_layout Itemize
Can create new features from existing ones.
\end_layout

\begin_deeper
\begin_layout Itemize
Example: area = frontage * depth.
\end_layout

\end_deeper
\begin_layout Itemize
Polynomial model: 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$
\end_inset

 for a quadratic function.
\end_layout

\begin_deeper
\begin_layout Itemize
Important to use feature scaling since when you square or cube features,
 the range becomes much larger.
\end_layout

\end_deeper
\begin_layout Subsection*
Normal equation
\end_layout

\begin_layout Itemize
Method to solve for 
\begin_inset Formula $\theta$
\end_inset

 analytically.
\end_layout

\begin_layout Itemize
How to go about it:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)=...=0$
\end_inset

, then solve for every 
\begin_inset Formula $\theta_{j}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Linear algebra method:
\end_layout

\begin_deeper
\begin_layout Itemize
Put all features into a matrix: 
\begin_inset Formula $X=\begin{array}{ccccc}
1 & 2104 & 5 & 1 & 45\\
1 & 1416 & 3 & 2 & 40\\
1 & 1534 & 3 & 2 & 30\\
1 & 852 & 2 & 1 & 36
\end{array}$
\end_inset

 (including a column of ones for 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\begin_layout Itemize
Put all 
\begin_inset Formula $y_{j}$
\end_inset

s into a vector: 
\begin_inset Formula $Y=\begin{array}{c}
460\\
232\\
315\\
178
\end{array}$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
In Matlab/Octave: 
\family typewriter
pinv(X'*X)*X'*Y
\end_layout

\begin_layout Itemize
Feature scaling is not needed for the normal equation method.
\end_layout

\end_deeper
\begin_layout Itemize
Normal equation vs.
 gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Pros:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: no need to choose 
\begin_inset Formula $\alpha$
\end_inset

, no need to iterate.
\end_layout

\begin_layout Itemize
Gradient descent: works well even when the number of features is large.
\end_layout

\end_deeper
\begin_layout Itemize
Cons:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: slow if number of features is very large, need to compute
 
\begin_inset Formula $\left(X^{T}X\right)^{-1}$
\end_inset


\end_layout

\begin_layout Itemize
Gradient descent: need to choose 
\begin_inset Formula $\alpha$
\end_inset

, needs many iterations.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation non-invertibility: what if 
\begin_inset Formula $\left(X^{T}X\right)$
\end_inset

 is non-invertible? Also known as 
\begin_inset Quotes eld
\end_inset

singular
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

degenerate.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Can occur when there are redundant features (linearly dependent) or if there
 are too many features.
\end_layout

\begin_deeper
\begin_layout Itemize
Possible solution: delete some features or use regularization.
\end_layout

\end_deeper
\begin_layout Itemize
If you use 
\family typewriter
pinv
\family default
 in Matlab/Octave instead of 
\family typewriter
inv
\family default
, it should handle singular cases for you.
\end_layout

\end_deeper
\begin_layout Section*
Logistic Regression
\end_layout

\begin_layout Subsection*
Classification and Representation
\end_layout

\begin_layout Itemize
Classification: putting things into discrete categories.
 Binary classification is when there are only two categories.
\end_layout

\begin_layout Itemize
Linear regression is not usually a very good solution for classification
 problems.
 Examples where we can clearly use some sort of threshold for classification
 make this clear.
\end_layout

\begin_layout Itemize
Logistic regression has the property that the predictions of the hypothesis
 function are always between 0 and 1: 
\begin_inset Formula $0\leq h_{\theta}\left(x\right)\leq1$
\end_inset

.
\end_layout

\begin_layout Itemize
For linear regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta^{T}x$
\end_inset

.
\end_layout

\begin_layout Itemize
For logistic regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta^{T}x\right)$
\end_inset

, where 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is the sigmoid or logistic function, 
\begin_inset Formula $g\left(z\right)=\frac{1}{1+e^{-z}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Thus, 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+\exp\left(-\theta^{T}x\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
As before, we need to fit the parameters 
\begin_inset Formula $\theta$
\end_inset

 to our data.
\end_layout

\end_deeper
\begin_layout Itemize
Interpretation of hypothesis output:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

: estimated probability that 
\begin_inset Formula $y=1$
\end_inset

 (i.e., positive result) on input 
\begin_inset Formula $x$
\end_inset

; i.e.
 
\begin_inset Formula $h_{\theta}\left(x\right)=p\left(y=1|x;\theta\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Decision boundary:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we predict that 
\begin_inset Formula $y=1$
\end_inset

 when 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 and 
\begin_inset Formula $y=0$
\end_inset

 when 
\begin_inset Formula $h_{\theta}<0.5$
\end_inset

.
\end_layout

\begin_layout Itemize
We see that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is 
\begin_inset Formula $\geq0.5$
\end_inset

 when 
\begin_inset Formula $z\geq0$
\end_inset

, thus 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 whenever 
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

.
\end_layout

\begin_layout Itemize
This essentially defines a threshold for making predictions with our model.
\end_layout

\begin_layout Itemize
The decision boundary is a property of the hypothesis and its parameters,
 not a property of the dataset.
\end_layout

\end_deeper
\begin_layout Itemize
Non-linear decision boundaries:
\end_layout

\begin_deeper
\begin_layout Itemize
Can use higher-order polynomials terms in logistic regression.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{2}^{2}+\theta_{5}x_{1}x_{2}+\theta_{6}x_{1}^{3}+...\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Logistic Regression Model
\end_layout

\begin_layout Itemize
Cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
Linear regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}=\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Logistic regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What cost function here should we use here?
\end_layout

\begin_layout Itemize
We can't use the same thing that we used for linear regression because it
 will be a non-convex function - thus, gradient descent will not necessarily
 converge to the global minimum.
\end_layout

\begin_layout Itemize
Instead, we use 
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=\left\{ \begin{array}{l}
-\log\left(h_{\theta}\left(x\right)\right)\text{ if }y=1\\
-\log\left(1-h_{\theta}\left(x\right)\right)\text{ if }y=0
\end{array}\right.$
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=1$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow0$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=0$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=0$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow1$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Simplified way of writing the cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=-y\log\left(h_{\theta}\left(x\right)\right)-\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)$
\end_inset


\end_layout

\begin_layout Itemize
Note: 
\begin_inset Formula $y$
\end_inset

 is always equal to 0 or 1.
\end_layout

\begin_layout Itemize
So the full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent: we do the same thing as before, we just have a new cost
 function.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{j}$
\end_inset

 := 
\begin_inset Formula $\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
This is the same as for linear regression, although our hypothesis function
 has changed.
\end_layout

\begin_layout Itemize
Make sure to simultaneously update all 
\begin_inset Formula $\theta_{j}$
\end_inset

!
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization:
\end_layout

\begin_deeper
\begin_layout Itemize
We have some cost function 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and we want the set of parameters
\begin_inset Formula $\theta$
\end_inset

 that gives the minimum of 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $\theta$
\end_inset

, we have code that can compute 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Optimization algorithms:
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descent
\end_layout

\begin_layout Itemize
Conjugate gradient
\end_layout

\begin_layout Itemize
BFGS
\end_layout

\begin_layout Itemize
L-BFGS
\end_layout

\end_deeper
\begin_layout Itemize
Advantages: no need to manually pick the learning rate 
\begin_inset Formula $\alpha$
\end_inset

, often faster than gradient descent.
\end_layout

\begin_layout Itemize
Disadvantages: more complex, can cause implementations to be error prone.
\end_layout

\begin_layout Itemize
Can use 
\family typewriter
fminunc
\family default
 in Matlab/Octave.
\end_layout

\begin_deeper
\begin_layout Itemize
Define a function which returns the cost and gradient.
\end_layout

\begin_layout Itemize
Example:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% function
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

jVal = (theta(1)-5)^2 + (theta(2)-5)^2;
\end_layout

\begin_layout Plain Layout

gradient(1) = 2*(theta(1)-5);
\end_layout

\begin_layout Plain Layout

gradient(2) = 2*(theta(2)-5);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Setup
\end_layout

\begin_layout Plain Layout

options = optimset('GradObj','on','MaxIter','100');
\end_layout

\begin_layout Plain Layout

initialTheta = zeros(2,1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Call fminunc
\end_layout

\begin_layout Plain Layout

[optTheta, functionVal, exitFlag] = ...
\end_layout

\begin_layout Plain Layout

	fminunc(@costFunction, initialTheta, options);
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Multi-class classification
\end_layout

\begin_layout Itemize
Examples of this type of problem:
\end_layout

\begin_deeper
\begin_layout Itemize
Email tagging into different folders.
\end_layout

\begin_layout Itemize
Medical diagnosis.
\end_layout

\begin_layout Itemize
Weather category.
\end_layout

\end_deeper
\begin_layout Itemize
One-vs-all classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: a training dataset with three classes.
\end_layout

\begin_layout Itemize
First: is it in class 1, or class 2/3?
\end_layout

\begin_layout Itemize
Second: is it in class 2, or class 1/3?
\end_layout

\begin_layout Itemize
Third: is it in class 3, or class 1/2?
\end_layout

\begin_layout Itemize
Use standard logistic regression for each step.
\end_layout

\begin_layout Itemize
Each gives a hypothesis: 
\begin_inset Formula $h_{\theta}^{\left(i\right)}\left(x\right)=P\left(y=i|x;\theta\right)$
\end_inset

, 
\begin_inset Formula $\left(i=1,2,3\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The hypothesis tells us the probability that 
\begin_inset Formula $y$
\end_inset

 is in class 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
To make a prediction on a new input 
\begin_inset Formula $x$
\end_inset

, pick the class 
\begin_inset Formula $i$
\end_inset

 that has the maximum probability.
\end_layout

\end_deeper
\begin_layout Section*
Regularization and overfitting
\end_layout

\begin_layout Itemize
Overfitting: similar number of parameters to data points.
 This allows the model to fit the training dataset very well, however, it
 will not make accurate predictions for new examples.
 Also called 
\begin_inset Quotes eld
\end_inset

high variance.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Occurs when you have a lot of features and not a lot of training data.
\end_layout

\begin_layout Itemize
Underfitting may occur when our model has too few parameters to properly
 model the data.
 In this situation, the model has 
\begin_inset Quotes eld
\end_inset

high bias.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Options for dealing with overfitting:
\end_layout

\begin_deeper
\begin_layout Itemize
Reduce number of features: can manually select which features to keep, or
 use a model selection algorithm to determine this (will be covered later
 in the course).
\end_layout

\begin_layout Itemize
Regularization: keep all of the features, but reduce the magnitude/values
 of the parameters 
\begin_inset Formula $\theta_{j}$
\end_inset

.
 Works well when we have a lot of features, each of which contributes a
 little bit to predicting 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection*
Regularization
\end_layout

\begin_layout Itemize
Suppose we penalize and make 
\begin_inset Formula $\theta_{3},\theta_{4}$
\end_inset

 very small.
 We do this by adjusting our cost function by adding in terms like 
\begin_inset Formula $1000\theta_{3}^{2}+1000\theta_{4}^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This will give values of 
\begin_inset Formula $\theta_{3}$
\end_inset

 and 
\begin_inset Formula $\theta_{4}$
\end_inset

 which are close to zero when the cost function is small.
\end_layout

\end_deeper
\begin_layout Itemize
General idea behind regularization:
\end_layout

\begin_deeper
\begin_layout Itemize
Small values for parameters 
\begin_inset Formula $\theta_{0},\theta_{1},...,\theta_{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Simpler hypothesis, less prone to overfitting.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example: housing price prediction.
\end_layout

\begin_deeper
\begin_layout Itemize
Features: 
\begin_inset Formula $x_{1},x_{2},...,x_{100}$
\end_inset


\end_layout

\begin_layout Itemize
Parameters: 
\begin_inset Formula $\theta_{0},\theta_{1},\theta_{2},...,\theta_{100}$
\end_inset


\end_layout

\begin_layout Itemize
We don't know which parameters to try to shrink 
\emph on
a priori
\emph default
.
\end_layout

\begin_layout Itemize
Take our cost function 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

 and add a new term to shrink 
\emph on
all
\emph default
 of our parameters except 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}+\lambda\sum_{j=1}^{n}\theta_{j}^{2}\right]$
\end_inset


\end_layout

\begin_layout Itemize
The first part in brackets captures the goal of fitting the data well.
\end_layout

\begin_layout Itemize
The second part captures the goal of keeping the parameters 
\begin_inset Formula $\theta$
\end_inset

 small.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 is the regularization parameter and controls the trade-off between these
 two goals.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\lambda$
\end_inset

 is too large, we w
\end_layout

\begin_layout Itemize
on't even fit the training dataset well.
 All of the parameters 
\begin_inset Formula $\theta$
\end_inset

 will go to zero except 
\begin_inset Formula $\theta_{0}$
\end_inset

, so we will be fitting a flat line to our data.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Regularized linear regression
\end_layout

\begin_layout Itemize
Recall that we don't want to shrink 
\begin_inset Formula $\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The term 
\begin_inset Formula $1-\alpha\frac{\lambda}{m}$
\end_inset

 is usually slightly less than 1 (assuming that your learning rate is small).
 This term shrinks your 
\begin_inset Formula $\theta_{j}$
\end_inset

, then the other term is just the original gradient descent term.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation
\end_layout

\begin_deeper
\begin_layout Itemize
Before, we had 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
With our new, regularized cost function, you can derive the minimum of the
 cost function by taking derivatives with respect to each parameter, as
 before.
\end_layout

\begin_layout Itemize
Now, we get 
\begin_inset Formula $\theta=\left(X^{T}X+\lambda Z\right)^{-1}X^{T}Y$
\end_inset

, where 
\begin_inset Formula $Z$
\end_inset

 is the identity matrix, except the top left entry is 0 instead of 1 (because
 we don't regularize 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Non-invertibility
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have fewer examples than features (
\begin_inset Formula $m\leq n$
\end_inset

).
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

 will give a strange answer since 
\begin_inset Formula $X^{T}X$
\end_inset

 is non-invertible/singular (make sure to use 
\family typewriter
pinv
\family default
 instead of 
\family typewriter
inv
\family default
).
\end_layout

\begin_layout Itemize
However, if 
\begin_inset Formula $\lambda>0$
\end_inset

 , we can prove that the matrix 
\begin_inset Formula $X^{T}X+\lambda Z$
\end_inset

 is invertible!
\end_layout

\end_deeper
\begin_layout Subsection*
Regularized logistic regression
\end_layout

\begin_layout Itemize
To modify our cost function to use regularization, we add a term 
\begin_inset Formula $\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
The full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\left[\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Recall that although this looks the same as for regularized linear regression,
 the hypothesis function is different for logistic regression.
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization methods:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% Define a function to compute the cost.
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

	jVal = %code to compute J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(1) = %code to compute d/dtheta_0 J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(2) = %code to compute d/dtheta_1 J(theta);
\end_layout

\begin_layout Plain Layout

	...
\end_layout

\begin_layout Plain Layout

	gradient(n+1) = %code to compute d/dtheta_n J(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% then, we pass this function to fminunc as before.
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Neural networks: representation
\end_layout

\begin_layout Subsection*
Motivation
\end_layout

\begin_layout Itemize
Used for solving complex, non-linear hypotheses.
\end_layout

\begin_layout Itemize
For many predictors, including even quadratic terms can require a huge number
 of terms (grows roughly as 
\begin_inset Formula $n^{2}$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
One option is to include only a subset of higher-order terms, but it will
 probably underfit the data.
\end_layout

\end_deeper
\begin_layout Itemize
Neural networks: algorithms that try to mimic the brain.
\end_layout

\begin_layout Itemize
Recent resurgence: state-of-the-art technique for many computational application
s.
\end_layout

\begin_layout Itemize
Computationally more expensive.
\end_layout

\begin_layout Subsection*
Neural networks
\end_layout

\begin_layout Itemize
Developed as simulating networks of neurons in the brain.
\end_layout

\begin_layout Itemize
Neuron structure:
\end_layout

\begin_deeper
\begin_layout Itemize
Dendrites: 
\begin_inset Quotes eld
\end_inset

input wires,
\begin_inset Quotes erd
\end_inset

 receive inputs from other locations.
\end_layout

\begin_layout Itemize
Axon: 
\begin_inset Quotes eld
\end_inset

output wire,
\begin_inset Quotes erd
\end_inset

 used to send signals to other neurons.
\end_layout

\end_deeper
\begin_layout Itemize
Neuron model: logistic unit.
\end_layout

\begin_deeper
\begin_layout Itemize
We use a model of what a neuron does: it takes some number of inputs (
\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset

), does some computation, and sends an output 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^{T}x}}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This is a sigmoid (logistic) activation function.
\end_layout

\begin_layout Itemize
The parameters 
\begin_inset Formula $\theta$
\end_inset

 are sometimes referred to as 
\begin_inset Quotes eld
\end_inset

weights.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x_{0}$
\end_inset

 is sometimes referred to as the 
\begin_inset Quotes eld
\end_inset

bias unit.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Neural network: divide into layers.
\end_layout

\begin_deeper
\begin_layout Itemize
Layer 1: 
\begin_inset Quotes eld
\end_inset

input layer,
\begin_inset Quotes erd
\end_inset

 made up of input features.
\end_layout

\begin_layout Itemize
Layer 2: some set of neurons, called 
\begin_inset Quotes eld
\end_inset

hidden layers.
\begin_inset Quotes erd
\end_inset

 Can be several hidden layers.
\end_layout

\begin_layout Itemize
Final layer: 
\begin_inset Quotes eld
\end_inset

output layer,
\begin_inset Quotes erd
\end_inset

 outputs final value computed by a hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $a_{i}^{\left(j\right)}$
\end_inset

: 
\begin_inset Quotes eld
\end_inset

activation
\begin_inset Quotes erd
\end_inset

 of unit 
\begin_inset Formula $i$
\end_inset

 in layer 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

: matrix of weights controlling function mapping from layer 
\begin_inset Formula $j$
\end_inset

 to layer 
\begin_inset Formula $j+1$
\end_inset

.
\end_layout

\begin_layout Itemize
If a network has 
\begin_inset Formula $s_{j}$
\end_inset

 units in layer 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $s_{j+1}$
\end_inset

units in layer 
\begin_inset Formula $j+1$
\end_inset

, then 
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

 will be of dimension 
\begin_inset Formula $s_{j+1}\times\left(s_{j}+1\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $a_{1}^{\left(2\right)}=g\left(\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Can write 
\begin_inset Formula $z_{1}^{\left(2\right)}=\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}$
\end_inset

; i.e.
 a row of 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Vectorized implementation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $z^{\left(2\right)}=\Theta^{\left(1\right)}x=\Theta^{\left(1\right)}a^{\left(1\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(2\right)}=g\left(z^{\left(2\right)}\right)$
\end_inset

 (here 
\begin_inset Formula $z^{\left(2\right)}$
\end_inset

 is a column vector of 
\begin_inset Formula $z_{1}^{\left(2\right)},...,z_{3}^{\left(2\right)}$
\end_inset

.
 Note that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is applied element-wise.
\end_layout

\begin_layout Itemize
Add 
\begin_inset Formula $a_{0}^{\left(2\right)}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $z^{\left(3\right)}=\Theta^{\left(2\right)}a^{\left(2\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\Theta}\left(x\right)=a^{\left(3\right)}=g\left(z^{\left(3\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
We can think of the way a neural network works as though it is 
\begin_inset Quotes eld
\end_inset

learning its own features.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The function mapping 
\begin_inset Formula $a^{\left(1\right)}$
\end_inset

 to 
\begin_inset Formula $a^{\left(2\right)}$
\end_inset

 is what changes the input features to new features which go into the hidden
 layers.
\end_layout

\end_deeper
\begin_layout Itemize
You can have neural networks with other types of layouts.
\end_layout

\begin_deeper
\begin_layout Itemize
The layout is referred to as the 
\begin_inset Quotes eld
\end_inset

architechture.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Applications
\end_layout

\begin_layout Itemize
Can set up a neural network which acts as a logical filter (AND, OR, XOR,
 NOT, etc.).
\end_layout

\begin_layout Itemize
Interesting because sometimes these functions (XOR, for example) require
 non-linear decision boundaries.
\end_layout

\begin_layout Itemize
Multi-class classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Build a neural network with as many outputs as there are classes.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 is a vector, and we take the largest entry to be the predicted class.
\end_layout

\end_deeper
\end_body
\end_document
