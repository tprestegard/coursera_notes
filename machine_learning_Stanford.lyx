#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning (Stanford)
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 10/5/2015 - 12/27/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Subsection*
Supervised learning
\end_layout

\begin_layout Itemize
Say you have data which gives the price and square footage of several houses.
\end_layout

\begin_layout Itemize
You want to predict the price given some square footage as an input.
\end_layout

\begin_layout Itemize
You can fit any type of function to the data: linear, quadratic, etc.
\end_layout

\begin_layout Itemize
Supervised learning: using a dataset where the 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 answers are known.
\end_layout

\begin_layout Itemize
Regression problem: predict a continuous valued output.
\end_layout

\begin_layout Itemize
Classification problem: predict for a problem with a discrete output.
\end_layout

\begin_deeper
\begin_layout Itemize
Also want to estimate the probability associated with the prediction.
\end_layout

\end_deeper
\begin_layout Itemize
Most interesting machine learning algorithms can deal with an infinite number
 of features!
\end_layout

\begin_deeper
\begin_layout Itemize
Requires a support vector machine - we will talk about this later in the
 course.
\end_layout

\end_deeper
\begin_layout Subsection*
Unsupervised learning
\end_layout

\begin_layout Itemize
We are given data where we don't know the 
\begin_inset Quotes eld
\end_inset

right answer.
\begin_inset Quotes erd
\end_inset

 The actual data is not classified as true or false.
\end_layout

\begin_layout Itemize
The question is: here is the dataset, can you find some structure in the
 data?
\end_layout

\begin_layout Itemize
Example: clustering algorithm.
 Used in Google news to group similar stories together.
\end_layout

\begin_deeper
\begin_layout Itemize
In this method, the algorithm assigns group labels to different clusters.
\end_layout

\end_deeper
\begin_layout Itemize
Other examples: social network analysis, organization of computer clusters,
 market segmentation, astronomical data analysis.
\end_layout

\begin_layout Itemize
Example: cocktail party problem - useful for separating highly correlated
 audio tracks into independent tracks.
\end_layout

\begin_deeper
\begin_layout Itemize
Cocktail party problem algorithm: 
\family typewriter
[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
\family default
 (singular value decomposition)
\end_layout

\end_deeper
\begin_layout Section*
Linear regression with one variable
\end_layout

\begin_layout Subsection*
Model and cost function
\end_layout

\begin_layout Itemize
Regression problem: predict a (continuous output).
\end_layout

\begin_layout Itemize
General notation for this course:
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
m
\emph default
: number of examples in the training dataset.
\end_layout

\begin_layout Itemize

\emph on
x
\emph default
: input variables/features.
\end_layout

\begin_layout Itemize

\emph on
y
\emph default
: output variable/target.
\end_layout

\begin_layout Itemize

\emph on
(x,y)
\emph default
: denotes a single training example.
\end_layout

\end_deeper
\begin_layout Itemize
General flow:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set -> learning algorithm -> hypothesis function.
\end_layout

\begin_layout Itemize
The hypothesis function takes input (
\emph on
x
\emph default
) and predicts the outcome (
\emph on
y
\emph default
).
 It maps from 
\emph on
x
\emph default
's to 
\emph on
y
\emph default
's.
\end_layout

\end_deeper
\begin_layout Itemize
How do we represent the hypothesis 
\emph on
h
\emph default
?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$
\end_inset

 for univariate linear regression, or linear regression with one variable.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\theta$
\end_inset

's are the parameters of the model.
\end_layout

\end_deeper
\begin_layout Itemize
How do we choose the parameters of the model?
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 such that we minimize the sum of the squared errors: 
\begin_inset Formula $\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Factor of 
\begin_inset Formula $\frac{1}{2m}$
\end_inset

 doesn't affect parameter values and will make later math a bit easier.
\end_layout

\begin_layout Itemize
This also defined as the cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
There are other cost functions that may work as well, but the squared error
 cost function is the most commonly used one for linear regression.
\end_layout

\end_deeper
\begin_layout Itemize
Can be useful to plot cost function in terms of the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
If we assume an intercept of 0 (
\begin_inset Formula $\theta_{0}=0$
\end_inset

), the value of 
\begin_inset Formula $\theta_{1}$
\end_inset

 that minimizes the cost function is 
\begin_inset Formula $\theta_{1}=\frac{\sum_{i}x^{\left(i\right)}y^{\left(i\right)}}{\sum_{i}x^{2\left(i\right)}}$
\end_inset


\end_layout

\begin_layout Itemize
For two parameters,
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{1}=\frac{\sum_{i=1}^{m}x^{\left(i\right)}y^{\left(i\right)}-\bar{x}\bar{y}}{\sum_{i=1}^{m}x^{\left(i\right)2}-\bar{x}^{2}}$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{0}=\bar{y}-\theta_{1}\bar{x}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Gradient descent
\end_layout

\begin_layout Itemize
Have some cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 that we want to minimize (find 
\begin_inset Formula $\underset{\theta_{0,}\theta_{1}}{\text{min}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Outline:
\end_layout

\begin_deeper
\begin_layout Itemize
Start with some 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
Keep changing 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

 to reduce 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 until we hopefully end up at a minimum.
\end_layout

\end_deeper
\begin_layout Itemize
Definition of gradient descent algorithm: 
\family typewriter
repeat until convergence { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 }.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $:=$
\end_inset

: assignment operator.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

: learning rate.
 Controls step size.
\end_layout

\end_deeper
\begin_layout Itemize
When actually programming this, make sure you change the parameters at the
 same time and then update them at the same time! Otherwise you may incorrectly
 use new values of the parameters to calculate the cost function.
\end_layout

\end_deeper
\begin_layout Itemize
Don't need to adjust 
\begin_inset Formula $\alpha$
\end_inset

 over time because the derivative term will get smaller as it approaches
 a local minimum.
\end_layout

\begin_layout Itemize
Applying gradient descent to our simple cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)=\frac{\partial}{\partial\theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}\left(\theta_{0}+\theta_{1}x^{\left(i\right)}-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=0$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{0}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=1$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{1}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Batch gradient descent: used in machine learning, each step of gradient
 descent uses all of the training examples.
\end_layout

\begin_layout Subsection*
Linear regression with multiple variables
\end_layout

\begin_layout Itemize
Also called multivariate linear regression.
\end_layout

\begin_layout Itemize
We now have multiple features (or predictors) that we want to use to develop
 a hypothesis function and make a prediction.
\end_layout

\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

: number of features.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: input (features) of 
\begin_inset Formula $i$
\end_inset

th training example.
 This is a vector of features in the 
\begin_inset Formula $i$
\end_inset

th observation.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{j}^{\left(i\right)}$
\end_inset

: value of feature 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $i$
\end_inset

th training example.
\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis function now has a new form:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$
\end_inset


\end_layout

\begin_layout Itemize
For convenience of notation, we can define 
\begin_inset Formula $x_{0}^{\left(i\right)}=1$
\end_inset

 so that 
\begin_inset Formula $h_{\theta}\left(x\right)=\sum_{i=1}^{n}\theta_{i}x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
We can also use linear algebra:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x=\left[x_{0},x_{1},...,x_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{0},\theta_{1},...,\theta_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta x^{T}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Gradient descent with multiple variables
\end_layout

\begin_layout Itemize
Write set of parameters as 
\begin_inset Formula $\theta$
\end_inset

 and cost function as 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Gradient descent: 
\family typewriter
repeat { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

 } 
\family default
(simultaneously update for each 
\begin_inset Formula $j=0,1,...,n$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Tips for gradient descent:
\end_layout

\begin_deeper
\begin_layout Itemize
Feature scaling - make sure that different features taken on similar ranges
 of values.
\end_layout

\begin_deeper
\begin_layout Itemize
Ideally, all features will be in the range 
\begin_inset Formula $-1\leq x\leq1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Mean normalization: normalize a feature to have a mean of zero.
 When doing this, you just replace a feature 
\begin_inset Formula $x_{i}$
\end_inset

 with 
\begin_inset Formula $x_{i}-\mu_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Do not apply this to 
\begin_inset Formula $x_{0}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Can also normalize by standard deviation: 
\begin_inset Formula $x_{i}\rightarrow\frac{x_{i}-\mu_{i}}{s_{i}}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Making sure that gradient descent is working properly:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot 
\begin_inset Formula $\underset{\theta}{\text{min}}J\left(\theta\right)$
\end_inset

 vs.
 number of iterations.
 It should decrease after every iteration for sufficiently small 
\begin_inset Formula $\alpha$
\end_inset

 (can be shown mathematically).
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too large, you may continue overshooting the minimum and never be able
 to actually reach it.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too small, convergence will happen, but very slowly.
\end_layout

\end_deeper
\begin_layout Itemize
Automatic convergence tests: algorithms that tell you whether gradient descent
 has converged:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: declare convergence if 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 decreases by less than some value (like 
\begin_inset Formula $10^{-3}$
\end_inset

) in one iteration.
\end_layout

\end_deeper
\end_body
\end_document
