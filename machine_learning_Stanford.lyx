#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning (Stanford)
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 10/5/2015 - 12/27/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Subsection*
Supervised learning
\end_layout

\begin_layout Itemize
Say you have data which gives the price and square footage of several houses.
\end_layout

\begin_layout Itemize
You want to predict the price given some square footage as an input.
\end_layout

\begin_layout Itemize
You can fit any type of function to the data: linear, quadratic, etc.
\end_layout

\begin_layout Itemize
Supervised learning: using a dataset where the 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 answers are known.
\end_layout

\begin_layout Itemize
Regression problem: predict a continuous valued output.
\end_layout

\begin_layout Itemize
Classification problem: predict for a problem with a discrete output.
\end_layout

\begin_deeper
\begin_layout Itemize
Also want to estimate the probability associated with the prediction.
\end_layout

\end_deeper
\begin_layout Itemize
Most interesting machine learning algorithms can deal with an infinite number
 of features!
\end_layout

\begin_deeper
\begin_layout Itemize
Requires a support vector machine - we will talk about this later in the
 course.
\end_layout

\end_deeper
\begin_layout Subsection*
Unsupervised learning
\end_layout

\begin_layout Itemize
We are given data where we don't know the 
\begin_inset Quotes eld
\end_inset

right answer.
\begin_inset Quotes erd
\end_inset

 The actual data is not classified as true or false.
\end_layout

\begin_layout Itemize
The question is: here is the dataset, can you find some structure in the
 data?
\end_layout

\begin_layout Itemize
Example: clustering algorithm.
 Used in Google news to group similar stories together.
\end_layout

\begin_deeper
\begin_layout Itemize
In this method, the algorithm assigns group labels to different clusters.
\end_layout

\end_deeper
\begin_layout Itemize
Other examples: social network analysis, organization of computer clusters,
 market segmentation, astronomical data analysis.
\end_layout

\begin_layout Itemize
Example: cocktail party problem - useful for separating highly correlated
 audio tracks into independent tracks.
\end_layout

\begin_deeper
\begin_layout Itemize
Cocktail party problem algorithm: 
\family typewriter
[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
\family default
 (singular value decomposition)
\end_layout

\end_deeper
\begin_layout Section*
Linear regression with one variable
\end_layout

\begin_layout Subsection*
Model and cost function
\end_layout

\begin_layout Itemize
Regression problem: predict a (continuous output).
\end_layout

\begin_layout Itemize
General notation for this course:
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
m
\emph default
: number of examples in the training dataset.
\end_layout

\begin_layout Itemize

\emph on
x
\emph default
: input variables/features.
\end_layout

\begin_layout Itemize

\emph on
y
\emph default
: output variable/target.
\end_layout

\begin_layout Itemize

\emph on
(x,y)
\emph default
: denotes a single training example.
\end_layout

\end_deeper
\begin_layout Itemize
General flow:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set -> learning algorithm -> hypothesis function.
\end_layout

\begin_layout Itemize
The hypothesis function takes input (
\emph on
x
\emph default
) and predicts the outcome (
\emph on
y
\emph default
).
 It maps from 
\emph on
x
\emph default
's to 
\emph on
y
\emph default
's.
\end_layout

\end_deeper
\begin_layout Itemize
How do we represent the hypothesis 
\emph on
h
\emph default
?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$
\end_inset

 for univariate linear regression, or linear regression with one variable.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\theta$
\end_inset

's are the parameters of the model.
\end_layout

\end_deeper
\begin_layout Itemize
How do we choose the parameters of the model?
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 such that we minimize the sum of the squared errors: 
\begin_inset Formula $\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Factor of 
\begin_inset Formula $\frac{1}{2m}$
\end_inset

 doesn't affect parameter values and will make later math a bit easier.
\end_layout

\begin_layout Itemize
This also defined as the cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
There are other cost functions that may work as well, but the squared error
 cost function is the most commonly used one for linear regression.
\end_layout

\end_deeper
\begin_layout Itemize
Can be useful to plot cost function in terms of the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
If we assume an intercept of 0 (
\begin_inset Formula $\theta_{0}=0$
\end_inset

), the value of 
\begin_inset Formula $\theta_{1}$
\end_inset

 that minimizes the cost function is 
\begin_inset Formula $\theta_{1}=\frac{\sum_{i}x^{\left(i\right)}y^{\left(i\right)}}{\sum_{i}x^{2\left(i\right)}}$
\end_inset


\end_layout

\begin_layout Itemize
For two parameters,
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{1}=\frac{\sum_{i=1}^{m}x^{\left(i\right)}y^{\left(i\right)}-\bar{x}\bar{y}}{\sum_{i=1}^{m}x^{\left(i\right)2}-\bar{x}^{2}}$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{0}=\bar{y}-\theta_{1}\bar{x}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Gradient descent
\end_layout

\begin_layout Itemize
Have some cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 that we want to minimize (find 
\begin_inset Formula $\underset{\theta_{0,}\theta_{1}}{\text{min}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Outline:
\end_layout

\begin_deeper
\begin_layout Itemize
Start with some 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
Keep changing 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

 to reduce 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 until we hopefully end up at a minimum.
\end_layout

\end_deeper
\begin_layout Itemize
Definition of gradient descent algorithm: 
\family typewriter
repeat until convergence { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 }.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $:=$
\end_inset

: assignment operator.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

: learning rate.
 Controls step size.
\end_layout

\end_deeper
\begin_layout Itemize
When actually programming this, make sure you change the parameters at the
 same time and then update them at the same time! Otherwise you may incorrectly
 use new values of the parameters to calculate the cost function.
\end_layout

\end_deeper
\begin_layout Itemize
Don't need to adjust 
\begin_inset Formula $\alpha$
\end_inset

 over time because the derivative term will get smaller as it approaches
 a local minimum.
\end_layout

\begin_layout Itemize
Applying gradient descent to our simple cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)=\frac{\partial}{\partial\theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}\left(\theta_{0}+\theta_{1}x^{\left(i\right)}-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=0$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{0}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=1$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{1}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Batch gradient descent: used in machine learning, each step of gradient
 descent uses all of the training examples.
\end_layout

\begin_layout Section*
Linear regression with multiple variables
\end_layout

\begin_layout Itemize
Also called multivariate linear regression.
\end_layout

\begin_layout Itemize
We now have multiple features (or predictors) that we want to use to develop
 a hypothesis function and make a prediction.
\end_layout

\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

: number of features.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: input (features) of 
\begin_inset Formula $i$
\end_inset

th training example.
 This is a vector of features in the 
\begin_inset Formula $i$
\end_inset

th observation.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{j}^{\left(i\right)}$
\end_inset

: value of feature 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $i$
\end_inset

th training example.
\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis function now has a new form:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$
\end_inset


\end_layout

\begin_layout Itemize
For convenience of notation, we can define 
\begin_inset Formula $x_{0}^{\left(i\right)}=1$
\end_inset

 so that 
\begin_inset Formula $h_{\theta}\left(x\right)=\sum_{i=1}^{n}\theta_{i}x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
We can also use linear algebra:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x=\left[x_{0},x_{1},...,x_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{0},\theta_{1},...,\theta_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta x^{T}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Gradient descent with multiple variables
\end_layout

\begin_layout Itemize
Write set of parameters as 
\begin_inset Formula $\theta$
\end_inset

 and cost function as 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Gradient descent: 
\family typewriter
repeat { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

 } 
\family default
(simultaneously update for each 
\begin_inset Formula $j=0,1,...,n$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Tips for gradient descent:
\end_layout

\begin_deeper
\begin_layout Itemize
Feature scaling - make sure that different features taken on similar ranges
 of values.
\end_layout

\begin_deeper
\begin_layout Itemize
Ideally, all features will be in the range 
\begin_inset Formula $-1\leq x\leq1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Mean normalization: normalize a feature to have a mean of zero.
 When doing this, you just replace a feature 
\begin_inset Formula $x_{i}$
\end_inset

 with 
\begin_inset Formula $x_{i}-\mu_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Do not apply this to 
\begin_inset Formula $x_{0}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Can also normalize by standard deviation: 
\begin_inset Formula $x_{i}\rightarrow\frac{x_{i}-\mu_{i}}{s_{i}}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Making sure that gradient descent is working properly:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot 
\begin_inset Formula $\underset{\theta}{\text{min}}J\left(\theta\right)$
\end_inset

 vs.
 number of iterations.
 It should decrease after every iteration for sufficiently small 
\begin_inset Formula $\alpha$
\end_inset

 (can be shown mathematically).
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too large, you may continue overshooting the minimum and never be able
 to actually reach it.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too small, convergence will happen, but very slowly.
\end_layout

\end_deeper
\begin_layout Itemize
Automatic convergence tests: algorithms that tell you whether gradient descent
 has converged:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: declare convergence if 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 decreases by less than some value (like 
\begin_inset Formula $10^{-3}$
\end_inset

) in one iteration.
\end_layout

\end_deeper
\begin_layout Subsection*
Polynomial regression
\end_layout

\begin_layout Itemize
Can create new features from existing ones.
\end_layout

\begin_deeper
\begin_layout Itemize
Example: area = frontage * depth.
\end_layout

\end_deeper
\begin_layout Itemize
Polynomial model: 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$
\end_inset

 for a quadratic function.
\end_layout

\begin_deeper
\begin_layout Itemize
Important to use feature scaling since when you square or cube features,
 the range becomes much larger.
\end_layout

\end_deeper
\begin_layout Subsection*
Normal equation
\end_layout

\begin_layout Itemize
Method to solve for 
\begin_inset Formula $\theta$
\end_inset

 analytically.
\end_layout

\begin_layout Itemize
How to go about it:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)=...=0$
\end_inset

, then solve for every 
\begin_inset Formula $\theta_{j}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Linear algebra method:
\end_layout

\begin_deeper
\begin_layout Itemize
Put all features into a matrix: 
\begin_inset Formula $X=\begin{array}{ccccc}
1 & 2104 & 5 & 1 & 45\\
1 & 1416 & 3 & 2 & 40\\
1 & 1534 & 3 & 2 & 30\\
1 & 852 & 2 & 1 & 36
\end{array}$
\end_inset

 (including a column of ones for 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\begin_layout Itemize
Put all 
\begin_inset Formula $y_{j}$
\end_inset

s into a vector: 
\begin_inset Formula $Y=\begin{array}{c}
460\\
232\\
315\\
178
\end{array}$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
In Matlab/Octave: 
\family typewriter
pinv(X'*X)*X'*Y
\end_layout

\begin_layout Itemize
Feature scaling is not needed for the normal equation method.
\end_layout

\end_deeper
\begin_layout Itemize
Normal equation vs.
 gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Pros:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: no need to choose 
\begin_inset Formula $\alpha$
\end_inset

, no need to iterate.
\end_layout

\begin_layout Itemize
Gradient descent: works well even when the number of features is large.
\end_layout

\end_deeper
\begin_layout Itemize
Cons:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: slow if number of features is very large, need to compute
 
\begin_inset Formula $\left(X^{T}X\right)^{-1}$
\end_inset


\end_layout

\begin_layout Itemize
Gradient descent: need to choose 
\begin_inset Formula $\alpha$
\end_inset

, needs many iterations.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation non-invertibility: what if 
\begin_inset Formula $\left(X^{T}X\right)$
\end_inset

 is non-invertible? Also known as 
\begin_inset Quotes eld
\end_inset

singular
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

degenerate.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Can occur when there are redundant features (linearly dependent) or if there
 are too many features.
\end_layout

\begin_deeper
\begin_layout Itemize
Possible solution: delete some features or use regularization.
\end_layout

\end_deeper
\begin_layout Itemize
If you use 
\family typewriter
pinv
\family default
 in Matlab/Octave instead of 
\family typewriter
inv
\family default
, it should handle singular cases for you.
\end_layout

\end_deeper
\begin_layout Section*
Logistic Regression
\end_layout

\begin_layout Subsection*
Classification and Representation
\end_layout

\begin_layout Itemize
Classification: putting things into discrete categories.
 Binary classification is when there are only two categories.
\end_layout

\begin_layout Itemize
Linear regression is not usually a very good solution for classification
 problems.
 Examples where we can clearly use some sort of threshold for classification
 make this clear.
\end_layout

\begin_layout Itemize
Logistic regression has the property that the predictions of the hypothesis
 function are always between 0 and 1: 
\begin_inset Formula $0\leq h_{\theta}\left(x\right)\leq1$
\end_inset

.
\end_layout

\begin_layout Itemize
For linear regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta^{T}x$
\end_inset

.
\end_layout

\begin_layout Itemize
For logistic regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta^{T}x\right)$
\end_inset

, where 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is the sigmoid or logistic function, 
\begin_inset Formula $g\left(z\right)=\frac{1}{1+e^{-z}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Thus, 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+\exp\left(-\theta^{T}x\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
As before, we need to fit the parameters 
\begin_inset Formula $\theta$
\end_inset

 to our data.
\end_layout

\end_deeper
\begin_layout Itemize
Interpretation of hypothesis output:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

: estimated probability that 
\begin_inset Formula $y=1$
\end_inset

 (i.e., positive result) on input 
\begin_inset Formula $x$
\end_inset

; i.e.
 
\begin_inset Formula $h_{\theta}\left(x\right)=p\left(y=1|x;\theta\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Decision boundary:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we predict that 
\begin_inset Formula $y=1$
\end_inset

 when 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 and 
\begin_inset Formula $y=0$
\end_inset

 when 
\begin_inset Formula $h_{\theta}<0.5$
\end_inset

.
\end_layout

\begin_layout Itemize
We see that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is 
\begin_inset Formula $\geq0.5$
\end_inset

 when 
\begin_inset Formula $z\geq0$
\end_inset

, thus 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 whenever 
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

.
\end_layout

\begin_layout Itemize
This essentially defines a threshold for making predictions with our model.
\end_layout

\begin_layout Itemize
The decision boundary is a property of the hypothesis and its parameters,
 not a property of the dataset.
\end_layout

\end_deeper
\begin_layout Itemize
Non-linear decision boundaries:
\end_layout

\begin_deeper
\begin_layout Itemize
Can use higher-order polynomials terms in logistic regression.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{2}^{2}+\theta_{5}x_{1}x_{2}+\theta_{6}x_{1}^{3}+...\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Logistic Regression Model
\end_layout

\begin_layout Itemize
Cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
Linear regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}=\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Logistic regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What cost function here should we use here?
\end_layout

\begin_layout Itemize
We can't use the same thing that we used for linear regression because it
 will be a non-convex function - thus, gradient descent will not necessarily
 converge to the global minimum.
\end_layout

\begin_layout Itemize
Instead, we use 
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=\left\{ \begin{array}{l}
-\log\left(h_{\theta}\left(x\right)\right)\text{ if }y=1\\
-\log\left(1-h_{\theta}\left(x\right)\right)\text{ if }y=0
\end{array}\right.$
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=1$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow0$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=0$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=0$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow1$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Simplified way of writing the cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=-y\log\left(h_{\theta}\left(x\right)\right)-\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)$
\end_inset


\end_layout

\begin_layout Itemize
Note: 
\begin_inset Formula $y$
\end_inset

 is always equal to 0 or 1.
\end_layout

\begin_layout Itemize
So the full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent: we do the same thing as before, we just have a new cost
 function.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{j}$
\end_inset

 := 
\begin_inset Formula $\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
This is the same as for linear regression, although our hypothesis function
 has changed.
\end_layout

\begin_layout Itemize
Make sure to simultaneously update all 
\begin_inset Formula $\theta_{j}$
\end_inset

!
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization:
\end_layout

\begin_deeper
\begin_layout Itemize
We have some cost function 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and we want the set of parameters
\begin_inset Formula $\theta$
\end_inset

 that gives the minimum of 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $\theta$
\end_inset

, we have code that can compute 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Optimization algorithms:
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descent
\end_layout

\begin_layout Itemize
Conjugate gradient
\end_layout

\begin_layout Itemize
BFGS
\end_layout

\begin_layout Itemize
L-BFGS
\end_layout

\end_deeper
\begin_layout Itemize
Advantages: no need to manually pick the learning rate 
\begin_inset Formula $\alpha$
\end_inset

, often faster than gradient descent.
\end_layout

\begin_layout Itemize
Disadvantages: more complex, can cause implementations to be error prone.
\end_layout

\begin_layout Itemize
Can use 
\family typewriter
fminunc
\family default
 in Matlab/Octave.
\end_layout

\begin_deeper
\begin_layout Itemize
Define a function which returns the cost and gradient.
\end_layout

\begin_layout Itemize
Example:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% function
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

jVal = (theta(1)-5)^2 + (theta(2)-5)^2;
\end_layout

\begin_layout Plain Layout

gradient(1) = 2*(theta(1)-5);
\end_layout

\begin_layout Plain Layout

gradient(2) = 2*(theta(2)-5);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Setup
\end_layout

\begin_layout Plain Layout

options = optimset('GradObj','on','MaxIter','100');
\end_layout

\begin_layout Plain Layout

initialTheta = zeros(2,1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Call fminunc
\end_layout

\begin_layout Plain Layout

[optTheta, functionVal, exitFlag] = ...
\end_layout

\begin_layout Plain Layout

	fminunc(@costFunction, initialTheta, options);
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Multi-class classification
\end_layout

\begin_layout Itemize
Examples of this type of problem:
\end_layout

\begin_deeper
\begin_layout Itemize
Email tagging into different folders.
\end_layout

\begin_layout Itemize
Medical diagnosis.
\end_layout

\begin_layout Itemize
Weather category.
\end_layout

\end_deeper
\begin_layout Itemize
One-vs-all classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: a training dataset with three classes.
\end_layout

\begin_layout Itemize
First: is it in class 1, or class 2/3?
\end_layout

\begin_layout Itemize
Second: is it in class 2, or class 1/3?
\end_layout

\begin_layout Itemize
Third: is it in class 3, or class 1/2?
\end_layout

\begin_layout Itemize
Use standard logistic regression for each step.
\end_layout

\begin_layout Itemize
Each gives a hypothesis: 
\begin_inset Formula $h_{\theta}^{\left(i\right)}\left(x\right)=P\left(y=i|x;\theta\right)$
\end_inset

, 
\begin_inset Formula $\left(i=1,2,3\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The hypothesis tells us the probability that 
\begin_inset Formula $y$
\end_inset

 is in class 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
To make a prediction on a new input 
\begin_inset Formula $x$
\end_inset

, pick the class 
\begin_inset Formula $i$
\end_inset

 that has the maximum probability.
\end_layout

\end_deeper
\begin_layout Section*
Regularization and overfitting
\end_layout

\begin_layout Itemize
Overfitting: similar number of parameters to data points.
 This allows the model to fit the training dataset very well, however, it
 will not make accurate predictions for new examples.
 Also called 
\begin_inset Quotes eld
\end_inset

high variance.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Occurs when you have a lot of features and not a lot of training data.
\end_layout

\begin_layout Itemize
Underfitting may occur when our model has too few parameters to properly
 model the data.
 In this situation, the model has 
\begin_inset Quotes eld
\end_inset

high bias.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Options for dealing with overfitting:
\end_layout

\begin_deeper
\begin_layout Itemize
Reduce number of features: can manually select which features to keep, or
 use a model selection algorithm to determine this (will be covered later
 in the course).
\end_layout

\begin_layout Itemize
Regularization: keep all of the features, but reduce the magnitude/values
 of the parameters 
\begin_inset Formula $\theta_{j}$
\end_inset

.
 Works well when we have a lot of features, each of which contributes a
 little bit to predicting 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection*
Regularization
\end_layout

\begin_layout Itemize
Suppose we penalize and make 
\begin_inset Formula $\theta_{3},\theta_{4}$
\end_inset

 very small.
 We do this by adjusting our cost function by adding in terms like 
\begin_inset Formula $1000\theta_{3}^{2}+1000\theta_{4}^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This will give values of 
\begin_inset Formula $\theta_{3}$
\end_inset

 and 
\begin_inset Formula $\theta_{4}$
\end_inset

 which are close to zero when the cost function is small.
\end_layout

\end_deeper
\begin_layout Itemize
General idea behind regularization:
\end_layout

\begin_deeper
\begin_layout Itemize
Small values for parameters 
\begin_inset Formula $\theta_{0},\theta_{1},...,\theta_{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Simpler hypothesis, less prone to overfitting.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example: housing price prediction.
\end_layout

\begin_deeper
\begin_layout Itemize
Features: 
\begin_inset Formula $x_{1},x_{2},...,x_{100}$
\end_inset


\end_layout

\begin_layout Itemize
Parameters: 
\begin_inset Formula $\theta_{0},\theta_{1},\theta_{2},...,\theta_{100}$
\end_inset


\end_layout

\begin_layout Itemize
We don't know which parameters to try to shrink 
\emph on
a priori
\emph default
.
\end_layout

\begin_layout Itemize
Take our cost function 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

 and add a new term to shrink 
\emph on
all
\emph default
 of our parameters except 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}+\lambda\sum_{j=1}^{n}\theta_{j}^{2}\right]$
\end_inset


\end_layout

\begin_layout Itemize
The first part in brackets captures the goal of fitting the data well.
\end_layout

\begin_layout Itemize
The second part captures the goal of keeping the parameters 
\begin_inset Formula $\theta$
\end_inset

 small.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 is the regularization parameter and controls the trade-off between these
 two goals.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\lambda$
\end_inset

 is too large, we w
\end_layout

\begin_layout Itemize
on't even fit the training dataset well.
 All of the parameters 
\begin_inset Formula $\theta$
\end_inset

 will go to zero except 
\begin_inset Formula $\theta_{0}$
\end_inset

, so we will be fitting a flat line to our data.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Regularized linear regression
\end_layout

\begin_layout Itemize
Recall that we don't want to shrink 
\begin_inset Formula $\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The term 
\begin_inset Formula $1-\alpha\frac{\lambda}{m}$
\end_inset

 is usually slightly less than 1 (assuming that your learning rate is small).
 This term shrinks your 
\begin_inset Formula $\theta_{j}$
\end_inset

, then the other term is just the original gradient descent term.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation
\end_layout

\begin_deeper
\begin_layout Itemize
Before, we had 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
With our new, regularized cost function, you can derive the minimum of the
 cost function by taking derivatives with respect to each parameter, as
 before.
\end_layout

\begin_layout Itemize
Now, we get 
\begin_inset Formula $\theta=\left(X^{T}X+\lambda Z\right)^{-1}X^{T}Y$
\end_inset

, where 
\begin_inset Formula $Z$
\end_inset

 is the identity matrix, except the top left entry is 0 instead of 1 (because
 we don't regularize 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Non-invertibility
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have fewer examples than features (
\begin_inset Formula $m\leq n$
\end_inset

).
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

 will give a strange answer since 
\begin_inset Formula $X^{T}X$
\end_inset

 is non-invertible/singular (make sure to use 
\family typewriter
pinv
\family default
 instead of 
\family typewriter
inv
\family default
).
\end_layout

\begin_layout Itemize
However, if 
\begin_inset Formula $\lambda>0$
\end_inset

 , we can prove that the matrix 
\begin_inset Formula $X^{T}X+\lambda Z$
\end_inset

 is invertible!
\end_layout

\end_deeper
\begin_layout Subsection*
Regularized logistic regression
\end_layout

\begin_layout Itemize
To modify our cost function to use regularization, we add a term 
\begin_inset Formula $\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
The full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\left[\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Recall that although this looks the same as for regularized linear regression,
 the hypothesis function is different for logistic regression.
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization methods:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% Define a function to compute the cost.
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

	jVal = %code to compute J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(1) = %code to compute d/dtheta_0 J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(2) = %code to compute d/dtheta_1 J(theta);
\end_layout

\begin_layout Plain Layout

	...
\end_layout

\begin_layout Plain Layout

	gradient(n+1) = %code to compute d/dtheta_n J(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% then, we pass this function to fminunc as before.
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Neural networks: representation
\end_layout

\begin_layout Subsection*
Motivation
\end_layout

\begin_layout Itemize
Used for solving complex, non-linear hypotheses.
\end_layout

\begin_layout Itemize
For many predictors, including even quadratic terms can require a huge number
 of terms (grows roughly as 
\begin_inset Formula $n^{2}$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
One option is to include only a subset of higher-order terms, but it will
 probably underfit the data.
\end_layout

\end_deeper
\begin_layout Itemize
Neural networks: algorithms that try to mimic the brain.
\end_layout

\begin_layout Itemize
Recent resurgence: state-of-the-art technique for many computational application
s.
\end_layout

\begin_layout Itemize
Computationally more expensive.
\end_layout

\begin_layout Subsection*
Neural networks
\end_layout

\begin_layout Itemize
Developed as simulating networks of neurons in the brain.
\end_layout

\begin_layout Itemize
Neuron structure:
\end_layout

\begin_deeper
\begin_layout Itemize
Dendrites: 
\begin_inset Quotes eld
\end_inset

input wires,
\begin_inset Quotes erd
\end_inset

 receive inputs from other locations.
\end_layout

\begin_layout Itemize
Axon: 
\begin_inset Quotes eld
\end_inset

output wire,
\begin_inset Quotes erd
\end_inset

 used to send signals to other neurons.
\end_layout

\end_deeper
\begin_layout Itemize
Neuron model: logistic unit.
\end_layout

\begin_deeper
\begin_layout Itemize
We use a model of what a neuron does: it takes some number of inputs (
\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset

), does some computation, and sends an output 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^{T}x}}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This is a sigmoid (logistic) activation function.
\end_layout

\begin_layout Itemize
The parameters 
\begin_inset Formula $\theta$
\end_inset

 are sometimes referred to as 
\begin_inset Quotes eld
\end_inset

weights.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x_{0}$
\end_inset

 is sometimes referred to as the 
\begin_inset Quotes eld
\end_inset

bias unit.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Neural network: divide into layers.
\end_layout

\begin_deeper
\begin_layout Itemize
Layer 1: 
\begin_inset Quotes eld
\end_inset

input layer,
\begin_inset Quotes erd
\end_inset

 made up of input features.
\end_layout

\begin_layout Itemize
Layer 2: some set of neurons, called 
\begin_inset Quotes eld
\end_inset

hidden layers.
\begin_inset Quotes erd
\end_inset

 Can be several hidden layers.
\end_layout

\begin_layout Itemize
Final layer: 
\begin_inset Quotes eld
\end_inset

output layer,
\begin_inset Quotes erd
\end_inset

 outputs final value computed by a hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $a_{i}^{\left(j\right)}$
\end_inset

: 
\begin_inset Quotes eld
\end_inset

activation
\begin_inset Quotes erd
\end_inset

 of unit 
\begin_inset Formula $i$
\end_inset

 in layer 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

: matrix of weights controlling function mapping from layer 
\begin_inset Formula $j$
\end_inset

 to layer 
\begin_inset Formula $j+1$
\end_inset

.
\end_layout

\begin_layout Itemize
If a network has 
\begin_inset Formula $s_{j}$
\end_inset

 units in layer 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $s_{j+1}$
\end_inset

units in layer 
\begin_inset Formula $j+1$
\end_inset

, then 
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

 will be of dimension 
\begin_inset Formula $s_{j+1}\times\left(s_{j}+1\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $a_{1}^{\left(2\right)}=g\left(\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Can write 
\begin_inset Formula $z_{1}^{\left(2\right)}=\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}$
\end_inset

; i.e.
 a row of 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Vectorized implementation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $z^{\left(2\right)}=\Theta^{\left(1\right)}x=\Theta^{\left(1\right)}a^{\left(1\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(2\right)}=g\left(z^{\left(2\right)}\right)$
\end_inset

 (here 
\begin_inset Formula $z^{\left(2\right)}$
\end_inset

 is a column vector of 
\begin_inset Formula $z_{1}^{\left(2\right)},...,z_{3}^{\left(2\right)}$
\end_inset

.
 Note that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is applied element-wise.
\end_layout

\begin_layout Itemize
Add 
\begin_inset Formula $a_{0}^{\left(2\right)}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $z^{\left(3\right)}=\Theta^{\left(2\right)}a^{\left(2\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\Theta}\left(x\right)=a^{\left(3\right)}=g\left(z^{\left(3\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
We can think of the way a neural network works as though it is 
\begin_inset Quotes eld
\end_inset

learning its own features.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The function mapping 
\begin_inset Formula $a^{\left(1\right)}$
\end_inset

 to 
\begin_inset Formula $a^{\left(2\right)}$
\end_inset

 is what changes the input features to new features which go into the hidden
 layers.
\end_layout

\end_deeper
\begin_layout Itemize
You can have neural networks with other types of layouts.
\end_layout

\begin_deeper
\begin_layout Itemize
The layout is referred to as the 
\begin_inset Quotes eld
\end_inset

architechture.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Applications
\end_layout

\begin_layout Itemize
Can set up a neural network which acts as a logical filter (AND, OR, XOR,
 NOT, etc.).
\end_layout

\begin_layout Itemize
Interesting because sometimes these functions (XOR, for example) require
 non-linear decision boundaries.
\end_layout

\begin_layout Itemize
Multi-class classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Build a neural network with as many outputs as there are classes.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 is a vector, and we take the largest entry to be the predicted class.
\end_layout

\end_deeper
\begin_layout Section*
Neural networks: learning
\end_layout

\begin_layout Subsection*
Cost function and backpropagation
\end_layout

\begin_layout Itemize
\begin_inset Formula $L$
\end_inset

: number of layers in the network.
\end_layout

\begin_layout Itemize
\begin_inset Formula $s_{l}$
\end_inset

: number of units (not counting bias unit) in layer 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Itemize
Binary classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Labels 
\begin_inset Formula $y$
\end_inset

 are either 0 or 1.
\end_layout

\begin_layout Itemize
One output unit (
\begin_inset Formula $K=1$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Multi-class classification:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 classes, 
\begin_inset Formula $y\in R^{K}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 output units.
\end_layout

\end_deeper
\begin_layout Itemize
Cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
Logistic regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\left[\sum_{i=1}^{m}y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Neural network: 
\begin_inset Formula $h_{\Theta}\left(x\right)\in R^{K}$
\end_inset

 (
\begin_inset Formula $K$
\end_inset

-dimensional vector), 
\begin_inset Formula $\left(h_{\Theta}\left(x\right)\right)_{i}=i$
\end_inset

th output.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(\Theta\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{\left(i\right)}\log\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}+\left(1-y_{k}^{\left(i\right)}\right)\log\left(1-\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}\right)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{\left(l\right)}\right)^{2}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Gradient computation
\end_layout

\begin_deeper
\begin_layout Itemize
One training example 
\begin_inset Formula $\left(x,y\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Forward propagation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $a^{\left(1\right)}=x$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(2\right)}=g\left(z^{\left(2\right)}\right)=g\left(\Theta^{\left(1\right)}a^{\left(1\right)}\right)$
\end_inset

 (add 
\begin_inset Formula $a_{0}^{\left(2\right)}$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(3\right)}=g\left(z^{\left(3\right)}\right)=g\left(\Theta^{\left(2\right)}a^{\left(2\right)}\right)$
\end_inset

 (add 
\begin_inset Formula $a_{0}^{\left(3\right)}$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(4\right)}=h_{\Theta}\left(x\right)=g\left(z^{\left(4\right)}\right)=g\left(\Theta^{\left(3\right)}a^{\left(3\right)}\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Then, we use backpropagation:
\end_layout

\begin_deeper
\begin_layout Itemize
Intuition: 
\begin_inset Formula $\delta_{j}^{\left(l\right)}=$
\end_inset

 
\begin_inset Quotes eld
\end_inset

error
\begin_inset Quotes erd
\end_inset

 of node 
\begin_inset Formula $j$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Itemize
For each output unit (layer 
\begin_inset Formula $L=4$
\end_inset

), 
\begin_inset Formula $\delta_{j}^{\left(4\right)}=a_{j}^{\left(4\right)}-y_{j}$
\end_inset

.
 (difference between the hypothesis and the actual classification)
\end_layout

\begin_layout Itemize
Vectorized version: 
\begin_inset Formula $\delta^{\left(4\right)}=a^{\left(4\right)}-y$
\end_inset

, each of these is a vector with number of elements equal to the number
 of output units 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta^{\left(3\right)}=\left(\Theta^{\left(3\right)}\right)^{T}\delta^{\left(4\right)}.*g'\left(z^{\left(3\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta^{\left(2\right)}=\left(\Theta^{\left(2\right)}\right)^{T}\delta^{\left(3\right)}.*g'\left(z^{\left(2\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
No 
\begin_inset Formula $\delta^{\left(1\right)}$
\end_inset

 for this example.
\end_layout

\begin_layout Itemize
Notes: 
\begin_inset Formula $g'\left(z^{\left(i\right)}\right)=a^{\left(i\right)}.*\left(1-a^{\left(i\right)}\right)=g\left(z^{\left(i\right)}\right).*\left(1-g\left(z^{\left(i\right)}\right)\right)$
\end_inset

, and 
\begin_inset Formula $.*$
\end_inset

 indicates element-wise multiplication.
\end_layout

\begin_layout Itemize
It is possible to prove that the partial derivative terms we want are given
 by 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset

 (without regularization).
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
Backpropagation algorithm:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\left\{ \left(x^{\left(1\right)},y^{\left(1\right)}\right),...,\left(x^{\left(m\right)},y^{\left(m\right)}\right)\right\} $
\end_inset


\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}=0$
\end_inset

 for all 
\begin_inset Formula $l,i,j$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $i=1:m$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Set 
\begin_inset Formula $a^{\left(1\right)}=x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Perform forward propagation to compute 
\begin_inset Formula $a^{\left(l\right)}$
\end_inset

 for 
\begin_inset Formula $l=2,3,...,L$
\end_inset

.
\end_layout

\begin_layout Itemize
Using 
\begin_inset Formula $y^{\left(i\right)}$
\end_inset

, compute 
\begin_inset Formula $\delta^{\left(L\right)}=a^{\left(L\right)}-y^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Compute 
\begin_inset Formula $\delta^{\left(L-1\right)},\delta^{\left(L-2\right)},...,\delta^{\left(2\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}:=\Delta_{ij}^{\left(l\right)}+a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Vectorized form: 
\begin_inset Formula $\Delta^{\left(l\right)}:=\Delta^{\left(l\right)}+\delta^{\left(l+1\right)}\left(a^{\left(l\right)}\right)^{T}$
\end_inset


\end_layout

\begin_layout Itemize
After the for loop, we compute:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\frac{1}{m}\Delta_{ij}^{\left(l\right)}+\lambda\Theta_{ij}^{\left(l\right)}$
\end_inset

, if 
\begin_inset Formula $j\neq0$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\frac{1}{m}\Delta_{ij}^{\left(l\right)}$
\end_inset

, if 
\begin_inset Formula $j=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Can show that 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=D_{ij}^{\left(l\right)}$
\end_inset

, so we can use this in gradient descent or other optimization algorithms.
\end_layout

\end_deeper
\begin_layout Itemize
What is backpropagation doing?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\delta_{j}^{\left(l\right)}$
\end_inset

: 
\begin_inset Quotes eld
\end_inset

error
\begin_inset Quotes erd
\end_inset

 of cost for 
\begin_inset Formula $a_{j}^{\left(l\right)}$
\end_inset

 (unit 
\begin_inset Formula $j$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

).
 I.e., 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta_{j}^{\left(l\right)}=\frac{\partial}{\partial z_{j}^{\left(l\right)}}\text{cost}\left(i\right)$
\end_inset

 for 
\begin_inset Formula $j\geq0$
\end_inset

, where cost
\begin_inset Formula $\left(i\right)=y^{\left(i\right)}\log h_{\Theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log h_{\Theta}\left(x^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
These are a measure of how much we would like to change the neural network's
 weights so as to change the intermediate values of the computation, and
 thus change the final outputs, and the cost.
\end_layout

\end_deeper
\begin_layout Subsection*
Backpropagation in practice
\end_layout

\begin_layout Itemize
Unrolling parameters from matrices into vectors:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% Let's say you defined a function for computing the cost function
\end_layout

\begin_layout Plain Layout

% Here jVal is a number, and theta and grad are vectors in R^{n+1}
\end_layout

\begin_layout Plain Layout

function [jVal, grad] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

% You will pass it to fminunc as:
\end_layout

\begin_layout Plain Layout

optTheta = fminunc(@costFunction, initialTheta, options)
\end_layout

\begin_layout Plain Layout

% For a neural network, your parameters Theta are matrices.
\end_layout

\begin_layout Plain Layout

% The gradients D are also matrices.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Example: a neural net with 10 inputs, a hidden layer of 10 nodes, and 1
 output.
 I.e., 
\begin_inset Formula $s_{1}=10$
\end_inset

, 
\begin_inset Formula $s_{2}=10$
\end_inset

, 
\begin_inset Formula $s_{3}=1$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\Theta^{\left(1\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}\in R^{1\times11}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $D^{\left(1\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}\in R^{1\times11}$
\end_inset


\end_layout

\begin_layout Itemize
To unroll them, you can do: 
\family typewriter
thetaVec = [Theta1(:); Theta2(:); Theta3(:)];
\family default
, and the same for D.
\end_layout

\begin_layout Itemize
To go back to the matrix representation, you can do 
\family typewriter
Theta1 = reshape(thetaVec(1:110),10,11); Theta2 = reshape(thetaVec(111:220),10,1
1); Theta3 = reshape(thetaVec(221:231),1,11);
\end_layout

\end_deeper
\begin_layout Itemize
Example: have initial parameters 
\begin_inset Formula $\Theta^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Unroll to get 
\family typewriter
initialTheta
\family default
 to pass to 
\family typewriter
fminunc(@costFunction, initialTheta, options)
\end_layout

\begin_layout Itemize
Then we change our cost function:
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
function [jVal, gradientVec] = costFunction(thetaVec)
\end_layout

\begin_layout Itemize
From 
\family typewriter
thetaVec
\family default
, get 
\begin_inset Formula $\Theta^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Use forward propagation and backpropagation to compute 
\begin_inset Formula $D^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}$
\end_inset

 and 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Unroll 
\begin_inset Formula $D^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}$
\end_inset

 to get 
\family typewriter
gradientVec
\family default
.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Downside of backpropagation - easy to have many subtle bugs in your implementati
on.
\end_layout

\begin_deeper
\begin_layout Itemize
However, there is something called 
\begin_inset Quotes eld
\end_inset

gradient checking
\begin_inset Quotes erd
\end_inset

 which allows us to catch almost all of these bugs.
\end_layout

\end_deeper
\begin_layout Itemize
Gradient checking
\end_layout

\begin_deeper
\begin_layout Itemize
Numerical estimate of gradients: 
\begin_inset Formula $\frac{\partial}{\partial\theta}J\left(\theta\right)\approx\frac{J\left(\theta+\epsilon\right)-J\left(\theta-\epsilon\right)}{2\epsilon}$
\end_inset

.
 Usually, we take 
\begin_inset Formula $\epsilon$
\end_inset

 to be very small, something like 
\begin_inset Formula $10^{-4}$
\end_inset

.
\end_layout

\begin_layout Itemize
Implementation in Matlab: 
\family typewriter
gradApprox = (J(theta + eps) - J(theta - eps))/(2*eps);
\end_layout

\end_deeper
\begin_layout Itemize
Gradient checking with a parameter vector 
\begin_inset Formula $\theta\in R^{n}$
\end_inset

 (
\begin_inset Formula $\theta$
\end_inset

 is the unrolled version of 
\begin_inset Formula $\Theta^{\left(1\right)},\Theta^{\left(2\right)}$
\end_inset

, etc.).
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{1},\theta_{2},...,\theta_{n}\right]$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{i}}J\left(\theta\right)\approx\frac{J\left(\theta_{1},...,\theta_{i}+\epsilon,...,\theta_{n}\right)-J\left(\theta_{1},...,\theta_{i}-\epsilon,...,\theta_{n}\right)}{2\epsilon}$
\end_inset


\end_layout

\begin_layout Itemize
These equations give a way to numerically approximate the gradient with
 respect to any of your parameters.
\end_layout

\begin_layout Itemize
Implementation in Matlab:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

for i=1:n
\end_layout

\begin_layout Plain Layout

	thetaPlus = theta;
\end_layout

\begin_layout Plain Layout

	thetaPlus(i) = thetaPlus(i) + epsilon;
\end_layout

\begin_layout Plain Layout

	thetaMinus = theta;
\end_layout

\begin_layout Plain Layout

	thetaMinus(i) = thetaMinus(i) - epsilon;
\end_layout

\begin_layout Plain Layout

	gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon);
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Implemenation:
\end_layout

\begin_deeper
\begin_layout Itemize
Implement backpropagation to compute 
\family typewriter
DVec
\family default
.
\end_layout

\begin_layout Itemize
Implement numerical gradient checking to compute 
\family typewriter
gradApprox
\family default
.
\end_layout

\begin_layout Itemize
Make sure they give similar values for a few test cases.
\end_layout

\begin_layout Itemize
Turn off gradient checking; just use backpropagation for future runs.
\end_layout

\end_deeper
\begin_layout Itemize
Important: make sure to disable your gradient checking code before training
 your classifier, otherwise your code will be very slow.
\end_layout

\end_deeper
\begin_layout Itemize
Random initialization: we need to pick some initial value for 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
For gradient descent, we previously set 
\family typewriter
initialTheta = zeros(n,1)
\family default
.
\end_layout

\begin_deeper
\begin_layout Itemize
This doesn't work for training a neural network; it will result in all nodes
 being the same and all deltas being the same.
 All of the partial derivatives will be equal to each other, as well.
\end_layout

\end_deeper
\begin_layout Itemize
So, for a neural network, we need to use a random choice for 
\family typewriter
initialTheta
\family default
 between 
\begin_inset Formula $\left[-\epsilon,\epsilon\right]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
Theta1 = rand(10,11)*(2*initEps) - initEps;
\end_layout

\begin_layout Itemize

\family typewriter
Theta2 = rand(1,11)*(2*initEps) - initEps;
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Putting it all together:
\end_layout

\begin_deeper
\begin_layout Itemize
First, pick some network architecture, or connectivity pattern between neurons.
\end_layout

\begin_deeper
\begin_layout Itemize
Number of input units should be the number of features.
\end_layout

\begin_layout Itemize
Number of output units should be the number of possible classifications.
\end_layout

\begin_layout Itemize
Reasonable default: one hidden layer, or if more than one hidden layer,
 have the same number of units in each hidden layer (usually the more, the
 better, although more units requires more computational power).
\end_layout

\end_deeper
\begin_layout Itemize
Next, we need to train a neural network.
\end_layout

\begin_deeper
\begin_layout Itemize
First, randomly initialize the weights.
\end_layout

\begin_layout Itemize
Then, implement forward propagation to get 
\begin_inset Formula $h_{\Theta}\left(x^{\left(i\right)}\right)$
\end_inset

 for any 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Implement code to compute the cost function 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Implement backpropagation to compute partial derivatives 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{jk}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Usually requires a for loop.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Use gradient checking to compare the partial derivatives computed using
 backpropagation to those computed numerically for a few examples.
\end_layout

\begin_layout Itemize
Disable gradient checking code if everything seems OK.
\end_layout

\begin_layout Itemize
Use gradient descent or advanced optimization methods with backpropagation
 to try to minimize 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 as a function of the parameters 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Note: 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 is not necessarily convex, so it is possible for the algorithms to get
 stuck in local optima.
\end_layout

\end_deeper
\end_deeper
\begin_layout Section*
Advice for applying machine learning
\end_layout

\begin_layout Subsection*
Evaluating a learning algorithm
\end_layout

\begin_layout Itemize
How do you decide what the most promising avenues to explore are?
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose you have implemented regularized linear regression to predict housing
 prices.
\end_layout

\begin_layout Itemize
However, when you test your hypothesis on a new set of houses, you find
 that it makes unacceptably large errors in its predictions.
\end_layout

\begin_layout Itemize
What should you try next?
\end_layout

\begin_deeper
\begin_layout Itemize
Get more training examples, although this doesn't always help.
\end_layout

\begin_layout Itemize
Spend time trying to carefully select a smaller set of features.
\end_layout

\begin_layout Itemize
Try getting additional features.
\end_layout

\begin_layout Itemize
Try adding polynomial features.
\end_layout

\begin_layout Itemize
Try increasing or decreasing the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Machine learning diagnostics: tests you can run to gain insight into what
 is/isn't working with a learning algorithm, and gain guidance as to how
 best to improve its performance.
\end_layout

\begin_deeper
\begin_layout Itemize
Diagnostics can take time to implement, but doing so can be a very good
 use of your time.
\end_layout

\end_deeper
\begin_layout Itemize
Evaluating your hypothesis:
\end_layout

\begin_deeper
\begin_layout Itemize
Split your data into two portions: training and testing datasets.
\end_layout

\begin_deeper
\begin_layout Itemize
Typical split is about 70% training, 30% testing.
\end_layout

\begin_layout Itemize
\begin_inset Formula $m_{test}$
\end_inset

: number of test examples.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left(x_{test}^{\left(i\right)},y_{test}^{\left(i\right)}\right)$
\end_inset

: 
\begin_inset Formula $i$
\end_inset

th example from test dataset.
\end_layout

\begin_layout Itemize
Should usually divide dataset by randomly choosing samples since there may
 sometimes be some order to the samples.
\end_layout

\end_deeper
\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Learn parameters 
\begin_inset Formula $\theta$
\end_inset

 from training data by minimizing training error 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Compute test set error 
\begin_inset Formula $J_{test}\left(\theta\right)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}\left(h_{\theta}\left(x_{test}^{\left(i\right)}\right)-y_{test}^{\left(i\right)}\right)^{2}$
\end_inset

 (this example is for linear regression).
\end_layout

\begin_layout Itemize
Alternate definition of test set error: 0/1 misclassification error.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $err\left(h_{\theta}\left(x\right),y\right)=\left\{ \begin{array}{c}
1,h_{\theta}\left(x\right)\geq0.5,y=0|h_{\theta}\left(x\right)<0.5,y=1\\
0,\text{otherwise}
\end{array}\right.$
\end_inset


\end_layout

\begin_layout Itemize
Test error: 
\begin_inset Formula $\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
When parameters are fit using a training set, the error of the parameters
 as measured on that data (training set error) is likely to be lower than
 when you measure the error on another, independent dataset (generalization
 error).
\end_layout

\begin_layout Itemize
Model selection
\end_layout

\begin_deeper
\begin_layout Itemize
Say you have a choice of models with different degrees of polynomials,.
\end_layout

\begin_layout Itemize
It's as if you have another parameter, 
\begin_inset Formula $d$
\end_inset

, which is the highest degree of the polynomial used in the model.
\end_layout

\begin_layout Itemize
You could fit each model to your training data and get a set of parameters
 for each model 
\begin_inset Formula $\theta^{\left(d\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Then you can look at the test set and compute the cost for each model: 
\begin_inset Formula $J_{test}\left(\theta^{\left(d\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, rind the model with the lowest cost.
 For this example, assume it's the model with parameters 
\begin_inset Formula $\theta^{\left(5\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Question: how well does the model generalize? Could report test set error
 
\begin_inset Formula $J_{test}\left(\theta^{\left(5\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Problem: 
\begin_inset Formula $J_{test}\left(\theta^{\left(5\right)}\right)$
\end_inset

 is likely to be optimistic because our extra parameter 
\begin_inset Formula $d$
\end_inset

 is fit to our test data set.
\end_layout

\begin_layout Itemize
Thus, we now need to split our dataset into three pieces:
\end_layout

\begin_deeper
\begin_layout Itemize
Training dataset (
\begin_inset Formula $\approx60\%$
\end_inset

).
\end_layout

\begin_layout Itemize
Cross-validation (CV) dataset, sometimes also called validation dataset
 (
\begin_inset Formula $\approx20\%$
\end_inset

).
 
\begin_inset Formula $m_{cv}$
\end_inset

 is the number of CV examples.
\end_layout

\begin_layout Itemize
Test dataset (
\begin_inset Formula $\approx20\%$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Can calculate the CV and test errors just as we have for the training dataset.
\end_layout

\begin_layout Itemize
For the best model selection strategy, calculate 
\begin_inset Formula $J_{cv}\left(\theta^{\left(d\right)}\right)$
\end_inset

.
 Then you can safely use 
\begin_inset Formula $J_{test}\left(\theta^{\left(d\right)}\right)$
\end_inset

 for the best model to determine the generalization error for that model.
\end_layout

\end_deeper
\begin_layout Subsection*
Bias and variance
\end_layout

\begin_layout Itemize
High bias is equivalent to underfitting the data.
\end_layout

\begin_layout Itemize
High variance is equivalent to overfitting the data.
\end_layout

\begin_layout Itemize
Idea for diagnosing bias/variance:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot training error vs.
 degree of polynomial 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Plot cross-validation error vs.
 degree of polynomial 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Training error will probably decrease with 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Cross-validation error will most likely reach a minimum at a certain value
 of 
\begin_inset Formula $d$
\end_inset

, then start increasing again.
\end_layout

\begin_layout Itemize
High bias: high training error and high CV error.
 
\begin_inset Formula $J_{train}\left(\theta\right)\approx J_{cv}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
High variance: low training error, high CV error.
\end_layout

\end_deeper
\begin_layout Itemize
Regularization can help prevent overfitting, but how does it affect bias
 and variance?
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we are fitting a high-order polynomial.
\end_layout

\begin_layout Itemize
How do we choose the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

 to minimize bias and variance?
\end_layout

\begin_deeper
\begin_layout Itemize
Try a range of 
\begin_inset Formula $\lambda$
\end_inset

 values.
\end_layout

\begin_layout Itemize
For each 
\begin_inset Formula $\lambda$
\end_inset

 value, we find the set of parameters 
\begin_inset Formula $\theta$
\end_inset

 which minimizes 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, apply this set of parameters to the cross-validation dataset and pick
 whichever 
\begin_inset Formula $\lambda$
\end_inset

 ends up giving the lowest 
\begin_inset Formula $J_{cv}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, apply this to the test set.
\end_layout

\begin_layout Itemize
This is model selection applied to the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Learning curves
\end_layout

\begin_deeper
\begin_layout Itemize
Plotting 
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $J_{cv}\left(\theta\right)$
\end_inset

 versus 
\begin_inset Formula $m$
\end_inset

 (training set size).
\end_layout

\begin_layout Itemize
Lets you see how your algorithm changes with more training examples.
\end_layout

\begin_layout Itemize
Average training error will grow with 
\begin_inset Formula $m$
\end_inset

 (easy to fit only a few data points, harder to fit many).
 Will usually start to plateau at a certain point.
\end_layout

\begin_layout Itemize
Cross-validation error will tend to decrease as 
\begin_inset Formula $m$
\end_inset

 increases.
\end_layout

\begin_layout Itemize
High bias (underfitting) case:
\end_layout

\begin_deeper
\begin_layout Itemize
Cross-validation error will initially decrease with 
\begin_inset Formula $m$
\end_inset

, but will plateau relatively quickly.
\end_layout

\begin_layout Itemize
Training error will start small and increase with 
\begin_inset Formula $m$
\end_inset

, and will eventually plateau and end up very close to the cross-validation
 error.
\end_layout

\begin_layout Itemize
If a learning algorithm has high bias, getting more training data will not
 help very much on its own.
\end_layout

\end_deeper
\begin_layout Itemize
High variance (overfitting) case:
\end_layout

\begin_deeper
\begin_layout Itemize
Training error will start small and increase very slowly with 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Itemize
Cross-validation error will start high and decrease very slowly with 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Itemize
There will be a big gap between the training and cross-validation error.
\end_layout

\begin_layout Itemize
If a learning algorithm has high variance, getting more training data is
 likely to help.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Debugging a learning algorithm
\end_layout

\begin_deeper
\begin_layout Itemize
Getting more training examples will help with high variance cases.
\end_layout

\begin_layout Itemize
Trying a smaller set of features will also help with high variance cases.
\end_layout

\begin_layout Itemize
Adding features will help with high bias problems.
\end_layout

\begin_layout Itemize
Adding polynomial features will also help with high bias problems.
\end_layout

\begin_layout Itemize
Decreasing 
\begin_inset Formula $\lambda$
\end_inset

 will help fix high bias problems.
\end_layout

\begin_layout Itemize
Increasing 
\begin_inset Formula $\lambda$
\end_inset

 will help fix high variance problems.
\end_layout

\end_deeper
\begin_layout Itemize
Neural networks and overfitting.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Small
\begin_inset Quotes erd
\end_inset

 neural network: fewer parameters, computationally cheaper, more prone to
 underfitting.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Large
\begin_inset Quotes erd
\end_inset

 neural network: more parameters, computationally more expensive, more prone
 to overfitting.
 Use regularization to address overfitting.
\end_layout

\end_deeper
\begin_layout Section*
Machine learning system design
\end_layout

\begin_layout Subsection*
Building a spam classifier
\end_layout

\begin_layout Itemize
Supervised learning: how do we want to define 
\begin_inset Formula $x$
\end_inset

, the set of features of the e-mail?
\end_layout

\begin_deeper
\begin_layout Itemize
Could choose 
\begin_inset Formula $\approx$
\end_inset

100 words indicative of spam/not spam.
 (e.g., deal, buy, discount, your name, etc.)
\end_layout

\begin_layout Itemize
Given an e-mail, parse it and record whether each word occurs or not.
\end_layout

\begin_layout Itemize
Another option is to look through a training set of spam e-mail and pick
 out the most frequently occurring words automatically, rather than choosing
 the features manually.
\end_layout

\end_deeper
\begin_layout Itemize
How to spend your time to make your algorithm have low error?
\end_layout

\begin_deeper
\begin_layout Itemize
Collect lots of data.
\end_layout

\begin_layout Itemize
Develop sophisticated features based on e-mail routing information from
 e-mail header.
\end_layout

\begin_layout Itemize
Develop sophisticated features for message body.
\end_layout

\begin_deeper
\begin_layout Itemize
Should 
\begin_inset Quotes eld
\end_inset

deal
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Dealer
\begin_inset Quotes erd
\end_inset

 be treated as the same word?
\end_layout

\begin_layout Itemize
Features about punctuation (spam uses more exclamation points).
\end_layout

\end_deeper
\begin_layout Itemize
Detect misspellings (m0rtgage, w4tches, etc.).
\end_layout

\end_deeper
\begin_layout Itemize
Recommended approach:
\end_layout

\begin_deeper
\begin_layout Itemize
Start with a simple algorithm that you can implement quickly.
\end_layout

\begin_layout Itemize
Implement it and test on your cross-validation data set.
\end_layout

\begin_layout Itemize
Plot learning curves to decide if more data, more features, etc.
 are likely to help.
\end_layout

\begin_layout Itemize
Error analysis: manually examine the examples (in the cross-validation data
 set) that your algorithm made errors on.
 See if you spot any systematic trend in what type of examples it is making
 errors on.
\end_layout

\end_deeper
\begin_layout Itemize
Example:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $m_{CV}=500$
\end_inset

 examples in CV data set.
\end_layout

\begin_layout Itemize
Algorithm misclassifies 100 e-mails.
\end_layout

\begin_layout Itemize
Manually examine the 100 errors and categorize them baesd on:
\end_layout

\begin_deeper
\begin_layout Itemize
What type of e-mail it is.
\end_layout

\begin_layout Itemize
What cues (features) you think would have helped the algorithm classify
 them correctly.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The importance of numerical evaluation
\end_layout

\begin_deeper
\begin_layout Itemize
Should discount/discounts/discounted/discounting be treated as the same
 word?
\end_layout

\begin_layout Itemize
In natural language processing, can use 
\begin_inset Quotes eld
\end_inset

stemming
\begin_inset Quotes erd
\end_inset

 software (e.g.
 
\begin_inset Quotes eld
\end_inset

porter stemmer
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Itemize
Difficult example: universe/university.
\end_layout

\begin_layout Itemize
Error analysis may not be helpful for deciding whether stemming would be
 useful.
 Best approach is to just try it and see if it works (i.e., is the cross-validatio
n error lower with or without stemming?).
\end_layout

\begin_layout Itemize
Need numerical evaluation (e.g., cross-validation error) of algorithm's performanc
e with/without stemming.
\end_layout

\begin_layout Itemize
Distinguish between upper/lower case.
\end_layout

\end_deeper
\begin_layout Subsection*
Handling skewed data
\end_layout

\begin_layout Itemize
Example: cancer classification.
\end_layout

\begin_deeper
\begin_layout Itemize
Train logistic regression model 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

; 
\begin_inset Formula $y=1$
\end_inset

 means that the patient has cancer.
\end_layout

\begin_layout Itemize
Find that we get 1% error on test set (99% correct diagnoses).
\end_layout

\begin_layout Itemize
But if only 0.50% of patients have cancer, our result is not particularly
 good.
\end_layout

\begin_layout Itemize
This is an example of skewed classes, where we have a lot more of one class
 than the other.
\end_layout

\begin_layout Itemize
In this case, classification accuracy is not very useful.
\end_layout

\end_deeper
\begin_layout Itemize
Precision/recall evaluation metric:
\end_layout

\begin_deeper
\begin_layout Itemize
Take 
\begin_inset Formula $y=1$
\end_inset

 in presence of rare class that we want to detect.
\end_layout

\begin_layout Itemize
True positive: predicted class is 1 and actual class is 1.
\end_layout

\begin_layout Itemize
True negative: predicted class is 0 and actual class is 0.
\end_layout

\begin_layout Itemize
False positive: predicted class is 1 and actual class is 0.
\end_layout

\begin_layout Itemize
False negative: predicted class is 0 and actual class is 1.
\end_layout

\begin_layout Itemize
Precision: of all patients where we predicted 
\begin_inset Formula $y=1$
\end_inset

, what fraction actually has cancer?
\end_layout

\begin_deeper
\begin_layout Itemize
Equal to number of true positives divided by the total number of predicted
 positives: 
\begin_inset Formula $N_{TP}/\left(N_{TP}+N_{FP}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Recall: of all patients that actually have cancer, what fraction did we
 correctly detect as having cancer?
\end_layout

\begin_deeper
\begin_layout Itemize
Equal to number of true positives divided by the number of actual positives:
 
\begin_inset Formula $N_{TP}/\left(N_{TP}+N_{FN}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Also called 
\series bold
sensitivity
\series default
.
\end_layout

\end_deeper
\begin_layout Itemize
This may be more useful in general for skewed classes than classification
 accuracy would be.
\end_layout

\end_deeper
\begin_layout Itemize
Example: suppose we want to predict 
\begin_inset Formula $y=1$
\end_inset

 (cancer) only if very confident.
\end_layout

\begin_deeper
\begin_layout Itemize
Could increase our prediction threshold:
\end_layout

\begin_deeper
\begin_layout Itemize
Predict 1 if 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.7$
\end_inset

.
\end_layout

\begin_layout Itemize
Predict 0 if 
\begin_inset Formula $h_{\theta}\left(x\right)<0.7$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
This will increase our precision - more of our predictions will be true.
\end_layout

\begin_layout Itemize
It will also lower the recall because we predict fewer people will have
 cancer.
\end_layout

\end_deeper
\begin_layout Itemize
Example: suppose we want to avoid missing too many cases of cancer (avoid
 false negatives).
\end_layout

\begin_deeper
\begin_layout Itemize
Could decrease our prediction threshold:
\end_layout

\begin_deeper
\begin_layout Itemize
Predict 1 if 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.3$
\end_inset

.
\end_layout

\begin_layout Itemize
Predict 0 if 
\begin_inset Formula $h_{\theta}\left(x\right)<0.3$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
This will give higher recall - we will flag almost all patients who do have
 cancer.
\end_layout

\begin_layout Itemize
But the precision will be lower - a higher fraction of the patients who
 we predict to have cancer will not actually have cancer.
\end_layout

\end_deeper
\begin_layout Itemize
Main point: you can vary the value of your threshold and plot a curve which
 shows a precision vs.
 recall threshold.
\end_layout

\begin_layout Itemize
Question: is there a way to choose this threshold optimally?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $F_{1}$
\end_inset

 score: tells us how to compare/weight precision and recall.
 (also just called F score)
\end_layout

\begin_layout Itemize
\begin_inset Formula $F_{1}=2\frac{PR}{P+R}$
\end_inset

.
 Like taking an average, but gives the lower score more weight.
\end_layout

\begin_layout Subsection*
Data for machine learning
\end_layout

\begin_layout Itemize
Designing a high-accuracy learning system
\end_layout

\begin_deeper
\begin_layout Itemize
Example: classify between confusable words (to, two, too; then, than).
\end_layout

\begin_deeper
\begin_layout Itemize
Algorithms: Perceptron (logistic regression), Winnow, memory-based, naive
 Bayes.
\end_layout

\end_deeper
\begin_layout Itemize
Test all algorithms and vary training set size - as training set size increases,
 at a certain point, most algorithms end up getting very similar results.
\end_layout

\begin_layout Itemize
Saying: 
\begin_inset Quotes eld
\end_inset

It's not who has the best algorithm that wins, it's who has the most data.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Large data rationale
\end_layout

\begin_deeper
\begin_layout Itemize
Assume features 
\begin_inset Formula $x\in R^{n+1}$
\end_inset

 has sufficient information to predict 
\begin_inset Formula $y$
\end_inset

 accurately.
\end_layout

\begin_layout Itemize
Example: for breakfast I ate _____ eggs.
 (algorithm should put in 
\emph on
two
\emph default
)
\end_layout

\begin_layout Itemize
Counterexample: predict housing price from only size (square feet) and no
 other features.
\end_layout

\begin_layout Itemize
Useful test: given the input 
\begin_inset Formula $x$
\end_inset

, can a human expert confidently predict 
\begin_inset Formula $y$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Itemize
Can help determine if we have enough information for the learning algorithm
 to do a good job.
\end_layout

\end_deeper
\begin_layout Itemize
Use a learning algorithm with many parameters (e.g., logistic/linear regression
 with many features or a neural network with many hidden units).
\end_layout

\begin_deeper
\begin_layout Itemize
This should give low bias and 
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

 should be small.
\end_layout

\end_deeper
\begin_layout Itemize
Use a very large training set (unlikely to overfit).
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $J_{train}\left(\theta\right)\approx J_{test}\left(\theta\right)$
\end_inset

, then 
\begin_inset Formula $J_{test}\left(\theta\right)$
\end_inset

 will most likely be small.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Section*
Support vector machines
\end_layout

\begin_layout Subsection*
Large margin classification
\end_layout

\begin_layout Itemize
Alternative view of logistic regression:
\end_layout

\begin_deeper
\begin_layout Itemize
Think about what we want logistic regression to do.
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

, we want 
\begin_inset Formula $h_{\theta}\left(x\right)\approx1$
\end_inset

, 
\begin_inset Formula $\theta^{T}x\gg0$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=0$
\end_inset

, we want 
\begin_inset Formula $h_{\theta}\left(x\right)\approx0$
\end_inset

, 
\begin_inset Formula $\theta^{T}x\ll0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Cost of example: 
\begin_inset Formula $-\left(y\log h_{\theta}\left(x\right)+\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

, the cost is 
\begin_inset Formula $-\log\left(h_{\theta}\left(x\right)\right)=-\log\left(\frac{1}{1+\exp\left(-\theta^{T}x\right)}\right)$
\end_inset

; as 
\begin_inset Formula $\theta^{T}x$
\end_inset

 gets large, the cost becomes small.
\end_layout

\end_deeper
\begin_layout Itemize
To make a support vector machine, we use a 
\begin_inset Quotes eld
\end_inset

simpler
\begin_inset Quotes erd
\end_inset

 cost function: instead we use two lines.
\end_layout

\begin_deeper
\begin_layout Itemize
Cost = 0 for 
\begin_inset Formula $z=\theta^{T}x>1$
\end_inset

 (for 
\begin_inset Formula $y=1$
\end_inset

).
\end_layout

\begin_layout Itemize
Cost = linear function which follows curve for 
\begin_inset Formula $z<1$
\end_inset

 (for 
\begin_inset Formula $y=1$
\end_inset

).
\end_layout

\begin_layout Itemize
Do the same thing for the case where 
\begin_inset Formula $y=0$
\end_inset

.
\end_layout

\begin_layout Itemize
This will give us a computational advantage.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Support vector machine cost function: 
\begin_inset Formula $J\left(\theta\right)=C\sum_{i=1}^{m}y^{\left(i\right)}\text{cost}_{1}\left(\theta^{T}x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\text{cost}_{0}\left(\theta^{T}x^{\left(i\right)}\right)+\frac{1}{2}\sum_{i=0}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We ignore the factor of 
\begin_inset Formula $m$
\end_inset

 that was in previous cost functions because it's just a normalization factor.
\end_layout

\begin_layout Itemize
We also pull out 
\begin_inset Formula $\lambda$
\end_inset

from the regularization term.
 
\begin_inset Formula $C=\frac{1}{\lambda}$
\end_inset

 is now the coefficient of the first term.
\end_layout

\end_deeper
\begin_layout Itemize
We could have set our threshold at 
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

 for 
\begin_inset Formula $y=1$
\end_inset

 (and similarly for 
\begin_inset Formula $y=0$
\end_inset

), but putting at 
\begin_inset Formula $\theta^{T}x\geq1$
\end_inset

 means a stronger prediction.
\end_layout

\begin_layout Itemize
Consider a case where we set 
\begin_inset Formula $C$
\end_inset

 to be a very large value, like 
\begin_inset Formula $10^{5}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
When minimizing the cost function, it will try to set the first term to
 be 0.
\end_layout

\begin_layout Itemize
This requires we find 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\geq0$
\end_inset

 for 
\begin_inset Formula $y^{\left(i\right)}=1$
\end_inset

 and 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\leq-1$
\end_inset

 for 
\begin_inset Formula $y^{\left(i\right)}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
End result is a very interesting decision boundary.
 For a linearly separable case, the decision boundary will be close to what
 appears optimal by eye.
\end_layout

\begin_deeper
\begin_layout Itemize
This 
\begin_inset Quotes eld
\end_inset

distance
\begin_inset Quotes erd
\end_inset

 between the two classes is the 
\emph on
margin
\emph default
, the SVM tries to separate the two classes with as large of a margin as
 possible.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Large margin classifiers can be sensitive to outliers.
\end_layout

\begin_deeper
\begin_layout Itemize
Can use an intermediate value for 
\begin_inset Formula $C$
\end_inset

 instead of a very large one in order to reduce the effect of outliers.
\end_layout

\end_deeper
\begin_layout Itemize
Vector stuff
\end_layout

\begin_deeper
\begin_layout Itemize
Magnitude: 
\begin_inset Formula $\left|u\right|=\sqrt{u_{1}^{2}+u_{2}^{2}}$
\end_inset


\end_layout

\begin_layout Itemize
Inner product: 
\begin_inset Formula $u\cdot v=\sum_{i}u_{i}v_{i}=u^{T}v=v^{T}u=p\left|u\right|$
\end_inset

.
 This is equal to the length of the projection of 
\begin_inset Formula $v$
\end_inset

 onto 
\begin_inset Formula $u$
\end_inset

 (
\begin_inset Formula $p$
\end_inset

) multiplied by the magnitude of 
\begin_inset Formula $u$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
SVM decision boundary
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\text{min}\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

, such that 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\geq1$
\end_inset

 if 
\begin_inset Formula $y^{\left(i\right)}=1$
\end_inset

 and 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\leq-1$
\end_inset

 if 
\begin_inset Formula $y^{\left(i\right)}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
Can think of 
\begin_inset Formula $\sum_{j=1}^{n}\theta_{j}^{2}=\left(\sqrt{\sum_{j=1}^{n}\theta_{j}^{2}}\right)^{2}$
\end_inset

, which is like the magnitude of the parameter vector theta:
\begin_inset Formula $\left|\theta\right|^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Note: we make a simplification and take 
\begin_inset Formula $\theta_{0}$
\end_inset

 to be 0 (this means that the decision boundary will pass through the origin).
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\theta^{T}x=p^{\left(i\right)}\cdot\left|\theta\right|$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Kernels
\end_layout

\begin_layout Itemize
Question: is there a different or better way to choose features?
\end_layout

\begin_layout Itemize
Example: given 
\begin_inset Formula $x$
\end_inset

, compute new features based on proximity to landmarks 
\begin_inset Formula $l^{1}$
\end_inset

, 
\begin_inset Formula $l^{2}$
\end_inset

, 
\begin_inset Formula $l^{3}$
\end_inset

 (points randomly chosen to be 
\begin_inset Quotes eld
\end_inset

important
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

landmarks
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
Given 
\begin_inset Formula $x$
\end_inset

: 
\begin_inset Formula $f_{1}=similarity\left(x,l^{1}\right)=\exp\left(-\frac{\left|x-l^{1}\right|^{2}}{2\sigma^{2}}\right)$
\end_inset

, similarly for 
\begin_inset Formula $f_{2},f_{3}$
\end_inset

.
\end_layout

\begin_layout Itemize
Here, the formula for similarity is called the 
\emph on
kernel
\emph default
.
 In this case, we use a Gaussian kernel.
 Can also be written as 
\begin_inset Formula $k\left(x,l^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Kernels and similarity
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $f_{1}=similarity\left(x,l^{1}\right)=\exp\left(-\frac{\left|x-l^{1}\right|^{2}}{2\sigma^{2}}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $x\approx l^{1}$
\end_inset

, then 
\begin_inset Formula $f_{1}\approx1$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $x$
\end_inset

 is far from 
\begin_inset Formula $l^{1}$
\end_inset

, then 
\begin_inset Formula $f_{1}\approx0$
\end_inset

.
\end_layout

\begin_layout Itemize
This feature 
\begin_inset Formula $f_{1}$
\end_inset

 measures how close 
\begin_inset Formula $x$
\end_inset

 is to the first landmark, 
\begin_inset Formula $l^{1}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma$
\end_inset

 is a parameter of the kernel.
\end_layout

\begin_deeper
\begin_layout Itemize
Larger 
\begin_inset Formula $\sigma$
\end_inset

 means a more spread out Gaussian distribution.
\end_layout

\end_deeper
\begin_layout Itemize
How do we choose the landmarks?
\end_layout

\begin_deeper
\begin_layout Itemize
Start by choosing the positions of actual samples as landmarks (1 landmark
 per sample).
\end_layout

\begin_layout Itemize
This says your features will measure how close an example is to something
 from the training set.
\end_layout

\begin_layout Itemize
For each new example, there will be 
\begin_inset Formula $m$
\end_inset

 features, one for each landmark.
\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis: given 
\begin_inset Formula $x$
\end_inset

, compute features 
\begin_inset Formula $f\in R^{m+1}$
\end_inset

, predict 
\begin_inset Formula $y=1$
\end_inset

 if 
\begin_inset Formula $\theta^{T}f\geq0$
\end_inset

.
 (
\begin_inset Formula $\theta\in R^{m+1}$
\end_inset

)
\end_layout

\begin_layout Itemize
Training: minimize cost function 
\begin_inset Formula $\text{min}C\sum_{i=1}^{m}y^{\left(i\right)}cost_{1}\left(\theta^{T}f^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)cost_{0}\left(\theta^{T}f^{\left(i\right)}\right)+\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

 (here, 
\begin_inset Formula $n=m$
\end_inset

, since the number of features is equal to the number of samples).
\end_layout

\begin_deeper
\begin_layout Itemize
Can write last term as 
\begin_inset Formula $\sum_{j=1}^{n}\theta_{j}^{2}=\theta^{T}\theta$
\end_inset

 (ignoring 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\begin_layout Itemize
Most SVMs replace this with 
\begin_inset Formula $\theta^{T}M\theta$
\end_inset

, where 
\begin_inset Formula $M$
\end_inset

 is a matrix generally chosen based on your kernel.
 This is a mathematical detail that allows the algorithm to run much more
 efficiently.
\end_layout

\end_deeper
\begin_layout Itemize
Can apply kernel method for things like logistic regression, but it doesn't
 work quite as well.
 SVMs use some computational tricks that logistic regression doesn't.
\end_layout

\begin_layout Itemize
Don't recommend to write software to minimize the SVM cost function, use
 stuff that has already been developed.
\end_layout

\begin_layout Itemize
SVM parameters:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $C=1/\lambda$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Large 
\begin_inset Formula $C$
\end_inset

: lower bias, higher variance.
\end_layout

\begin_layout Itemize
Small 
\begin_inset Formula $C$
\end_inset

: higher bias, lower variance.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Large 
\begin_inset Formula $\sigma^{2}$
\end_inset

: features 
\begin_inset Formula $f_{i}$
\end_inset

 vary more smoothly.
 Higher bias, lower variance.
\end_layout

\end_deeper
\end_deeper
\end_body
\end_document
