#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning (Stanford)
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 10/5/2015 - 12/27/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Subsection*
Supervised learning
\end_layout

\begin_layout Itemize
Say you have data which gives the price and square footage of several houses.
\end_layout

\begin_layout Itemize
You want to predict the price given some square footage as an input.
\end_layout

\begin_layout Itemize
You can fit any type of function to the data: linear, quadratic, etc.
\end_layout

\begin_layout Itemize
Supervised learning: using a dataset where the 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 answers are known.
\end_layout

\begin_layout Itemize
Regression problem: predict a continuous valued output.
\end_layout

\begin_layout Itemize
Classification problem: predict for a problem with a discrete output.
\end_layout

\begin_deeper
\begin_layout Itemize
Also want to estimate the probability associated with the prediction.
\end_layout

\end_deeper
\begin_layout Itemize
Most interesting machine learning algorithms can deal with an infinite number
 of features!
\end_layout

\begin_deeper
\begin_layout Itemize
Requires a support vector machine - we will talk about this later in the
 course.
\end_layout

\end_deeper
\begin_layout Subsection*
Unsupervised learning
\end_layout

\begin_layout Itemize
We are given data where we don't know the 
\begin_inset Quotes eld
\end_inset

right answer.
\begin_inset Quotes erd
\end_inset

 The actual data is not classified as true or false.
\end_layout

\begin_layout Itemize
The question is: here is the dataset, can you find some structure in the
 data?
\end_layout

\begin_layout Itemize
Example: clustering algorithm.
 Used in Google news to group similar stories together.
\end_layout

\begin_deeper
\begin_layout Itemize
In this method, the algorithm assigns group labels to different clusters.
\end_layout

\end_deeper
\begin_layout Itemize
Other examples: social network analysis, organization of computer clusters,
 market segmentation, astronomical data analysis.
\end_layout

\begin_layout Itemize
Example: cocktail party problem - useful for separating highly correlated
 audio tracks into independent tracks.
\end_layout

\begin_deeper
\begin_layout Itemize
Cocktail party problem algorithm: 
\family typewriter
[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
\family default
 (singular value decomposition)
\end_layout

\end_deeper
\begin_layout Section*
Linear regression with one variable
\end_layout

\begin_layout Subsection*
Model and cost function
\end_layout

\begin_layout Itemize
Regression problem: predict a (continuous output).
\end_layout

\begin_layout Itemize
General notation for this course:
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
m
\emph default
: number of examples in the training dataset.
\end_layout

\begin_layout Itemize

\emph on
x
\emph default
: input variables/features.
\end_layout

\begin_layout Itemize

\emph on
y
\emph default
: output variable/target.
\end_layout

\begin_layout Itemize

\emph on
(x,y)
\emph default
: denotes a single training example.
\end_layout

\end_deeper
\begin_layout Itemize
General flow:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set -> learning algorithm -> hypothesis function.
\end_layout

\begin_layout Itemize
The hypothesis function takes input (
\emph on
x
\emph default
) and predicts the outcome (
\emph on
y
\emph default
).
 It maps from 
\emph on
x
\emph default
's to 
\emph on
y
\emph default
's.
\end_layout

\end_deeper
\begin_layout Itemize
How do we represent the hypothesis 
\emph on
h
\emph default
?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$
\end_inset

 for univariate linear regression, or linear regression with one variable.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\theta$
\end_inset

's are the parameters of the model.
\end_layout

\end_deeper
\begin_layout Itemize
How do we choose the parameters of the model?
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 such that we minimize the sum of the squared errors: 
\begin_inset Formula $\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Factor of 
\begin_inset Formula $\frac{1}{2m}$
\end_inset

 doesn't affect parameter values and will make later math a bit easier.
\end_layout

\begin_layout Itemize
This also defined as the cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
There are other cost functions that may work as well, but the squared error
 cost function is the most commonly used one for linear regression.
\end_layout

\end_deeper
\begin_layout Itemize
Can be useful to plot cost function in terms of the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
If we assume an intercept of 0 (
\begin_inset Formula $\theta_{0}=0$
\end_inset

), the value of 
\begin_inset Formula $\theta_{1}$
\end_inset

 that minimizes the cost function is 
\begin_inset Formula $\theta_{1}=\frac{\sum_{i}x^{\left(i\right)}y^{\left(i\right)}}{\sum_{i}x^{2\left(i\right)}}$
\end_inset


\end_layout

\begin_layout Itemize
For two parameters,
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{1}=\frac{\sum_{i=1}^{m}x^{\left(i\right)}y^{\left(i\right)}-\bar{x}\bar{y}}{\sum_{i=1}^{m}x^{\left(i\right)2}-\bar{x}^{2}}$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{0}=\bar{y}-\theta_{1}\bar{x}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Gradient descent
\end_layout

\begin_layout Itemize
Have some cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 that we want to minimize (find 
\begin_inset Formula $\underset{\theta_{0,}\theta_{1}}{\text{min}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Outline:
\end_layout

\begin_deeper
\begin_layout Itemize
Start with some 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
Keep changing 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

 to reduce 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 until we hopefully end up at a minimum.
\end_layout

\end_deeper
\begin_layout Itemize
Definition of gradient descent algorithm: 
\family typewriter
repeat until convergence { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 }.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $:=$
\end_inset

: assignment operator.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

: learning rate.
 Controls step size.
\end_layout

\end_deeper
\begin_layout Itemize
When actually programming this, make sure you change the parameters at the
 same time and then update them at the same time! Otherwise you may incorrectly
 use new values of the parameters to calculate the cost function.
\end_layout

\end_deeper
\begin_layout Itemize
Don't need to adjust 
\begin_inset Formula $\alpha$
\end_inset

 over time because the derivative term will get smaller as it approaches
 a local minimum.
\end_layout

\begin_layout Itemize
Applying gradient descent to our simple cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)=\frac{\partial}{\partial\theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}\left(\theta_{0}+\theta_{1}x^{\left(i\right)}-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=0$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{0}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=1$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{1}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Batch gradient descent: used in machine learning, each step of gradient
 descent uses all of the training examples.
\end_layout

\begin_layout Section*
Linear regression with multiple variables
\end_layout

\begin_layout Itemize
Also called multivariate linear regression.
\end_layout

\begin_layout Itemize
We now have multiple features (or predictors) that we want to use to develop
 a hypothesis function and make a prediction.
\end_layout

\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

: number of features.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: input (features) of 
\begin_inset Formula $i$
\end_inset

th training example.
 This is a vector of features in the 
\begin_inset Formula $i$
\end_inset

th observation.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{j}^{\left(i\right)}$
\end_inset

: value of feature 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $i$
\end_inset

th training example.
\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis function now has a new form:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$
\end_inset


\end_layout

\begin_layout Itemize
For convenience of notation, we can define 
\begin_inset Formula $x_{0}^{\left(i\right)}=1$
\end_inset

 so that 
\begin_inset Formula $h_{\theta}\left(x\right)=\sum_{i=1}^{n}\theta_{i}x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
We can also use linear algebra:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x=\left[x_{0},x_{1},...,x_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{0},\theta_{1},...,\theta_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta x^{T}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Gradient descent with multiple variables
\end_layout

\begin_layout Itemize
Write set of parameters as 
\begin_inset Formula $\theta$
\end_inset

 and cost function as 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Gradient descent: 
\family typewriter
repeat { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

 } 
\family default
(simultaneously update for each 
\begin_inset Formula $j=0,1,...,n$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Tips for gradient descent:
\end_layout

\begin_deeper
\begin_layout Itemize
Feature scaling - make sure that different features taken on similar ranges
 of values.
\end_layout

\begin_deeper
\begin_layout Itemize
Ideally, all features will be in the range 
\begin_inset Formula $-1\leq x\leq1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Mean normalization: normalize a feature to have a mean of zero.
 When doing this, you just replace a feature 
\begin_inset Formula $x_{i}$
\end_inset

 with 
\begin_inset Formula $x_{i}-\mu_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Do not apply this to 
\begin_inset Formula $x_{0}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Can also normalize by standard deviation: 
\begin_inset Formula $x_{i}\rightarrow\frac{x_{i}-\mu_{i}}{s_{i}}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Making sure that gradient descent is working properly:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot 
\begin_inset Formula $\underset{\theta}{\text{min}}J\left(\theta\right)$
\end_inset

 vs.
 number of iterations.
 It should decrease after every iteration for sufficiently small 
\begin_inset Formula $\alpha$
\end_inset

 (can be shown mathematically).
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too large, you may continue overshooting the minimum and never be able
 to actually reach it.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too small, convergence will happen, but very slowly.
\end_layout

\end_deeper
\begin_layout Itemize
Automatic convergence tests: algorithms that tell you whether gradient descent
 has converged:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: declare convergence if 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 decreases by less than some value (like 
\begin_inset Formula $10^{-3}$
\end_inset

) in one iteration.
\end_layout

\end_deeper
\begin_layout Subsection*
Polynomial regression
\end_layout

\begin_layout Itemize
Can create new features from existing ones.
\end_layout

\begin_deeper
\begin_layout Itemize
Example: area = frontage * depth.
\end_layout

\end_deeper
\begin_layout Itemize
Polynomial model: 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$
\end_inset

 for a quadratic function.
\end_layout

\begin_deeper
\begin_layout Itemize
Important to use feature scaling since when you square or cube features,
 the range becomes much larger.
\end_layout

\end_deeper
\begin_layout Subsection*
Normal equation
\end_layout

\begin_layout Itemize
Method to solve for 
\begin_inset Formula $\theta$
\end_inset

 analytically.
\end_layout

\begin_layout Itemize
How to go about it:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)=...=0$
\end_inset

, then solve for every 
\begin_inset Formula $\theta_{j}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Linear algebra method:
\end_layout

\begin_deeper
\begin_layout Itemize
Put all features into a matrix: 
\begin_inset Formula $X=\begin{array}{ccccc}
1 & 2104 & 5 & 1 & 45\\
1 & 1416 & 3 & 2 & 40\\
1 & 1534 & 3 & 2 & 30\\
1 & 852 & 2 & 1 & 36
\end{array}$
\end_inset

 (including a column of ones for 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\begin_layout Itemize
Put all 
\begin_inset Formula $y_{j}$
\end_inset

s into a vector: 
\begin_inset Formula $Y=\begin{array}{c}
460\\
232\\
315\\
178
\end{array}$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
In Matlab/Octave: 
\family typewriter
pinv(X'*X)*X'*Y
\end_layout

\begin_layout Itemize
Feature scaling is not needed for the normal equation method.
\end_layout

\end_deeper
\begin_layout Itemize
Normal equation vs.
 gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Pros:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: no need to choose 
\begin_inset Formula $\alpha$
\end_inset

, no need to iterate.
\end_layout

\begin_layout Itemize
Gradient descent: works well even when the number of features is large.
\end_layout

\end_deeper
\begin_layout Itemize
Cons:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: slow if number of features is very large, need to compute
 
\begin_inset Formula $\left(X^{T}X\right)^{-1}$
\end_inset


\end_layout

\begin_layout Itemize
Gradient descent: need to choose 
\begin_inset Formula $\alpha$
\end_inset

, needs many iterations.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation non-invertibility: what if 
\begin_inset Formula $\left(X^{T}X\right)$
\end_inset

 is non-invertible? Also known as 
\begin_inset Quotes eld
\end_inset

singular
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

degenerate.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Can occur when there are redundant features (linearly dependent) or if there
 are too many features.
\end_layout

\begin_deeper
\begin_layout Itemize
Possible solution: delete some features or use regularization.
\end_layout

\end_deeper
\begin_layout Itemize
If you use 
\family typewriter
pinv
\family default
 in Matlab/Octave instead of 
\family typewriter
inv
\family default
, it should handle singular cases for you.
\end_layout

\end_deeper
\begin_layout Section*
Logistic Regression
\end_layout

\begin_layout Subsection*
Classification and Representation
\end_layout

\begin_layout Itemize
Classification: putting things into discrete categories.
 Binary classification is when there are only two categories.
\end_layout

\begin_layout Itemize
Linear regression is not usually a very good solution for classification
 problems.
 Examples where we can clearly use some sort of threshold for classification
 make this clear.
\end_layout

\begin_layout Itemize
Logistic regression has the property that the predictions of the hypothesis
 function are always between 0 and 1: 
\begin_inset Formula $0\leq h_{\theta}\left(x\right)\leq1$
\end_inset

.
\end_layout

\begin_layout Itemize
For linear regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta^{T}x$
\end_inset

.
\end_layout

\begin_layout Itemize
For logistic regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta^{T}x\right)$
\end_inset

, where 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is the sigmoid or logistic function, 
\begin_inset Formula $g\left(z\right)=\frac{1}{1+e^{-z}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Thus, 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+\exp\left(-\theta^{T}x\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
As before, we need to fit the parameters 
\begin_inset Formula $\theta$
\end_inset

 to our data.
\end_layout

\end_deeper
\begin_layout Itemize
Interpretation of hypothesis output:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

: estimated probability that 
\begin_inset Formula $y=1$
\end_inset

 (i.e., positive result) on input 
\begin_inset Formula $x$
\end_inset

; i.e.
 
\begin_inset Formula $h_{\theta}\left(x\right)=p\left(y=1|x;\theta\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Decision boundary:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we predict that 
\begin_inset Formula $y=1$
\end_inset

 when 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 and 
\begin_inset Formula $y=0$
\end_inset

 when 
\begin_inset Formula $h_{\theta}<0.5$
\end_inset

.
\end_layout

\begin_layout Itemize
We see that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is 
\begin_inset Formula $\geq0.5$
\end_inset

 when 
\begin_inset Formula $z\geq0$
\end_inset

, thus 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 whenever 
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

.
\end_layout

\begin_layout Itemize
This essentially defines a threshold for making predictions with our model.
\end_layout

\begin_layout Itemize
The decision boundary is a property of the hypothesis and its parameters,
 not a property of the dataset.
\end_layout

\end_deeper
\begin_layout Itemize
Non-linear decision boundaries:
\end_layout

\begin_deeper
\begin_layout Itemize
Can use higher-order polynomials terms in logistic regression.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{2}^{2}+\theta_{5}x_{1}x_{2}+\theta_{6}x_{1}^{3}+...\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Logistic Regression Model
\end_layout

\begin_layout Itemize
Cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
Linear regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}=\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Logistic regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What cost function here should we use here?
\end_layout

\begin_layout Itemize
We can't use the same thing that we used for linear regression because it
 will be a non-convex function - thus, gradient descent will not necessarily
 converge to the global minimum.
\end_layout

\begin_layout Itemize
Instead, we use 
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=\left\{ \begin{array}{l}
-\log\left(h_{\theta}\left(x\right)\right)\text{ if }y=1\\
-\log\left(1-h_{\theta}\left(x\right)\right)\text{ if }y=0
\end{array}\right.$
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=1$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow0$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=0$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=0$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow1$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Simplified way of writing the cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=-y\log\left(h_{\theta}\left(x\right)\right)-\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)$
\end_inset


\end_layout

\begin_layout Itemize
Note: 
\begin_inset Formula $y$
\end_inset

 is always equal to 0 or 1.
\end_layout

\begin_layout Itemize
So the full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent: we do the same thing as before, we just have a new cost
 function.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{j}$
\end_inset

 := 
\begin_inset Formula $\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
This is the same as for linear regression, although our hypothesis function
 has changed.
\end_layout

\begin_layout Itemize
Make sure to simultaneously update all 
\begin_inset Formula $\theta_{j}$
\end_inset

!
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization:
\end_layout

\begin_deeper
\begin_layout Itemize
We have some cost function 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and we want the set of parameters
\begin_inset Formula $\theta$
\end_inset

 that gives the minimum of 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $\theta$
\end_inset

, we have code that can compute 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Optimization algorithms:
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descent
\end_layout

\begin_layout Itemize
Conjugate gradient
\end_layout

\begin_layout Itemize
BFGS
\end_layout

\begin_layout Itemize
L-BFGS
\end_layout

\end_deeper
\begin_layout Itemize
Advantages: no need to manually pick the learning rate 
\begin_inset Formula $\alpha$
\end_inset

, often faster than gradient descent.
\end_layout

\begin_layout Itemize
Disadvantages: more complex, can cause implementations to be error prone.
\end_layout

\begin_layout Itemize
Can use 
\family typewriter
fminunc
\family default
 in Matlab/Octave.
\end_layout

\begin_deeper
\begin_layout Itemize
Define a function which returns the cost and gradient.
\end_layout

\begin_layout Itemize
Example:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% function
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

jVal = (theta(1)-5)^2 + (theta(2)-5)^2;
\end_layout

\begin_layout Plain Layout

gradient(1) = 2*(theta(1)-5);
\end_layout

\begin_layout Plain Layout

gradient(2) = 2*(theta(2)-5);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Setup
\end_layout

\begin_layout Plain Layout

options = optimset('GradObj','on','MaxIter','100');
\end_layout

\begin_layout Plain Layout

initialTheta = zeros(2,1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Call fminunc
\end_layout

\begin_layout Plain Layout

[optTheta, functionVal, exitFlag] = ...
\end_layout

\begin_layout Plain Layout

	fminunc(@costFunction, initialTheta, options);
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Multi-class classification
\end_layout

\begin_layout Itemize
Examples of this type of problem:
\end_layout

\begin_deeper
\begin_layout Itemize
Email tagging into different folders.
\end_layout

\begin_layout Itemize
Medical diagnosis.
\end_layout

\begin_layout Itemize
Weather category.
\end_layout

\end_deeper
\begin_layout Itemize
One-vs-all classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: a training dataset with three classes.
\end_layout

\begin_layout Itemize
First: is it in class 1, or class 2/3?
\end_layout

\begin_layout Itemize
Second: is it in class 2, or class 1/3?
\end_layout

\begin_layout Itemize
Third: is it in class 3, or class 1/2?
\end_layout

\begin_layout Itemize
Use standard logistic regression for each step.
\end_layout

\begin_layout Itemize
Each gives a hypothesis: 
\begin_inset Formula $h_{\theta}^{\left(i\right)}\left(x\right)=P\left(y=i|x;\theta\right)$
\end_inset

, 
\begin_inset Formula $\left(i=1,2,3\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The hypothesis tells us the probability that 
\begin_inset Formula $y$
\end_inset

 is in class 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
To make a prediction on a new input 
\begin_inset Formula $x$
\end_inset

, pick the class 
\begin_inset Formula $i$
\end_inset

 that has the maximum probability.
\end_layout

\end_deeper
\begin_layout Section*
Regularization and overfitting
\end_layout

\begin_layout Itemize
Overfitting: similar number of parameters to data points.
 This allows the model to fit the training dataset very well, however, it
 will not make accurate predictions for new examples.
 Also called 
\begin_inset Quotes eld
\end_inset

high variance.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Occurs when you have a lot of features and not a lot of training data.
\end_layout

\begin_layout Itemize
Underfitting may occur when our model has too few parameters to properly
 model the data.
 In this situation, the model has 
\begin_inset Quotes eld
\end_inset

high bias.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Options for dealing with overfitting:
\end_layout

\begin_deeper
\begin_layout Itemize
Reduce number of features: can manually select which features to keep, or
 use a model selection algorithm to determine this (will be covered later
 in the course).
\end_layout

\begin_layout Itemize
Regularization: keep all of the features, but reduce the magnitude/values
 of the parameters 
\begin_inset Formula $\theta_{j}$
\end_inset

.
 Works well when we have a lot of features, each of which contributes a
 little bit to predicting 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection*
Regularization
\end_layout

\begin_layout Itemize
Suppose we penalize and make 
\begin_inset Formula $\theta_{3},\theta_{4}$
\end_inset

 very small.
 We do this by adjusting our cost function by adding in terms like 
\begin_inset Formula $1000\theta_{3}^{2}+1000\theta_{4}^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This will give values of 
\begin_inset Formula $\theta_{3}$
\end_inset

 and 
\begin_inset Formula $\theta_{4}$
\end_inset

 which are close to zero when the cost function is small.
\end_layout

\end_deeper
\begin_layout Itemize
General idea behind regularization:
\end_layout

\begin_deeper
\begin_layout Itemize
Small values for parameters 
\begin_inset Formula $\theta_{0},\theta_{1},...,\theta_{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Simpler hypothesis, less prone to overfitting.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example: housing price prediction.
\end_layout

\begin_deeper
\begin_layout Itemize
Features: 
\begin_inset Formula $x_{1},x_{2},...,x_{100}$
\end_inset


\end_layout

\begin_layout Itemize
Parameters: 
\begin_inset Formula $\theta_{0},\theta_{1},\theta_{2},...,\theta_{100}$
\end_inset


\end_layout

\begin_layout Itemize
We don't know which parameters to try to shrink 
\emph on
a priori
\emph default
.
\end_layout

\begin_layout Itemize
Take our cost function 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

 and add a new term to shrink 
\emph on
all
\emph default
 of our parameters except 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}+\lambda\sum_{j=1}^{n}\theta_{j}^{2}\right]$
\end_inset


\end_layout

\begin_layout Itemize
The first part in brackets captures the goal of fitting the data well.
\end_layout

\begin_layout Itemize
The second part captures the goal of keeping the parameters 
\begin_inset Formula $\theta$
\end_inset

 small.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 is the regularization parameter and controls the trade-off between these
 two goals.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\lambda$
\end_inset

 is too large, we w
\end_layout

\begin_layout Itemize
on't even fit the training dataset well.
 All of the parameters 
\begin_inset Formula $\theta$
\end_inset

 will go to zero except 
\begin_inset Formula $\theta_{0}$
\end_inset

, so we will be fitting a flat line to our data.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Regularized linear regression
\end_layout

\begin_layout Itemize
Recall that we don't want to shrink 
\begin_inset Formula $\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The term 
\begin_inset Formula $1-\alpha\frac{\lambda}{m}$
\end_inset

 is usually slightly less than 1 (assuming that your learning rate is small).
 This term shrinks your 
\begin_inset Formula $\theta_{j}$
\end_inset

, then the other term is just the original gradient descent term.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation
\end_layout

\begin_deeper
\begin_layout Itemize
Before, we had 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
With our new, regularized cost function, you can derive the minimum of the
 cost function by taking derivatives with respect to each parameter, as
 before.
\end_layout

\begin_layout Itemize
Now, we get 
\begin_inset Formula $\theta=\left(X^{T}X+\lambda Z\right)^{-1}X^{T}Y$
\end_inset

, where 
\begin_inset Formula $Z$
\end_inset

 is the identity matrix, except the top left entry is 0 instead of 1 (because
 we don't regularize 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Non-invertibility
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have fewer examples than features (
\begin_inset Formula $m\leq n$
\end_inset

).
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

 will give a strange answer since 
\begin_inset Formula $X^{T}X$
\end_inset

 is non-invertible/singular (make sure to use 
\family typewriter
pinv
\family default
 instead of 
\family typewriter
inv
\family default
).
\end_layout

\begin_layout Itemize
However, if 
\begin_inset Formula $\lambda>0$
\end_inset

 , we can prove that the matrix 
\begin_inset Formula $X^{T}X+\lambda Z$
\end_inset

 is invertible!
\end_layout

\end_deeper
\begin_layout Subsection*
Regularized logistic regression
\end_layout

\begin_layout Itemize
To modify our cost function to use regularization, we add a term 
\begin_inset Formula $\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
The full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\left[\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Recall that although this looks the same as for regularized linear regression,
 the hypothesis function is different for logistic regression.
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization methods:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% Define a function to compute the cost.
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

	jVal = %code to compute J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(1) = %code to compute d/dtheta_0 J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(2) = %code to compute d/dtheta_1 J(theta);
\end_layout

\begin_layout Plain Layout

	...
\end_layout

\begin_layout Plain Layout

	gradient(n+1) = %code to compute d/dtheta_n J(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% then, we pass this function to fminunc as before.
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Neural networks: representation
\end_layout

\begin_layout Subsection*
Motivation
\end_layout

\begin_layout Itemize
Used for solving complex, non-linear hypotheses.
\end_layout

\begin_layout Itemize
For many predictors, including even quadratic terms can require a huge number
 of terms (grows roughly as 
\begin_inset Formula $n^{2}$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
One option is to include only a subset of higher-order terms, but it will
 probably underfit the data.
\end_layout

\end_deeper
\begin_layout Itemize
Neural networks: algorithms that try to mimic the brain.
\end_layout

\begin_layout Itemize
Recent resurgence: state-of-the-art technique for many computational application
s.
\end_layout

\begin_layout Itemize
Computationally more expensive.
\end_layout

\begin_layout Subsection*
Neural networks
\end_layout

\begin_layout Itemize
Developed as simulating networks of neurons in the brain.
\end_layout

\begin_layout Itemize
Neuron structure:
\end_layout

\begin_deeper
\begin_layout Itemize
Dendrites: 
\begin_inset Quotes eld
\end_inset

input wires,
\begin_inset Quotes erd
\end_inset

 receive inputs from other locations.
\end_layout

\begin_layout Itemize
Axon: 
\begin_inset Quotes eld
\end_inset

output wire,
\begin_inset Quotes erd
\end_inset

 used to send signals to other neurons.
\end_layout

\end_deeper
\begin_layout Itemize
Neuron model: logistic unit.
\end_layout

\begin_deeper
\begin_layout Itemize
We use a model of what a neuron does: it takes some number of inputs (
\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset

), does some computation, and sends an output 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^{T}x}}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This is a sigmoid (logistic) activation function.
\end_layout

\begin_layout Itemize
The parameters 
\begin_inset Formula $\theta$
\end_inset

 are sometimes referred to as 
\begin_inset Quotes eld
\end_inset

weights.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x_{0}$
\end_inset

 is sometimes referred to as the 
\begin_inset Quotes eld
\end_inset

bias unit.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Neural network: divide into layers.
\end_layout

\begin_deeper
\begin_layout Itemize
Layer 1: 
\begin_inset Quotes eld
\end_inset

input layer,
\begin_inset Quotes erd
\end_inset

 made up of input features.
\end_layout

\begin_layout Itemize
Layer 2: some set of neurons, called 
\begin_inset Quotes eld
\end_inset

hidden layers.
\begin_inset Quotes erd
\end_inset

 Can be several hidden layers.
\end_layout

\begin_layout Itemize
Final layer: 
\begin_inset Quotes eld
\end_inset

output layer,
\begin_inset Quotes erd
\end_inset

 outputs final value computed by a hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $a_{i}^{\left(j\right)}$
\end_inset

: 
\begin_inset Quotes eld
\end_inset

activation
\begin_inset Quotes erd
\end_inset

 of unit 
\begin_inset Formula $i$
\end_inset

 in layer 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

: matrix of weights controlling function mapping from layer 
\begin_inset Formula $j$
\end_inset

 to layer 
\begin_inset Formula $j+1$
\end_inset

.
\end_layout

\begin_layout Itemize
If a network has 
\begin_inset Formula $s_{j}$
\end_inset

 units in layer 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $s_{j+1}$
\end_inset

units in layer 
\begin_inset Formula $j+1$
\end_inset

, then 
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

 will be of dimension 
\begin_inset Formula $s_{j+1}\times\left(s_{j}+1\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $a_{1}^{\left(2\right)}=g\left(\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Can write 
\begin_inset Formula $z_{1}^{\left(2\right)}=\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}$
\end_inset

; i.e.
 a row of 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Vectorized implementation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $z^{\left(2\right)}=\Theta^{\left(1\right)}x=\Theta^{\left(1\right)}a^{\left(1\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(2\right)}=g\left(z^{\left(2\right)}\right)$
\end_inset

 (here 
\begin_inset Formula $z^{\left(2\right)}$
\end_inset

 is a column vector of 
\begin_inset Formula $z_{1}^{\left(2\right)},...,z_{3}^{\left(2\right)}$
\end_inset

.
 Note that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is applied element-wise.
\end_layout

\begin_layout Itemize
Add 
\begin_inset Formula $a_{0}^{\left(2\right)}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $z^{\left(3\right)}=\Theta^{\left(2\right)}a^{\left(2\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\Theta}\left(x\right)=a^{\left(3\right)}=g\left(z^{\left(3\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
We can think of the way a neural network works as though it is 
\begin_inset Quotes eld
\end_inset

learning its own features.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The function mapping 
\begin_inset Formula $a^{\left(1\right)}$
\end_inset

 to 
\begin_inset Formula $a^{\left(2\right)}$
\end_inset

 is what changes the input features to new features which go into the hidden
 layers.
\end_layout

\end_deeper
\begin_layout Itemize
You can have neural networks with other types of layouts.
\end_layout

\begin_deeper
\begin_layout Itemize
The layout is referred to as the 
\begin_inset Quotes eld
\end_inset

architechture.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Applications
\end_layout

\begin_layout Itemize
Can set up a neural network which acts as a logical filter (AND, OR, XOR,
 NOT, etc.).
\end_layout

\begin_layout Itemize
Interesting because sometimes these functions (XOR, for example) require
 non-linear decision boundaries.
\end_layout

\begin_layout Itemize
Multi-class classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Build a neural network with as many outputs as there are classes.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 is a vector, and we take the largest entry to be the predicted class.
\end_layout

\end_deeper
\begin_layout Section*
Neural networks: learning
\end_layout

\begin_layout Subsection*
Cost function and backpropagation
\end_layout

\begin_layout Itemize
\begin_inset Formula $L$
\end_inset

: number of layers in the network.
\end_layout

\begin_layout Itemize
\begin_inset Formula $s_{l}$
\end_inset

: number of units (not counting bias unit) in layer 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Itemize
Binary classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Labels 
\begin_inset Formula $y$
\end_inset

 are either 0 or 1.
\end_layout

\begin_layout Itemize
One output unit (
\begin_inset Formula $K=1$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Multi-class classification:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 classes, 
\begin_inset Formula $y\in R^{K}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 output units.
\end_layout

\end_deeper
\begin_layout Itemize
Cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
Logistic regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\left[\sum_{i=1}^{m}y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Neural network: 
\begin_inset Formula $h_{\Theta}\left(x\right)\in R^{K}$
\end_inset

 (
\begin_inset Formula $K$
\end_inset

-dimensional vector), 
\begin_inset Formula $\left(h_{\Theta}\left(x\right)\right)_{i}=i$
\end_inset

th output.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(\Theta\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{\left(i\right)}\log\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}+\left(1-y_{k}^{\left(i\right)}\right)\log\left(1-\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}\right)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{\left(l\right)}\right)^{2}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Gradient computation
\end_layout

\begin_deeper
\begin_layout Itemize
One training example 
\begin_inset Formula $\left(x,y\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Forward propagation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $a^{\left(1\right)}=x$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(2\right)}=g\left(z^{\left(2\right)}\right)=g\left(\Theta^{\left(1\right)}a^{\left(1\right)}\right)$
\end_inset

 (add 
\begin_inset Formula $a_{0}^{\left(2\right)}$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(3\right)}=g\left(z^{\left(3\right)}\right)=g\left(\Theta^{\left(2\right)}a^{\left(2\right)}\right)$
\end_inset

 (add 
\begin_inset Formula $a_{0}^{\left(3\right)}$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(4\right)}=h_{\Theta}\left(x\right)=g\left(z^{\left(4\right)}\right)=g\left(\Theta^{\left(3\right)}a^{\left(3\right)}\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Then, we use backpropagation:
\end_layout

\begin_deeper
\begin_layout Itemize
Intuition: 
\begin_inset Formula $\delta_{j}^{\left(l\right)}=$
\end_inset

 
\begin_inset Quotes eld
\end_inset

error
\begin_inset Quotes erd
\end_inset

 of node 
\begin_inset Formula $j$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Itemize
For each output unit (layer 
\begin_inset Formula $L=4$
\end_inset

), 
\begin_inset Formula $\delta_{j}^{\left(4\right)}=a_{j}^{\left(4\right)}-y_{j}$
\end_inset

.
 (difference between the hypothesis and the actual classification)
\end_layout

\begin_layout Itemize
Vectorized version: 
\begin_inset Formula $\delta^{\left(4\right)}=a^{\left(4\right)}-y$
\end_inset

, each of these is a vector with number of elements equal to the number
 of output units 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta^{\left(3\right)}=\left(\Theta^{\left(3\right)}\right)^{T}\delta^{\left(4\right)}.*g'\left(z^{\left(3\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta^{\left(2\right)}=\left(\Theta^{\left(2\right)}\right)^{T}\delta^{\left(3\right)}.*g'\left(z^{\left(2\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
No 
\begin_inset Formula $\delta^{\left(1\right)}$
\end_inset

 for this example.
\end_layout

\begin_layout Itemize
Notes: 
\begin_inset Formula $g'\left(z^{\left(i\right)}\right)=a^{\left(i\right)}.*\left(1-a^{\left(i\right)}\right)=g\left(z^{\left(i\right)}\right).*\left(1-g\left(z^{\left(i\right)}\right)\right)$
\end_inset

, and 
\begin_inset Formula $.*$
\end_inset

 indicates element-wise multiplication.
\end_layout

\begin_layout Itemize
It is possible to prove that the partial derivative terms we want are given
 by 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset

 (without regularization).
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
Backpropagation algorithm:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\left\{ \left(x^{\left(1\right)},y^{\left(1\right)}\right),...,\left(x^{\left(m\right)},y^{\left(m\right)}\right)\right\} $
\end_inset


\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}=0$
\end_inset

 for all 
\begin_inset Formula $l,i,j$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $i=1:m$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Set 
\begin_inset Formula $a^{\left(1\right)}=x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Perform forward propagation to compute 
\begin_inset Formula $a^{\left(l\right)}$
\end_inset

 for 
\begin_inset Formula $l=2,3,...,L$
\end_inset

.
\end_layout

\begin_layout Itemize
Using 
\begin_inset Formula $y^{\left(i\right)}$
\end_inset

, compute 
\begin_inset Formula $\delta^{\left(L\right)}=a^{\left(L\right)}-y^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Compute 
\begin_inset Formula $\delta^{\left(L-1\right)},\delta^{\left(L-2\right)},...,\delta^{\left(2\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}:=\Delta_{ij}^{\left(l\right)}+a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Vectorized form: 
\begin_inset Formula $\Delta^{\left(l\right)}:=\Delta^{\left(l\right)}+\delta^{\left(l+1\right)}\left(a^{\left(l\right)}\right)^{T}$
\end_inset


\end_layout

\begin_layout Itemize
After the for loop, we compute:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\frac{1}{m}\Delta_{ij}^{\left(l\right)}+\lambda\Theta_{ij}^{\left(l\right)}$
\end_inset

, if 
\begin_inset Formula $j\neq0$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\frac{1}{m}\Delta_{ij}^{\left(l\right)}$
\end_inset

, if 
\begin_inset Formula $j=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Can show that 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=D_{ij}^{\left(l\right)}$
\end_inset

, so we can use this in gradient descent or other optimization algorithms.
\end_layout

\end_deeper
\begin_layout Itemize
What is backpropagation doing?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\delta_{j}^{\left(l\right)}$
\end_inset

: 
\begin_inset Quotes eld
\end_inset

error
\begin_inset Quotes erd
\end_inset

 of cost for 
\begin_inset Formula $a_{j}^{\left(l\right)}$
\end_inset

 (unit 
\begin_inset Formula $j$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

).
 I.e., 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta_{j}^{\left(l\right)}=\frac{\partial}{\partial z_{j}^{\left(l\right)}}\text{cost}\left(i\right)$
\end_inset

 for 
\begin_inset Formula $j\geq0$
\end_inset

, where cost
\begin_inset Formula $\left(i\right)=y^{\left(i\right)}\log h_{\Theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log h_{\Theta}\left(x^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
These are a measure of how much we would like to change the neural network's
 weights so as to change the intermediate values of the computation, and
 thus change the final outputs, and the cost.
\end_layout

\end_deeper
\begin_layout Subsection*
Backpropagation in practice
\end_layout

\begin_layout Itemize
Unrolling parameters from matrices into vectors:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% Let's say you defined a function for computing the cost function
\end_layout

\begin_layout Plain Layout

% Here jVal is a number, and theta and grad are vectors in R^{n+1}
\end_layout

\begin_layout Plain Layout

function [jVal, grad] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

% You will pass it to fminunc as:
\end_layout

\begin_layout Plain Layout

optTheta = fminunc(@costFunction, initialTheta, options)
\end_layout

\begin_layout Plain Layout

% For a neural network, your parameters Theta are matrices.
\end_layout

\begin_layout Plain Layout

% The gradients D are also matrices.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Example: a neural net with 10 inputs, a hidden layer of 10 nodes, and 1
 output.
 I.e., 
\begin_inset Formula $s_{1}=10$
\end_inset

, 
\begin_inset Formula $s_{2}=10$
\end_inset

, 
\begin_inset Formula $s_{3}=1$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\Theta^{\left(1\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}\in R^{1\times11}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $D^{\left(1\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}\in R^{1\times11}$
\end_inset


\end_layout

\begin_layout Itemize
To unroll them, you can do: 
\family typewriter
thetaVec = [Theta1(:); Theta2(:); Theta3(:)];
\family default
, and the same for D.
\end_layout

\begin_layout Itemize
To go back to the matrix representation, you can do 
\family typewriter
Theta1 = reshape(thetaVec(1:110),10,11); Theta2 = reshape(thetaVec(111:220),10,1
1); Theta3 = reshape(thetaVec(221:231),1,11);
\end_layout

\end_deeper
\begin_layout Itemize
Example: have initial parameters 
\begin_inset Formula $\Theta^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Unroll to get 
\family typewriter
initialTheta
\family default
 to pass to 
\family typewriter
fminunc(@costFunction, initialTheta, options)
\end_layout

\begin_layout Itemize
Then we change our cost function:
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
function [jVal, gradientVec] = costFunction(thetaVec)
\end_layout

\begin_layout Itemize
From 
\family typewriter
thetaVec
\family default
, get 
\begin_inset Formula $\Theta^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Use forward propagation and backpropagation to compute 
\begin_inset Formula $D^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}$
\end_inset

 and 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Unroll 
\begin_inset Formula $D^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}$
\end_inset

 to get 
\family typewriter
gradientVec
\family default
.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Downside of backpropagation - easy to have many subtle bugs in your implementati
on.
\end_layout

\begin_deeper
\begin_layout Itemize
However, there is something called 
\begin_inset Quotes eld
\end_inset

gradient checking
\begin_inset Quotes erd
\end_inset

 which allows us to catch almost all of these bugs.
\end_layout

\end_deeper
\begin_layout Itemize
Gradient checking
\end_layout

\begin_deeper
\begin_layout Itemize
Numerical estimate of gradients: 
\begin_inset Formula $\frac{\partial}{\partial\theta}J\left(\theta\right)\approx\frac{J\left(\theta+\epsilon\right)-J\left(\theta-\epsilon\right)}{2\epsilon}$
\end_inset

.
 Usually, we take 
\begin_inset Formula $\epsilon$
\end_inset

 to be very small, something like 
\begin_inset Formula $10^{-4}$
\end_inset

.
\end_layout

\begin_layout Itemize
Implementation in Matlab: 
\family typewriter
gradApprox = (J(theta + eps) - J(theta - eps))/(2*eps);
\end_layout

\end_deeper
\begin_layout Itemize
Gradient checking with a parameter vector 
\begin_inset Formula $\theta\in R^{n}$
\end_inset

 (
\begin_inset Formula $\theta$
\end_inset

 is the unrolled version of 
\begin_inset Formula $\Theta^{\left(1\right)},\Theta^{\left(2\right)}$
\end_inset

, etc.).
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{1},\theta_{2},...,\theta_{n}\right]$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{i}}J\left(\theta\right)\approx\frac{J\left(\theta_{1},...,\theta_{i}+\epsilon,...,\theta_{n}\right)-J\left(\theta_{1},...,\theta_{i}-\epsilon,...,\theta_{n}\right)}{2\epsilon}$
\end_inset


\end_layout

\begin_layout Itemize
These equations give a way to numerically approximate the gradient with
 respect to any of your parameters.
\end_layout

\begin_layout Itemize
Implementation in Matlab:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

for i=1:n
\end_layout

\begin_layout Plain Layout

	thetaPlus = theta;
\end_layout

\begin_layout Plain Layout

	thetaPlus(i) = thetaPlus(i) + epsilon;
\end_layout

\begin_layout Plain Layout

	thetaMinus = theta;
\end_layout

\begin_layout Plain Layout

	thetaMinus(i) = thetaMinus(i) - epsilon;
\end_layout

\begin_layout Plain Layout

	gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon);
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Implemenation:
\end_layout

\begin_deeper
\begin_layout Itemize
Implement backpropagation to compute 
\family typewriter
DVec
\family default
.
\end_layout

\begin_layout Itemize
Implement numerical gradient checking to compute 
\family typewriter
gradApprox
\family default
.
\end_layout

\begin_layout Itemize
Make sure they give similar values for a few test cases.
\end_layout

\begin_layout Itemize
Turn off gradient checking; just use backpropagation for future runs.
\end_layout

\end_deeper
\begin_layout Itemize
Important: make sure to disable your gradient checking code before training
 your classifier, otherwise your code will be very slow.
\end_layout

\end_deeper
\begin_layout Itemize
Random initialization: we need to pick some initial value for 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
For gradient descent, we previously set 
\family typewriter
initialTheta = zeros(n,1)
\family default
.
\end_layout

\begin_deeper
\begin_layout Itemize
This doesn't work for training a neural network; it will result in all nodes
 being the same and all deltas being the same.
 All of the partial derivatives will be equal to each other, as well.
\end_layout

\end_deeper
\begin_layout Itemize
So, for a neural network, we need to use a random choice for 
\family typewriter
initialTheta
\family default
 between 
\begin_inset Formula $\left[-\epsilon,\epsilon\right]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
Theta1 = rand(10,11)*(2*initEps) - initEps;
\end_layout

\begin_layout Itemize

\family typewriter
Theta2 = rand(1,11)*(2*initEps) - initEps;
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Putting it all together:
\end_layout

\begin_deeper
\begin_layout Itemize
First, pick some network architecture, or connectivity pattern between neurons.
\end_layout

\begin_deeper
\begin_layout Itemize
Number of input units should be the number of features.
\end_layout

\begin_layout Itemize
Number of output units should be the number of possible classifications.
\end_layout

\begin_layout Itemize
Reasonable default: one hidden layer, or if more than one hidden layer,
 have the same number of units in each hidden layer (usually the more, the
 better, although more units requires more computational power).
\end_layout

\end_deeper
\begin_layout Itemize
Next, we need to train a neural network.
\end_layout

\begin_deeper
\begin_layout Itemize
First, randomly initialize the weights.
\end_layout

\begin_layout Itemize
Then, implement forward propagation to get 
\begin_inset Formula $h_{\Theta}\left(x^{\left(i\right)}\right)$
\end_inset

 for any 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Implement code to compute the cost function 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Implement backpropagation to compute partial derivatives 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{jk}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Usually requires a for loop.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Use gradient checking to compare the partial derivatives computed using
 backpropagation to those computed numerically for a few examples.
\end_layout

\begin_layout Itemize
Disable gradient checking code if everything seems OK.
\end_layout

\begin_layout Itemize
Use gradient descent or advanced optimization methods with backpropagation
 to try to minimize 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 as a function of the parameters 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Note: 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 is not necessarily convex, so it is possible for the algorithms to get
 stuck in local optima.
\end_layout

\end_deeper
\end_deeper
\begin_layout Section*
Advice for applying machine learning
\end_layout

\begin_layout Subsection*
Evaluating a learning algorithm
\end_layout

\begin_layout Itemize
How do you decide what the most promising avenues to explore are?
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose you have implemented regularized linear regression to predict housing
 prices.
\end_layout

\begin_layout Itemize
However, when you test your hypothesis on a new set of houses, you find
 that it makes unacceptably large errors in its predictions.
\end_layout

\begin_layout Itemize
What should you try next?
\end_layout

\begin_deeper
\begin_layout Itemize
Get more training examples, although this doesn't always help.
\end_layout

\begin_layout Itemize
Spend time trying to carefully select a smaller set of features.
\end_layout

\begin_layout Itemize
Try getting additional features.
\end_layout

\begin_layout Itemize
Try adding polynomial features.
\end_layout

\begin_layout Itemize
Try increasing or decreasing the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Machine learning diagnostics: tests you can run to gain insight into what
 is/isn't working with a learning algorithm, and gain guidance as to how
 best to improve its performance.
\end_layout

\begin_deeper
\begin_layout Itemize
Diagnostics can take time to implement, but doing so can be a very good
 use of your time.
\end_layout

\end_deeper
\begin_layout Itemize
Evaluating your hypothesis:
\end_layout

\begin_deeper
\begin_layout Itemize
Split your data into two portions: training and testing datasets.
\end_layout

\begin_deeper
\begin_layout Itemize
Typical split is about 70% training, 30% testing.
\end_layout

\begin_layout Itemize
\begin_inset Formula $m_{test}$
\end_inset

: number of test examples.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left(x_{test}^{\left(i\right)},y_{test}^{\left(i\right)}\right)$
\end_inset

: 
\begin_inset Formula $i$
\end_inset

th example from test dataset.
\end_layout

\begin_layout Itemize
Should usually divide dataset by randomly choosing samples since there may
 sometimes be some order to the samples.
\end_layout

\end_deeper
\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Learn parameters 
\begin_inset Formula $\theta$
\end_inset

 from training data by minimizing training error 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Compute test set error 
\begin_inset Formula $J_{test}\left(\theta\right)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}\left(h_{\theta}\left(x_{test}^{\left(i\right)}\right)-y_{test}^{\left(i\right)}\right)^{2}$
\end_inset

 (this example is for linear regression).
\end_layout

\begin_layout Itemize
Alternate definition of test set error: 0/1 misclassification error.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $err\left(h_{\theta}\left(x\right),y\right)=\left\{ \begin{array}{c}
1,h_{\theta}\left(x\right)\geq0.5,y=0|h_{\theta}\left(x\right)<0.5,y=1\\
0,\text{otherwise}
\end{array}\right.$
\end_inset


\end_layout

\begin_layout Itemize
Test error: 
\begin_inset Formula $\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
When parameters are fit using a training set, the error of the parameters
 as measured on that data (training set error) is likely to be lower than
 when you measure the error on another, independent dataset (generalization
 error).
\end_layout

\begin_layout Itemize
Model selection
\end_layout

\begin_deeper
\begin_layout Itemize
Say you have a choice of models with different degrees of polynomials,.
\end_layout

\begin_layout Itemize
It's as if you have another parameter, 
\begin_inset Formula $d$
\end_inset

, which is the highest degree of the polynomial used in the model.
\end_layout

\begin_layout Itemize
You could fit each model to your training data and get a set of parameters
 for each model 
\begin_inset Formula $\theta^{\left(d\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Then you can look at the test set and compute the cost for each model: 
\begin_inset Formula $J_{test}\left(\theta^{\left(d\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, rind the model with the lowest cost.
 For this example, assume it's the model with parameters 
\begin_inset Formula $\theta^{\left(5\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Question: how well does the model generalize? Could report test set error
 
\begin_inset Formula $J_{test}\left(\theta^{\left(5\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Problem: 
\begin_inset Formula $J_{test}\left(\theta^{\left(5\right)}\right)$
\end_inset

 is likely to be optimistic because our extra parameter 
\begin_inset Formula $d$
\end_inset

 is fit to our test data set.
\end_layout

\begin_layout Itemize
Thus, we now need to split our dataset into three pieces:
\end_layout

\begin_deeper
\begin_layout Itemize
Training dataset (
\begin_inset Formula $\approx60\%$
\end_inset

).
\end_layout

\begin_layout Itemize
Cross-validation (CV) dataset, sometimes also called validation dataset
 (
\begin_inset Formula $\approx20\%$
\end_inset

).
 
\begin_inset Formula $m_{cv}$
\end_inset

 is the number of CV examples.
\end_layout

\begin_layout Itemize
Test dataset (
\begin_inset Formula $\approx20\%$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Can calculate the CV and test errors just as we have for the training dataset.
\end_layout

\begin_layout Itemize
For the best model selection strategy, calculate 
\begin_inset Formula $J_{cv}\left(\theta^{\left(d\right)}\right)$
\end_inset

.
 Then you can safely use 
\begin_inset Formula $J_{test}\left(\theta^{\left(d\right)}\right)$
\end_inset

 for the best model to determine the generalization error for that model.
\end_layout

\end_deeper
\begin_layout Subsection*
Bias and variance
\end_layout

\begin_layout Itemize
High bias is equivalent to underfitting the data.
\end_layout

\begin_layout Itemize
High variance is equivalent to overfitting the data.
\end_layout

\begin_layout Itemize
Idea for diagnosing bias/variance:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot training error vs.
 degree of polynomial 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Plot cross-validation error vs.
 degree of polynomial 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Training error will probably decrease with 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Cross-validation error will most likely reach a minimum at a certain value
 of 
\begin_inset Formula $d$
\end_inset

, then start increasing again.
\end_layout

\begin_layout Itemize
High bias: high training error and high CV error.
 
\begin_inset Formula $J_{train}\left(\theta\right)\approx J_{cv}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
High variance: low training error, high CV error.
\end_layout

\end_deeper
\begin_layout Itemize
Regularization can help prevent overfitting, but how does it affect bias
 and variance?
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we are fitting a high-order polynomial.
\end_layout

\begin_layout Itemize
How do we choose the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

 to minimize bias and variance?
\end_layout

\begin_deeper
\begin_layout Itemize
Try a range of 
\begin_inset Formula $\lambda$
\end_inset

 values.
\end_layout

\begin_layout Itemize
For each 
\begin_inset Formula $\lambda$
\end_inset

 value, we find the set of parameters 
\begin_inset Formula $\theta$
\end_inset

 which minimizes 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, apply this set of parameters to the cross-validation dataset and pick
 whichever 
\begin_inset Formula $\lambda$
\end_inset

 ends up giving the lowest 
\begin_inset Formula $J_{cv}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, apply this to the test set.
\end_layout

\begin_layout Itemize
This is model selection applied to the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Learning curves
\end_layout

\begin_deeper
\begin_layout Itemize
Plotting 
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $J_{cv}\left(\theta\right)$
\end_inset

 versus 
\begin_inset Formula $m$
\end_inset

 (training set size).
\end_layout

\begin_layout Itemize
Lets you see how your algorithm changes with more training examples.
\end_layout

\begin_layout Itemize
Average training error will grow with 
\begin_inset Formula $m$
\end_inset

 (easy to fit only a few data points, harder to fit many).
 Will usually start to plateau at a certain point.
\end_layout

\begin_layout Itemize
Cross-validation error will tend to decrease as 
\begin_inset Formula $m$
\end_inset

 increases.
\end_layout

\begin_layout Itemize
High bias (underfitting) case:
\end_layout

\begin_deeper
\begin_layout Itemize
Cross-validation error will initially decrease with 
\begin_inset Formula $m$
\end_inset

, but will plateau relatively quickly.
\end_layout

\begin_layout Itemize
Training error will start small and increase with 
\begin_inset Formula $m$
\end_inset

, and will eventually plateau and end up very close to the cross-validation
 error.
\end_layout

\begin_layout Itemize
If a learning algorithm has high bias, getting more training data will not
 help very much on its own.
\end_layout

\end_deeper
\begin_layout Itemize
High variance (overfitting) case:
\end_layout

\begin_deeper
\begin_layout Itemize
Training error will start small and increase very slowly with 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Itemize
Cross-validation error will start high and decrease very slowly with 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Itemize
There will be a big gap between the training and cross-validation error.
\end_layout

\begin_layout Itemize
If a learning algorithm has high variance, getting more training data is
 likely to help.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Debugging a learning algorithm
\end_layout

\begin_deeper
\begin_layout Itemize
Getting more training examples will help with high variance cases.
\end_layout

\begin_layout Itemize
Trying a smaller set of features will also help with high variance cases.
\end_layout

\begin_layout Itemize
Adding features will help with high bias problems.
\end_layout

\begin_layout Itemize
Adding polynomial features will also help with high bias problems.
\end_layout

\begin_layout Itemize
Decreasing 
\begin_inset Formula $\lambda$
\end_inset

 will help fix high bias problems.
\end_layout

\begin_layout Itemize
Increasing 
\begin_inset Formula $\lambda$
\end_inset

 will help fix high variance problems.
\end_layout

\end_deeper
\begin_layout Itemize
Neural networks and overfitting.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Small
\begin_inset Quotes erd
\end_inset

 neural network: fewer parameters, computationally cheaper, more prone to
 underfitting.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Large
\begin_inset Quotes erd
\end_inset

 neural network: more parameters, computationally more expensive, more prone
 to overfitting.
 Use regularization to address overfitting.
\end_layout

\end_deeper
\end_body
\end_document
