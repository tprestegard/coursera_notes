#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Practical machine learning - Notes
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 9/7/2015 - 10/4/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Subsection*

\series bold
Motivation and prerequisites
\end_layout

\begin_layout Itemize
Basic ideas behind machine learning/prediction
\end_layout

\begin_deeper
\begin_layout Itemize
Study design: training vs.
 test sets.
\end_layout

\begin_layout Itemize
Conceptual issues: out of sample error, ROC curves.
\end_layout

\begin_layout Itemize
Practical implementation: the caret package.
\end_layout

\end_deeper
\begin_layout Itemize
Who predicts things?
\end_layout

\begin_deeper
\begin_layout Itemize
Governments: pension payments.
\end_layout

\begin_layout Itemize
Google: whether you will click on an ad.
\end_layout

\begin_layout Itemize
Amazon: what movies you will watch.
\end_layout

\begin_layout Itemize
Insurance companies: what your risk of death is.
\end_layout

\begin_layout Itemize
Johns Hopkins: who will succeed in their programs.
\end_layout

\end_deeper
\begin_layout Subsection*
What is prediction?
\end_layout

\begin_layout Itemize
Components of a predictor:
\end_layout

\begin_deeper
\begin_layout Itemize
Question.
\end_layout

\begin_layout Itemize
Input data.
\end_layout

\begin_layout Itemize
Features.
\end_layout

\begin_layout Itemize
Algorithm.
\end_layout

\begin_layout Itemize
Parameters.
\end_layout

\begin_layout Itemize
Evaluation.
\end_layout

\end_deeper
\begin_layout Standard
Relative order of importance
\end_layout

\begin_layout Itemize
Defining the question is the most important step!
\end_layout

\begin_layout Itemize
Input data: garbage in = garbage out.
\end_layout

\begin_deeper
\begin_layout Itemize
May be easy: movie ratings -> new movie ratings.
\end_layout

\begin_layout Itemize
May be hard: gene expression data -> disease.
\end_layout

\begin_layout Itemize
Depends on how you define a 
\begin_inset Quotes eld
\end_inset

good prediction.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Often more data helps more than better models.
\end_layout

\begin_layout Itemize
Very important to collect the 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 data that is relevant to your question.
\end_layout

\end_deeper
\begin_layout Itemize
Features matter!
\end_layout

\begin_deeper
\begin_layout Itemize
Properties of good features:
\end_layout

\begin_deeper
\begin_layout Itemize
Lead to data compression.
\end_layout

\begin_layout Itemize
Retain relevant information.
\end_layout

\begin_layout Itemize
Are created based on expert application knowledge.
\end_layout

\end_deeper
\begin_layout Itemize
Common mistakes:
\end_layout

\begin_deeper
\begin_layout Itemize
Trying to automate feature selection.
\end_layout

\begin_layout Itemize
Not paying attention to data-specific quirks.
\end_layout

\begin_layout Itemize
Throwing away information unnecessarily.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Algorithms matter less than you'd think.
\end_layout

\begin_layout Itemize
Issues to consider: your method should be interpretable, simple, accurate,
 fast (to train and test), and scalable.
\end_layout

\begin_layout Itemize
Prediction is about accuracy tradeoffs:
\end_layout

\begin_deeper
\begin_layout Itemize
Interpretability versus accuracy.
\end_layout

\begin_layout Itemize
Speed versus accuracy.
\end_layout

\begin_layout Itemize
Simplicity versus accuracy.
\end_layout

\begin_layout Itemize
Scalability versus accuracy.
\end_layout

\end_deeper
\begin_layout Subsection*
In sample and out of sample errors
\end_layout

\begin_layout Itemize
In sample error: the error rate you get on the same data set that you used
 to build your predictor.
 Sometimes called resubstitution error.
 Usually slightly optimistic.
\end_layout

\begin_layout Itemize
Out of sample error: the error rate you get on a new data set.
 Sometimes calles generalization error.
\end_layout

\begin_layout Itemize
Key ideas:
\end_layout

\begin_deeper
\begin_layout Itemize
Out of sample error is what you really care about.
\end_layout

\begin_layout Itemize
In sample error < out of sample error, due to overfitting (matching your
 algorithm to the data you have).
\end_layout

\end_deeper
\begin_layout Itemize
Data have two parts: signal and noise.
\end_layout

\begin_deeper
\begin_layout Itemize
The goal of a predictor is to find signal.
\end_layout

\begin_layout Itemize
You can always design a perfect in-sample predictor, but you capture both
 signal and noise when you do that.
\end_layout

\begin_layout Itemize
This predictor won't perform as well on new samples (overfitting again).
\end_layout

\end_deeper
\begin_layout Subsection*
Prediction study design
\end_layout

\begin_layout Itemize
Define your error rate.
\end_layout

\begin_layout Itemize
Split data into: training, testing, and validation (optional) datasets.
\end_layout

\begin_layout Itemize
On the training set, pick features and use cross-validation.
\end_layout

\begin_layout Itemize
On the training set, pick a prediction function and use cross-validation.
\end_layout

\begin_layout Itemize
If no validation, apply the function once to the test set.
\end_layout

\begin_layout Itemize
If using validation, apply the function to the test set and refine, then
 apply once to the validation dataset.
\end_layout

\begin_layout Itemize
Avoid small sample sizes
\end_layout

\begin_deeper
\begin_layout Itemize
Example: predicting a binary outcome, like flipping a coin.
\end_layout

\begin_layout Itemize
Probability of perfect classification is approximately 
\begin_inset Formula $\left(1/2\right)^{\text{sample size}}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n=1$
\end_inset

: flipping a coin gives 50% chance of 100% accuracy.
\end_layout

\begin_layout Itemize
\begin_inset Formula $n=2$
\end_inset

: flipping a coin gives 25% chance of 100% accuracy.
\end_layout

\begin_layout Itemize
\begin_inset Formula $n=100$
\end_inset

: flipping a coin gives 0.1% chance of 100% accuracy.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Rules of thumb for prediction study design
\end_layout

\begin_deeper
\begin_layout Itemize
If you have a large sample size:
\end_layout

\begin_deeper
\begin_layout Itemize
60% training.
\end_layout

\begin_layout Itemize
20% test.
\end_layout

\begin_layout Itemize
20% validation.
\end_layout

\end_deeper
\begin_layout Itemize
If you have a medium sample size:
\end_layout

\begin_deeper
\begin_layout Itemize
60% training.
\end_layout

\begin_layout Itemize
40% testing.
\end_layout

\end_deeper
\begin_layout Itemize
If you have a small sample size:
\end_layout

\begin_deeper
\begin_layout Itemize
Do cross-validation.
\end_layout

\begin_layout Itemize
Report caveats of small sample size.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Some principles to remember:
\end_layout

\begin_deeper
\begin_layout Itemize
Set the test/validation set aside and don't look at it!
\end_layout

\begin_layout Itemize
In general, randomly sample the training and test datasets.
\end_layout

\begin_layout Itemize
Your datasets must reflect the structure of the problem: if predictions
 evolve with time, split the training/test by time chunks (called backtesting
 in finance).
\end_layout

\begin_layout Itemize
All subsets should reflect as much diversity as possible.
\end_layout

\begin_deeper
\begin_layout Itemize
Random assignment does this.
\end_layout

\begin_layout Itemize
You can also try to balance by features, but this is tricky.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Types of errors
\end_layout

\begin_layout Itemize
Positive = identified, negative = rejected.
\end_layout

\begin_deeper
\begin_layout Itemize
True positive (TP): correctly identified signal.
\end_layout

\begin_layout Itemize
False positive (FP): incorrectly identified noise as signal.
\end_layout

\begin_layout Itemize
True negative (TN): correctly rejected noise.
\end_layout

\begin_layout Itemize
False negative (FN): incorrectly rejected signal as noise.
\end_layout

\begin_layout Itemize
Sensitivity: Pr(positive test | sick person) = TP / (TP+FN)
\end_layout

\begin_layout Itemize
Specificity: Pr(negative test | healthy person) = TN / (FP + TN)
\end_layout

\begin_layout Itemize
Positive predictive value: Pr(sick person | positive test) = TP / (TP +
 FP)
\end_layout

\begin_layout Itemize
Negative predictive value: Pr(healthy person | negative test) = TN / (FN
 + TN)
\end_layout

\begin_layout Itemize
Accuracy: Pr(correct outcome) = (TP + TN) / (TP + FP + FN + TN)
\end_layout

\end_deeper
\begin_layout Itemize
For continuous data, there are a few ways to handle this.
\end_layout

\begin_deeper
\begin_layout Itemize
Mean squared error (MSE): 
\begin_inset Formula $MSE=\frac{1}{n}\sum_{i=1}^{n}\left(Prediction_{i}-Truth_{i}\right)^{2}$
\end_inset

 or root mean square error (RMSE): 
\begin_inset Formula $RMSE=\sqrt{MSE}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Continuous data, sensitive to outliers (outliers may raise the mean significantl
y).
\end_layout

\end_deeper
\begin_layout Itemize
Median absolute deviation.
\end_layout

\begin_deeper
\begin_layout Itemize
Continuous data, often more robust.
\end_layout

\end_deeper
\begin_layout Itemize
Sensitivity: if you want few positives called negatives.
\end_layout

\begin_layout Itemize
Specificity: if you want few negatives called positives.
\end_layout

\begin_layout Itemize
Accuracy: weights false positives and negatives equally.
\end_layout

\begin_layout Itemize
Concordance.
\end_layout

\end_deeper
\begin_layout Subsection*
Receiver operating characteristic (ROC) curves
\end_layout

\begin_layout Itemize
Why a curve?
\end_layout

\begin_deeper
\begin_layout Itemize
In binary classification you are predicting one of two categories.
\end_layout

\begin_layout Itemize
But your predictions are ofen quantitative: probability of this or that.
\end_layout

\begin_layout Itemize
The 
\emph on
cutoff
\emph default
 you choose gives different results.
\end_layout

\end_deeper
\begin_layout Itemize
ROC curves:
\end_layout

\begin_deeper
\begin_layout Itemize
X-axis: 1 - specificity, or probability of being a false positive.
\end_layout

\begin_layout Itemize
Y-axis: probability of being a true positive.
\end_layout

\begin_layout Itemize
To compare different curves, you can calculate the total area under each
 curve (more area generally means a better predictor).
\end_layout

\begin_deeper
\begin_layout Itemize
Area under curve = 0.5 is equivalent to random guessing.
\end_layout

\begin_layout Itemize
Area under curve = 1 is a perfect classifier.
\end_layout

\begin_layout Itemize
In general, if your area under the curve is more than 0.8, that is considered
 
\begin_inset Quotes eld
\end_inset

good.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Cross-validation
\end_layout

\begin_layout Itemize
Key ideas:
\end_layout

\begin_deeper
\begin_layout Itemize
Accuracy on the training set (resubstitution accuracy) is optimistic.
\end_layout

\begin_layout Itemize
A better estimate comes from an independent dataset (test set accuracy).
\end_layout

\begin_layout Itemize
But we can't use the test set when building the model or it becomes part
 of the training set.
\end_layout

\begin_layout Itemize
So we estimate the test set accuracy with the training set.
\end_layout

\end_deeper
\begin_layout Itemize
Cross-validation approach:
\end_layout

\begin_deeper
\begin_layout Itemize
Use the training set.
\end_layout

\begin_layout Itemize
Split it into training/test sets.
\end_layout

\begin_deeper
\begin_layout Itemize
Use random subsampling to do this.
\end_layout

\begin_layout Itemize
Can also do 
\begin_inset Quotes eld
\end_inset

K-fold
\begin_inset Quotes erd
\end_inset

 cross-validation.
\end_layout

\begin_layout Itemize
Another option: 
\begin_inset Quotes eld
\end_inset

leave one out
\begin_inset Quotes erd
\end_inset

.
 Use only one sample for test dataset and the rest for training; repeat
 with all samples.
\end_layout

\end_deeper
\begin_layout Itemize
Build a model on the training set.
\end_layout

\begin_layout Itemize
Evaluate on the test set.
\end_layout

\begin_layout Itemize
Repeat and average the estimated errors.
\end_layout

\end_deeper
\begin_layout Itemize
Useful for:
\end_layout

\begin_deeper
\begin_layout Itemize
Picking variables to include in the model.
\end_layout

\begin_layout Itemize
Picking the type of prediction function to use.
\end_layout

\begin_layout Itemize
Picking the parameters in the prediction function.
\end_layout

\begin_layout Itemize
Comparing different predictors.
\end_layout

\end_deeper
\begin_layout Itemize
Considerations:
\end_layout

\begin_deeper
\begin_layout Itemize
For time-series data, you must use chunks of data.
\end_layout

\begin_layout Itemize
For K-fold cross-validation.
\end_layout

\begin_deeper
\begin_layout Itemize
Larger K: less bias, more variance.
\end_layout

\begin_layout Itemize
Smaller K: more bias, less variance.
\end_layout

\end_deeper
\begin_layout Itemize
Random sampling must be done without replacement.
\end_layout

\begin_layout Itemize
Random sampling with replacement is called 
\emph on
bootstrapping.
\end_layout

\begin_deeper
\begin_layout Itemize
Underestimates the error.
\end_layout

\begin_layout Itemize
Can be corrected, but it's complicated (0.632 Bootstrap).
\end_layout

\end_deeper
\begin_layout Itemize
If you cross-validate to pick predictors, you must estimate errors on independen
t data.
\end_layout

\end_deeper
\begin_layout Subsection*
What data should you use?
\end_layout

\begin_layout Itemize
Key idea: to predict X, use data as closely related to X as you possibly
 can.
 (example: Moneyball; use player performance data to predict player performance)
\end_layout

\begin_layout Itemize
Using unrelated data is the most common mistake!
\end_layout

\end_body
\end_document
