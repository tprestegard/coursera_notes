#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Practical machine learning - Notes
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 9/7/2015 - 10/4/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Subsection*

\series bold
Motivation and prerequisites
\end_layout

\begin_layout Itemize
Basic ideas behind machine learning/prediction
\end_layout

\begin_deeper
\begin_layout Itemize
Study design: training vs.
 test sets.
\end_layout

\begin_layout Itemize
Conceptual issues: out of sample error, ROC curves.
\end_layout

\begin_layout Itemize
Practical implementation: the caret package.
\end_layout

\end_deeper
\begin_layout Itemize
Who predicts things?
\end_layout

\begin_deeper
\begin_layout Itemize
Governments: pension payments.
\end_layout

\begin_layout Itemize
Google: whether you will click on an ad.
\end_layout

\begin_layout Itemize
Amazon: what movies you will watch.
\end_layout

\begin_layout Itemize
Insurance companies: what your risk of death is.
\end_layout

\begin_layout Itemize
Johns Hopkins: who will succeed in their programs.
\end_layout

\end_deeper
\begin_layout Subsection*
What is prediction?
\end_layout

\begin_layout Itemize
Components of a predictor:
\end_layout

\begin_deeper
\begin_layout Itemize
Question.
\end_layout

\begin_layout Itemize
Input data.
\end_layout

\begin_layout Itemize
Features.
\end_layout

\begin_layout Itemize
Algorithm.
\end_layout

\begin_layout Itemize
Parameters.
\end_layout

\begin_layout Itemize
Evaluation.
\end_layout

\end_deeper
\begin_layout Standard
Relative order of importance
\end_layout

\begin_layout Itemize
Defining the question is the most important step!
\end_layout

\begin_layout Itemize
Input data: garbage in = garbage out.
\end_layout

\begin_deeper
\begin_layout Itemize
May be easy: movie ratings -> new movie ratings.
\end_layout

\begin_layout Itemize
May be hard: gene expression data -> disease.
\end_layout

\begin_layout Itemize
Depends on how you define a 
\begin_inset Quotes eld
\end_inset

good prediction.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Often more data helps more than better models.
\end_layout

\begin_layout Itemize
Very important to collect the 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 data that is relevant to your question.
\end_layout

\end_deeper
\begin_layout Itemize
Features matter!
\end_layout

\begin_deeper
\begin_layout Itemize
Properties of good features:
\end_layout

\begin_deeper
\begin_layout Itemize
Lead to data compression.
\end_layout

\begin_layout Itemize
Retain relevant information.
\end_layout

\begin_layout Itemize
Are created based on expert application knowledge.
\end_layout

\end_deeper
\begin_layout Itemize
Common mistakes:
\end_layout

\begin_deeper
\begin_layout Itemize
Trying to automate feature selection.
\end_layout

\begin_layout Itemize
Not paying attention to data-specific quirks.
\end_layout

\begin_layout Itemize
Throwing away information unnecessarily.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Algorithms matter less than you'd think.
\end_layout

\begin_layout Itemize
Issues to consider: your method should be interpretable, simple, accurate,
 fast (to train and test), and scalable.
\end_layout

\begin_layout Itemize
Prediction is about accuracy tradeoffs:
\end_layout

\begin_deeper
\begin_layout Itemize
Interpretability versus accuracy.
\end_layout

\begin_layout Itemize
Speed versus accuracy.
\end_layout

\begin_layout Itemize
Simplicity versus accuracy.
\end_layout

\begin_layout Itemize
Scalability versus accuracy.
\end_layout

\end_deeper
\begin_layout Subsection*
In sample and out of sample errors
\end_layout

\begin_layout Itemize
In sample error: the error rate you get on the same data set that you used
 to build your predictor.
 Sometimes called resubstitution error.
 Usually slightly optimistic.
\end_layout

\begin_layout Itemize
Out of sample error: the error rate you get on a new data set.
 Sometimes calles generalization error.
\end_layout

\begin_layout Itemize
Key ideas:
\end_layout

\begin_deeper
\begin_layout Itemize
Out of sample error is what you really care about.
\end_layout

\begin_layout Itemize
In sample error < out of sample error, due to overfitting (matching your
 algorithm to the data you have).
\end_layout

\end_deeper
\begin_layout Itemize
Data have two parts: signal and noise.
\end_layout

\begin_deeper
\begin_layout Itemize
The goal of a predictor is to find signal.
\end_layout

\begin_layout Itemize
You can always design a perfect in-sample predictor, but you capture both
 signal and noise when you do that.
\end_layout

\begin_layout Itemize
This predictor won't perform as well on new samples (overfitting again).
\end_layout

\end_deeper
\begin_layout Subsection*
Prediction study design
\end_layout

\begin_layout Itemize
Define your error rate.
\end_layout

\begin_layout Itemize
Split data into: training, testing, and validation (optional) datasets.
\end_layout

\begin_layout Itemize
On the training set, pick features and use cross-validation.
\end_layout

\begin_layout Itemize
On the training set, pick a prediction function and use cross-validation.
\end_layout

\begin_layout Itemize
If no validation, apply the function once to the test set.
\end_layout

\begin_layout Itemize
If using validation, apply the function to the test set and refine, then
 apply once to the validation dataset.
\end_layout

\begin_layout Itemize
Avoid small sample sizes
\end_layout

\begin_deeper
\begin_layout Itemize
Example: predicting a binary outcome, like flipping a coin.
\end_layout

\begin_layout Itemize
Probability of perfect classification is approximately 
\begin_inset Formula $\left(1/2\right)^{\text{sample size}}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n=1$
\end_inset

: flipping a coin gives 50% chance of 100% accuracy.
\end_layout

\begin_layout Itemize
\begin_inset Formula $n=2$
\end_inset

: flipping a coin gives 25% chance of 100% accuracy.
\end_layout

\begin_layout Itemize
\begin_inset Formula $n=100$
\end_inset

: flipping a coin gives 0.1% chance of 100% accuracy.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Rules of thumb for prediction study design
\end_layout

\begin_deeper
\begin_layout Itemize
If you have a large sample size:
\end_layout

\begin_deeper
\begin_layout Itemize
60% training.
\end_layout

\begin_layout Itemize
20% test.
\end_layout

\begin_layout Itemize
20% validation.
\end_layout

\end_deeper
\begin_layout Itemize
If you have a medium sample size:
\end_layout

\begin_deeper
\begin_layout Itemize
60% training.
\end_layout

\begin_layout Itemize
40% testing.
\end_layout

\end_deeper
\begin_layout Itemize
If you have a small sample size:
\end_layout

\begin_deeper
\begin_layout Itemize
Do cross-validation.
\end_layout

\begin_layout Itemize
Report caveats of small sample size.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Some principles to remember:
\end_layout

\begin_deeper
\begin_layout Itemize
Set the test/validation set aside and don't look at it!
\end_layout

\begin_layout Itemize
In general, randomly sample the training and test datasets.
\end_layout

\begin_layout Itemize
Your datasets must reflect the structure of the problem: if predictions
 evolve with time, split the training/test by time chunks (called backtesting
 in finance).
\end_layout

\begin_layout Itemize
All subsets should reflect as much diversity as possible.
\end_layout

\begin_deeper
\begin_layout Itemize
Random assignment does this.
\end_layout

\begin_layout Itemize
You can also try to balance by features, but this is tricky.
\end_layout

\end_deeper
\end_deeper
\end_body
\end_document
