#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning (Stanford)
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 10/5/2015 - 12/27/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Subsection*
Supervised learning
\end_layout

\begin_layout Itemize
Say you have data which gives the price and square footage of several houses.
\end_layout

\begin_layout Itemize
You want to predict the price given some square footage as an input.
\end_layout

\begin_layout Itemize
You can fit any type of function to the data: linear, quadratic, etc.
\end_layout

\begin_layout Itemize
Supervised learning: using a dataset where the 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 answers are known.
\end_layout

\begin_layout Itemize
Regression problem: predict a continuous valued output.
\end_layout

\begin_layout Itemize
Classification problem: predict for a problem with a discrete output.
\end_layout

\begin_deeper
\begin_layout Itemize
Also want to estimate the probability associated with the prediction.
\end_layout

\end_deeper
\begin_layout Itemize
Most interesting machine learning algorithms can deal with an infinite number
 of features!
\end_layout

\begin_deeper
\begin_layout Itemize
Requires a support vector machine - we will talk about this later in the
 course.
\end_layout

\end_deeper
\begin_layout Subsection*
Unsupervised learning
\end_layout

\begin_layout Itemize
We are given data where we don't know the 
\begin_inset Quotes eld
\end_inset

right answer.
\begin_inset Quotes erd
\end_inset

 The actual data is not classified as true or false.
\end_layout

\begin_layout Itemize
The question is: here is the dataset, can you find some structure in the
 data?
\end_layout

\begin_layout Itemize
Example: clustering algorithm.
 Used in Google news to group similar stories together.
\end_layout

\begin_deeper
\begin_layout Itemize
In this method, the algorithm assigns group labels to different clusters.
\end_layout

\end_deeper
\begin_layout Itemize
Other examples: social network analysis, organization of computer clusters,
 market segmentation, astronomical data analysis.
\end_layout

\begin_layout Itemize
Example: cocktail party problem - useful for separating highly correlated
 audio tracks into independent tracks.
\end_layout

\begin_deeper
\begin_layout Itemize
Cocktail party problem algorithm: 
\family typewriter
[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
\family default
 (singular value decomposition)
\end_layout

\end_deeper
\begin_layout Section*
Linear regression with one variable
\end_layout

\begin_layout Subsection*
Model and cost function
\end_layout

\begin_layout Itemize
Regression problem: predict a (continuous output).
\end_layout

\begin_layout Itemize
General notation for this course:
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
m
\emph default
: number of examples in the training dataset.
\end_layout

\begin_layout Itemize

\emph on
x
\emph default
: input variables/features.
\end_layout

\begin_layout Itemize

\emph on
y
\emph default
: output variable/target.
\end_layout

\begin_layout Itemize

\emph on
(x,y)
\emph default
: denotes a single training example.
\end_layout

\end_deeper
\begin_layout Itemize
General flow:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set -> learning algorithm -> hypothesis function.
\end_layout

\begin_layout Itemize
The hypothesis function takes input (
\emph on
x
\emph default
) and predicts the outcome (
\emph on
y
\emph default
).
 It maps from 
\emph on
x
\emph default
's to 
\emph on
y
\emph default
's.
\end_layout

\end_deeper
\begin_layout Itemize
How do we represent the hypothesis 
\emph on
h
\emph default
?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$
\end_inset

 for univariate linear regression, or linear regression with one variable.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\theta$
\end_inset

's are the parameters of the model.
\end_layout

\end_deeper
\begin_layout Itemize
How do we choose the parameters of the model?
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 such that we minimize the sum of the squared errors: 
\begin_inset Formula $\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Factor of 
\begin_inset Formula $\frac{1}{2m}$
\end_inset

 doesn't affect parameter values and will make later math a bit easier.
\end_layout

\begin_layout Itemize
This also defined as the cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
There are other cost functions that may work as well, but the squared error
 cost function is the most commonly used one for linear regression.
\end_layout

\end_deeper
\begin_layout Itemize
Can be useful to plot cost function in terms of the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
If we assume an intercept of 0 (
\begin_inset Formula $\theta_{0}=0$
\end_inset

), the value of 
\begin_inset Formula $\theta_{1}$
\end_inset

 that minimizes the cost function is 
\begin_inset Formula $\theta_{1}=\frac{\sum_{i}x^{\left(i\right)}y^{\left(i\right)}}{\sum_{i}x^{2\left(i\right)}}$
\end_inset


\end_layout

\begin_layout Itemize
For two parameters,
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{1}=\frac{\sum_{i=1}^{m}x^{\left(i\right)}y^{\left(i\right)}-\bar{x}\bar{y}}{\sum_{i=1}^{m}x^{\left(i\right)2}-\bar{x}^{2}}$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{0}=\bar{y}-\theta_{1}\bar{x}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Gradient descent
\end_layout

\begin_layout Itemize
Have some cost function 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 that we want to minimize (find 
\begin_inset Formula $\underset{\theta_{0,}\theta_{1}}{\text{min}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Outline:
\end_layout

\begin_deeper
\begin_layout Itemize
Start with some 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
Keep changing 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

 to reduce 
\begin_inset Formula $J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 until we hopefully end up at a minimum.
\end_layout

\end_deeper
\begin_layout Itemize
Definition of gradient descent algorithm: 
\family typewriter
repeat until convergence { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 }.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $:=$
\end_inset

: assignment operator.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

: learning rate.
 Controls step size.
\end_layout

\end_deeper
\begin_layout Itemize
When actually programming this, make sure you change the parameters at the
 same time and then update them at the same time! Otherwise you may incorrectly
 use new values of the parameters to calculate the cost function.
\end_layout

\end_deeper
\begin_layout Itemize
Don't need to adjust 
\begin_inset Formula $\alpha$
\end_inset

 over time because the derivative term will get smaller as it approaches
 a local minimum.
\end_layout

\begin_layout Itemize
Applying gradient descent to our simple cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)=\frac{\partial}{\partial\theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}\left(\theta_{0}+\theta_{1}x^{\left(i\right)}-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=0$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{0}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=1$
\end_inset

: 
\begin_inset Formula $\frac{\partial}{\partial\theta_{1}}J\left(\theta_{0},\theta_{1}\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Batch gradient descent: used in machine learning, each step of gradient
 descent uses all of the training examples.
\end_layout

\begin_layout Section*
Linear regression with multiple variables
\end_layout

\begin_layout Itemize
Also called multivariate linear regression.
\end_layout

\begin_layout Itemize
We now have multiple features (or predictors) that we want to use to develop
 a hypothesis function and make a prediction.
\end_layout

\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

: number of features.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: input (features) of 
\begin_inset Formula $i$
\end_inset

th training example.
 This is a vector of features in the 
\begin_inset Formula $i$
\end_inset

th observation.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{j}^{\left(i\right)}$
\end_inset

: value of feature 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $i$
\end_inset

th training example.
\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis function now has a new form:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$
\end_inset


\end_layout

\begin_layout Itemize
For convenience of notation, we can define 
\begin_inset Formula $x_{0}^{\left(i\right)}=1$
\end_inset

 so that 
\begin_inset Formula $h_{\theta}\left(x\right)=\sum_{i=1}^{n}\theta_{i}x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
We can also use linear algebra:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x=\left[x_{0},x_{1},...,x_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{0},\theta_{1},...,\theta_{n}\right]\in\mathbb{R}_{n+1}$
\end_inset

 (row vector)
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta x^{T}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Gradient descent with multiple variables
\end_layout

\begin_layout Itemize
Write set of parameters as 
\begin_inset Formula $\theta$
\end_inset

 and cost function as 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Gradient descent: 
\family typewriter
repeat { 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

 } 
\family default
(simultaneously update for each 
\begin_inset Formula $j=0,1,...,n$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Tips for gradient descent:
\end_layout

\begin_deeper
\begin_layout Itemize
Feature scaling - make sure that different features taken on similar ranges
 of values.
\end_layout

\begin_deeper
\begin_layout Itemize
Ideally, all features will be in the range 
\begin_inset Formula $-1\leq x\leq1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Mean normalization: normalize a feature to have a mean of zero.
 When doing this, you just replace a feature 
\begin_inset Formula $x_{i}$
\end_inset

 with 
\begin_inset Formula $x_{i}-\mu_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Do not apply this to 
\begin_inset Formula $x_{0}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Can also normalize by standard deviation: 
\begin_inset Formula $x_{i}\rightarrow\frac{x_{i}-\mu_{i}}{s_{i}}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Making sure that gradient descent is working properly:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot 
\begin_inset Formula $\underset{\theta}{\text{min}}J\left(\theta\right)$
\end_inset

 vs.
 number of iterations.
 It should decrease after every iteration for sufficiently small 
\begin_inset Formula $\alpha$
\end_inset

 (can be shown mathematically).
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too large, you may continue overshooting the minimum and never be able
 to actually reach it.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha$
\end_inset

 is too small, convergence will happen, but very slowly.
\end_layout

\end_deeper
\begin_layout Itemize
Automatic convergence tests: algorithms that tell you whether gradient descent
 has converged:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: declare convergence if 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 decreases by less than some value (like 
\begin_inset Formula $10^{-3}$
\end_inset

) in one iteration.
\end_layout

\end_deeper
\begin_layout Subsection*
Polynomial regression
\end_layout

\begin_layout Itemize
Can create new features from existing ones.
\end_layout

\begin_deeper
\begin_layout Itemize
Example: area = frontage * depth.
\end_layout

\end_deeper
\begin_layout Itemize
Polynomial model: 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$
\end_inset

 for a quadratic function.
\end_layout

\begin_deeper
\begin_layout Itemize
Important to use feature scaling since when you square or cube features,
 the range becomes much larger.
\end_layout

\end_deeper
\begin_layout Subsection*
Normal equation
\end_layout

\begin_layout Itemize
Method to solve for 
\begin_inset Formula $\theta$
\end_inset

 analytically.
\end_layout

\begin_layout Itemize
How to go about it:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)=...=0$
\end_inset

, then solve for every 
\begin_inset Formula $\theta_{j}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Linear algebra method:
\end_layout

\begin_deeper
\begin_layout Itemize
Put all features into a matrix: 
\begin_inset Formula $X=\begin{array}{ccccc}
1 & 2104 & 5 & 1 & 45\\
1 & 1416 & 3 & 2 & 40\\
1 & 1534 & 3 & 2 & 30\\
1 & 852 & 2 & 1 & 36
\end{array}$
\end_inset

 (including a column of ones for 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\begin_layout Itemize
Put all 
\begin_inset Formula $y_{j}$
\end_inset

s into a vector: 
\begin_inset Formula $Y=\begin{array}{c}
460\\
232\\
315\\
178
\end{array}$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
In Matlab/Octave: 
\family typewriter
pinv(X'*X)*X'*Y
\end_layout

\begin_layout Itemize
Feature scaling is not needed for the normal equation method.
\end_layout

\end_deeper
\begin_layout Itemize
Normal equation vs.
 gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Pros:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: no need to choose 
\begin_inset Formula $\alpha$
\end_inset

, no need to iterate.
\end_layout

\begin_layout Itemize
Gradient descent: works well even when the number of features is large.
\end_layout

\end_deeper
\begin_layout Itemize
Cons:
\end_layout

\begin_deeper
\begin_layout Itemize
Normal equation: slow if number of features is very large, need to compute
 
\begin_inset Formula $\left(X^{T}X\right)^{-1}$
\end_inset


\end_layout

\begin_layout Itemize
Gradient descent: need to choose 
\begin_inset Formula $\alpha$
\end_inset

, needs many iterations.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation non-invertibility: what if 
\begin_inset Formula $\left(X^{T}X\right)$
\end_inset

 is non-invertible? Also known as 
\begin_inset Quotes eld
\end_inset

singular
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

degenerate.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Can occur when there are redundant features (linearly dependent) or if there
 are too many features.
\end_layout

\begin_deeper
\begin_layout Itemize
Possible solution: delete some features or use regularization.
\end_layout

\end_deeper
\begin_layout Itemize
If you use 
\family typewriter
pinv
\family default
 in Matlab/Octave instead of 
\family typewriter
inv
\family default
, it should handle singular cases for you.
\end_layout

\end_deeper
\begin_layout Section*
Logistic Regression
\end_layout

\begin_layout Subsection*
Classification and Representation
\end_layout

\begin_layout Itemize
Classification: putting things into discrete categories.
 Binary classification is when there are only two categories.
\end_layout

\begin_layout Itemize
Linear regression is not usually a very good solution for classification
 problems.
 Examples where we can clearly use some sort of threshold for classification
 make this clear.
\end_layout

\begin_layout Itemize
Logistic regression has the property that the predictions of the hypothesis
 function are always between 0 and 1: 
\begin_inset Formula $0\leq h_{\theta}\left(x\right)\leq1$
\end_inset

.
\end_layout

\begin_layout Itemize
For linear regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta^{T}x$
\end_inset

.
\end_layout

\begin_layout Itemize
For logistic regression, 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta^{T}x\right)$
\end_inset

, where 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is the sigmoid or logistic function, 
\begin_inset Formula $g\left(z\right)=\frac{1}{1+e^{-z}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Thus, 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+\exp\left(-\theta^{T}x\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
As before, we need to fit the parameters 
\begin_inset Formula $\theta$
\end_inset

 to our data.
\end_layout

\end_deeper
\begin_layout Itemize
Interpretation of hypothesis output:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

: estimated probability that 
\begin_inset Formula $y=1$
\end_inset

 (i.e., positive result) on input 
\begin_inset Formula $x$
\end_inset

; i.e.
 
\begin_inset Formula $h_{\theta}\left(x\right)=p\left(y=1|x;\theta\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Decision boundary:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we predict that 
\begin_inset Formula $y=1$
\end_inset

 when 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 and 
\begin_inset Formula $y=0$
\end_inset

 when 
\begin_inset Formula $h_{\theta}<0.5$
\end_inset

.
\end_layout

\begin_layout Itemize
We see that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is 
\begin_inset Formula $\geq0.5$
\end_inset

 when 
\begin_inset Formula $z\geq0$
\end_inset

, thus 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

 whenever 
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

.
\end_layout

\begin_layout Itemize
This essentially defines a threshold for making predictions with our model.
\end_layout

\begin_layout Itemize
The decision boundary is a property of the hypothesis and its parameters,
 not a property of the dataset.
\end_layout

\end_deeper
\begin_layout Itemize
Non-linear decision boundaries:
\end_layout

\begin_deeper
\begin_layout Itemize
Can use higher-order polynomials terms in logistic regression.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $h_{\theta}\left(x\right)=g\left(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{2}^{2}+\theta_{5}x_{1}x_{2}+\theta_{6}x_{1}^{3}+...\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Logistic Regression Model
\end_layout

\begin_layout Itemize
Cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
Linear regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}=\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Logistic regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\text{cost}\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What cost function here should we use here?
\end_layout

\begin_layout Itemize
We can't use the same thing that we used for linear regression because it
 will be a non-convex function - thus, gradient descent will not necessarily
 converge to the global minimum.
\end_layout

\begin_layout Itemize
Instead, we use 
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=\left\{ \begin{array}{l}
-\log\left(h_{\theta}\left(x\right)\right)\text{ if }y=1\\
-\log\left(1-h_{\theta}\left(x\right)\right)\text{ if }y=0
\end{array}\right.$
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=1$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow0$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=0$
\end_inset

: if 
\begin_inset Formula $h_{\theta}\left(x\right)=0$
\end_inset

, the cost is 0, but as 
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow1$
\end_inset

, the cost goes to 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Simplified way of writing the cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\text{cost}\left(h_{\theta}\left(x\right),y\right)=-y\log\left(h_{\theta}\left(x\right)\right)-\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)$
\end_inset


\end_layout

\begin_layout Itemize
Note: 
\begin_inset Formula $y$
\end_inset

 is always equal to 0 or 1.
\end_layout

\begin_layout Itemize
So the full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent: we do the same thing as before, we just have a new cost
 function.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{j}$
\end_inset

 := 
\begin_inset Formula $\theta_{j}-\alpha\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
This is the same as for linear regression, although our hypothesis function
 has changed.
\end_layout

\begin_layout Itemize
Make sure to simultaneously update all 
\begin_inset Formula $\theta_{j}$
\end_inset

!
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization:
\end_layout

\begin_deeper
\begin_layout Itemize
We have some cost function 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and we want the set of parameters
\begin_inset Formula $\theta$
\end_inset

 that gives the minimum of 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $\theta$
\end_inset

, we have code that can compute 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Optimization algorithms:
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descent
\end_layout

\begin_layout Itemize
Conjugate gradient
\end_layout

\begin_layout Itemize
BFGS
\end_layout

\begin_layout Itemize
L-BFGS
\end_layout

\end_deeper
\begin_layout Itemize
Advantages: no need to manually pick the learning rate 
\begin_inset Formula $\alpha$
\end_inset

, often faster than gradient descent.
\end_layout

\begin_layout Itemize
Disadvantages: more complex, can cause implementations to be error prone.
\end_layout

\begin_layout Itemize
Can use 
\family typewriter
fminunc
\family default
 in Matlab/Octave.
\end_layout

\begin_deeper
\begin_layout Itemize
Define a function which returns the cost and gradient.
\end_layout

\begin_layout Itemize
Example:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% function
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

jVal = (theta(1)-5)^2 + (theta(2)-5)^2;
\end_layout

\begin_layout Plain Layout

gradient(1) = 2*(theta(1)-5);
\end_layout

\begin_layout Plain Layout

gradient(2) = 2*(theta(2)-5);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Setup
\end_layout

\begin_layout Plain Layout

options = optimset('GradObj','on','MaxIter','100');
\end_layout

\begin_layout Plain Layout

initialTheta = zeros(2,1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Call fminunc
\end_layout

\begin_layout Plain Layout

[optTheta, functionVal, exitFlag] = ...
\end_layout

\begin_layout Plain Layout

	fminunc(@costFunction, initialTheta, options);
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Multi-class classification
\end_layout

\begin_layout Itemize
Examples of this type of problem:
\end_layout

\begin_deeper
\begin_layout Itemize
Email tagging into different folders.
\end_layout

\begin_layout Itemize
Medical diagnosis.
\end_layout

\begin_layout Itemize
Weather category.
\end_layout

\end_deeper
\begin_layout Itemize
One-vs-all classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: a training dataset with three classes.
\end_layout

\begin_layout Itemize
First: is it in class 1, or class 2/3?
\end_layout

\begin_layout Itemize
Second: is it in class 2, or class 1/3?
\end_layout

\begin_layout Itemize
Third: is it in class 3, or class 1/2?
\end_layout

\begin_layout Itemize
Use standard logistic regression for each step.
\end_layout

\begin_layout Itemize
Each gives a hypothesis: 
\begin_inset Formula $h_{\theta}^{\left(i\right)}\left(x\right)=P\left(y=i|x;\theta\right)$
\end_inset

, 
\begin_inset Formula $\left(i=1,2,3\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The hypothesis tells us the probability that 
\begin_inset Formula $y$
\end_inset

 is in class 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
To make a prediction on a new input 
\begin_inset Formula $x$
\end_inset

, pick the class 
\begin_inset Formula $i$
\end_inset

 that has the maximum probability.
\end_layout

\end_deeper
\begin_layout Section*
Regularization and overfitting
\end_layout

\begin_layout Itemize
Overfitting: similar number of parameters to data points.
 This allows the model to fit the training dataset very well, however, it
 will not make accurate predictions for new examples.
 Also called 
\begin_inset Quotes eld
\end_inset

high variance.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Occurs when you have a lot of features and not a lot of training data.
\end_layout

\begin_layout Itemize
Underfitting may occur when our model has too few parameters to properly
 model the data.
 In this situation, the model has 
\begin_inset Quotes eld
\end_inset

high bias.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Options for dealing with overfitting:
\end_layout

\begin_deeper
\begin_layout Itemize
Reduce number of features: can manually select which features to keep, or
 use a model selection algorithm to determine this (will be covered later
 in the course).
\end_layout

\begin_layout Itemize
Regularization: keep all of the features, but reduce the magnitude/values
 of the parameters 
\begin_inset Formula $\theta_{j}$
\end_inset

.
 Works well when we have a lot of features, each of which contributes a
 little bit to predicting 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection*
Regularization
\end_layout

\begin_layout Itemize
Suppose we penalize and make 
\begin_inset Formula $\theta_{3},\theta_{4}$
\end_inset

 very small.
 We do this by adjusting our cost function by adding in terms like 
\begin_inset Formula $1000\theta_{3}^{2}+1000\theta_{4}^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This will give values of 
\begin_inset Formula $\theta_{3}$
\end_inset

 and 
\begin_inset Formula $\theta_{4}$
\end_inset

 which are close to zero when the cost function is small.
\end_layout

\end_deeper
\begin_layout Itemize
General idea behind regularization:
\end_layout

\begin_deeper
\begin_layout Itemize
Small values for parameters 
\begin_inset Formula $\theta_{0},\theta_{1},...,\theta_{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Simpler hypothesis, less prone to overfitting.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example: housing price prediction.
\end_layout

\begin_deeper
\begin_layout Itemize
Features: 
\begin_inset Formula $x_{1},x_{2},...,x_{100}$
\end_inset


\end_layout

\begin_layout Itemize
Parameters: 
\begin_inset Formula $\theta_{0},\theta_{1},\theta_{2},...,\theta_{100}$
\end_inset


\end_layout

\begin_layout Itemize
We don't know which parameters to try to shrink 
\emph on
a priori
\emph default
.
\end_layout

\begin_layout Itemize
Take our cost function 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

 and add a new term to shrink 
\emph on
all
\emph default
 of our parameters except 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}+\lambda\sum_{j=1}^{n}\theta_{j}^{2}\right]$
\end_inset


\end_layout

\begin_layout Itemize
The first part in brackets captures the goal of fitting the data well.
\end_layout

\begin_layout Itemize
The second part captures the goal of keeping the parameters 
\begin_inset Formula $\theta$
\end_inset

 small.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 is the regularization parameter and controls the trade-off between these
 two goals.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\lambda$
\end_inset

 is too large, we w
\end_layout

\begin_layout Itemize
on't even fit the training dataset well.
 All of the parameters 
\begin_inset Formula $\theta$
\end_inset

 will go to zero except 
\begin_inset Formula $\theta_{0}$
\end_inset

, so we will be fitting a flat line to our data.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Regularized linear regression
\end_layout

\begin_layout Itemize
Recall that we don't want to shrink 
\begin_inset Formula $\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The term 
\begin_inset Formula $1-\alpha\frac{\lambda}{m}$
\end_inset

 is usually slightly less than 1 (assuming that your learning rate is small).
 This term shrinks your 
\begin_inset Formula $\theta_{j}$
\end_inset

, then the other term is just the original gradient descent term.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal equation
\end_layout

\begin_deeper
\begin_layout Itemize
Before, we had 
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

.
\end_layout

\begin_layout Itemize
With our new, regularized cost function, you can derive the minimum of the
 cost function by taking derivatives with respect to each parameter, as
 before.
\end_layout

\begin_layout Itemize
Now, we get 
\begin_inset Formula $\theta=\left(X^{T}X+\lambda Z\right)^{-1}X^{T}Y$
\end_inset

, where 
\begin_inset Formula $Z$
\end_inset

 is the identity matrix, except the top left entry is 0 instead of 1 (because
 we don't regularize 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Non-invertibility
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have fewer examples than features (
\begin_inset Formula $m\leq n$
\end_inset

).
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

 will give a strange answer since 
\begin_inset Formula $X^{T}X$
\end_inset

 is non-invertible/singular (make sure to use 
\family typewriter
pinv
\family default
 instead of 
\family typewriter
inv
\family default
).
\end_layout

\begin_layout Itemize
However, if 
\begin_inset Formula $\lambda>0$
\end_inset

 , we can prove that the matrix 
\begin_inset Formula $X^{T}X+\lambda Z$
\end_inset

 is invertible!
\end_layout

\end_deeper
\begin_layout Subsection*
Regularized logistic regression
\end_layout

\begin_layout Itemize
To modify our cost function to use regularization, we add a term 
\begin_inset Formula $\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
The full cost function is 
\begin_inset Formula $J\left(\theta\right)=-\left[\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Now, our gradient descent algorithm looks like:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\frac{\lambda}{m}\theta_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Can re-write this as 
\begin_inset Formula $\theta_{j}:=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $j=1,...,n$
\end_inset

.
\end_layout

\begin_layout Itemize
Recall that although this looks the same as for regularized linear regression,
 the hypothesis function is different for logistic regression.
\end_layout

\end_deeper
\begin_layout Itemize
Advanced optimization methods:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% Define a function to compute the cost.
\end_layout

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

	jVal = %code to compute J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(1) = %code to compute d/dtheta_0 J(theta);
\end_layout

\begin_layout Plain Layout

	gradient(2) = %code to compute d/dtheta_1 J(theta);
\end_layout

\begin_layout Plain Layout

	...
\end_layout

\begin_layout Plain Layout

	gradient(n+1) = %code to compute d/dtheta_n J(theta);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% then, we pass this function to fminunc as before.
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Neural networks: representation
\end_layout

\begin_layout Subsection*
Motivation
\end_layout

\begin_layout Itemize
Used for solving complex, non-linear hypotheses.
\end_layout

\begin_layout Itemize
For many predictors, including even quadratic terms can require a huge number
 of terms (grows roughly as 
\begin_inset Formula $n^{2}$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
One option is to include only a subset of higher-order terms, but it will
 probably underfit the data.
\end_layout

\end_deeper
\begin_layout Itemize
Neural networks: algorithms that try to mimic the brain.
\end_layout

\begin_layout Itemize
Recent resurgence: state-of-the-art technique for many computational application
s.
\end_layout

\begin_layout Itemize
Computationally more expensive.
\end_layout

\begin_layout Subsection*
Neural networks
\end_layout

\begin_layout Itemize
Developed as simulating networks of neurons in the brain.
\end_layout

\begin_layout Itemize
Neuron structure:
\end_layout

\begin_deeper
\begin_layout Itemize
Dendrites: 
\begin_inset Quotes eld
\end_inset

input wires,
\begin_inset Quotes erd
\end_inset

 receive inputs from other locations.
\end_layout

\begin_layout Itemize
Axon: 
\begin_inset Quotes eld
\end_inset

output wire,
\begin_inset Quotes erd
\end_inset

 used to send signals to other neurons.
\end_layout

\end_deeper
\begin_layout Itemize
Neuron model: logistic unit.
\end_layout

\begin_deeper
\begin_layout Itemize
We use a model of what a neuron does: it takes some number of inputs (
\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset

), does some computation, and sends an output 
\begin_inset Formula $h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^{T}x}}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This is a sigmoid (logistic) activation function.
\end_layout

\begin_layout Itemize
The parameters 
\begin_inset Formula $\theta$
\end_inset

 are sometimes referred to as 
\begin_inset Quotes eld
\end_inset

weights.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x_{0}$
\end_inset

 is sometimes referred to as the 
\begin_inset Quotes eld
\end_inset

bias unit.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Neural network: divide into layers.
\end_layout

\begin_deeper
\begin_layout Itemize
Layer 1: 
\begin_inset Quotes eld
\end_inset

input layer,
\begin_inset Quotes erd
\end_inset

 made up of input features.
\end_layout

\begin_layout Itemize
Layer 2: some set of neurons, called 
\begin_inset Quotes eld
\end_inset

hidden layers.
\begin_inset Quotes erd
\end_inset

 Can be several hidden layers.
\end_layout

\begin_layout Itemize
Final layer: 
\begin_inset Quotes eld
\end_inset

output layer,
\begin_inset Quotes erd
\end_inset

 outputs final value computed by a hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $a_{i}^{\left(j\right)}$
\end_inset

: 
\begin_inset Quotes eld
\end_inset

activation
\begin_inset Quotes erd
\end_inset

 of unit 
\begin_inset Formula $i$
\end_inset

 in layer 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

: matrix of weights controlling function mapping from layer 
\begin_inset Formula $j$
\end_inset

 to layer 
\begin_inset Formula $j+1$
\end_inset

.
\end_layout

\begin_layout Itemize
If a network has 
\begin_inset Formula $s_{j}$
\end_inset

 units in layer 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $s_{j+1}$
\end_inset

units in layer 
\begin_inset Formula $j+1$
\end_inset

, then 
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

 will be of dimension 
\begin_inset Formula $s_{j+1}\times\left(s_{j}+1\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Example: 
\begin_inset Formula $a_{1}^{\left(2\right)}=g\left(\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Can write 
\begin_inset Formula $z_{1}^{\left(2\right)}=\Theta_{10}^{\left(1\right)}x_{0}+\Theta_{11}^{\left(1\right)}x_{1}+\Theta_{12}^{\left(1\right)}x_{2}+\Theta_{13}^{\left(1\right)}x_{3}$
\end_inset

; i.e.
 a row of 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Vectorized implementation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $z^{\left(2\right)}=\Theta^{\left(1\right)}x=\Theta^{\left(1\right)}a^{\left(1\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(2\right)}=g\left(z^{\left(2\right)}\right)$
\end_inset

 (here 
\begin_inset Formula $z^{\left(2\right)}$
\end_inset

 is a column vector of 
\begin_inset Formula $z_{1}^{\left(2\right)},...,z_{3}^{\left(2\right)}$
\end_inset

.
 Note that 
\begin_inset Formula $g\left(z\right)$
\end_inset

 is applied element-wise.
\end_layout

\begin_layout Itemize
Add 
\begin_inset Formula $a_{0}^{\left(2\right)}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $z^{\left(3\right)}=\Theta^{\left(2\right)}a^{\left(2\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\Theta}\left(x\right)=a^{\left(3\right)}=g\left(z^{\left(3\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
We can think of the way a neural network works as though it is 
\begin_inset Quotes eld
\end_inset

learning its own features.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The function mapping 
\begin_inset Formula $a^{\left(1\right)}$
\end_inset

 to 
\begin_inset Formula $a^{\left(2\right)}$
\end_inset

 is what changes the input features to new features which go into the hidden
 layers.
\end_layout

\end_deeper
\begin_layout Itemize
You can have neural networks with other types of layouts.
\end_layout

\begin_deeper
\begin_layout Itemize
The layout is referred to as the 
\begin_inset Quotes eld
\end_inset

architechture.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Applications
\end_layout

\begin_layout Itemize
Can set up a neural network which acts as a logical filter (AND, OR, XOR,
 NOT, etc.).
\end_layout

\begin_layout Itemize
Interesting because sometimes these functions (XOR, for example) require
 non-linear decision boundaries.
\end_layout

\begin_layout Itemize
Multi-class classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Build a neural network with as many outputs as there are classes.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 is a vector, and we take the largest entry to be the predicted class.
\end_layout

\end_deeper
\begin_layout Section*
Neural networks: learning
\end_layout

\begin_layout Subsection*
Cost function and backpropagation
\end_layout

\begin_layout Itemize
\begin_inset Formula $L$
\end_inset

: number of layers in the network.
\end_layout

\begin_layout Itemize
\begin_inset Formula $s_{l}$
\end_inset

: number of units (not counting bias unit) in layer 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Itemize
Binary classification:
\end_layout

\begin_deeper
\begin_layout Itemize
Labels 
\begin_inset Formula $y$
\end_inset

 are either 0 or 1.
\end_layout

\begin_layout Itemize
One output unit (
\begin_inset Formula $K=1$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Multi-class classification:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 classes, 
\begin_inset Formula $y\in R^{K}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 output units.
\end_layout

\end_deeper
\begin_layout Itemize
Cost function:
\end_layout

\begin_deeper
\begin_layout Itemize
Logistic regression: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{m}\left[\sum_{i=1}^{m}y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Neural network: 
\begin_inset Formula $h_{\Theta}\left(x\right)\in R^{K}$
\end_inset

 (
\begin_inset Formula $K$
\end_inset

-dimensional vector), 
\begin_inset Formula $\left(h_{\Theta}\left(x\right)\right)_{i}=i$
\end_inset

th output.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(\Theta\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{\left(i\right)}\log\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}+\left(1-y_{k}^{\left(i\right)}\right)\log\left(1-\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}\right)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{\left(l\right)}\right)^{2}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Gradient computation
\end_layout

\begin_deeper
\begin_layout Itemize
One training example 
\begin_inset Formula $\left(x,y\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Forward propagation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $a^{\left(1\right)}=x$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(2\right)}=g\left(z^{\left(2\right)}\right)=g\left(\Theta^{\left(1\right)}a^{\left(1\right)}\right)$
\end_inset

 (add 
\begin_inset Formula $a_{0}^{\left(2\right)}$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(3\right)}=g\left(z^{\left(3\right)}\right)=g\left(\Theta^{\left(2\right)}a^{\left(2\right)}\right)$
\end_inset

 (add 
\begin_inset Formula $a_{0}^{\left(3\right)}$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $a^{\left(4\right)}=h_{\Theta}\left(x\right)=g\left(z^{\left(4\right)}\right)=g\left(\Theta^{\left(3\right)}a^{\left(3\right)}\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Then, we use backpropagation:
\end_layout

\begin_deeper
\begin_layout Itemize
Intuition: 
\begin_inset Formula $\delta_{j}^{\left(l\right)}=$
\end_inset

 
\begin_inset Quotes eld
\end_inset

error
\begin_inset Quotes erd
\end_inset

 of node 
\begin_inset Formula $j$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Itemize
For each output unit (layer 
\begin_inset Formula $L=4$
\end_inset

), 
\begin_inset Formula $\delta_{j}^{\left(4\right)}=a_{j}^{\left(4\right)}-y_{j}$
\end_inset

.
 (difference between the hypothesis and the actual classification)
\end_layout

\begin_layout Itemize
Vectorized version: 
\begin_inset Formula $\delta^{\left(4\right)}=a^{\left(4\right)}-y$
\end_inset

, each of these is a vector with number of elements equal to the number
 of output units 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta^{\left(3\right)}=\left(\Theta^{\left(3\right)}\right)^{T}\delta^{\left(4\right)}.*g'\left(z^{\left(3\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta^{\left(2\right)}=\left(\Theta^{\left(2\right)}\right)^{T}\delta^{\left(3\right)}.*g'\left(z^{\left(2\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
No 
\begin_inset Formula $\delta^{\left(1\right)}$
\end_inset

 for this example.
\end_layout

\begin_layout Itemize
Notes: 
\begin_inset Formula $g'\left(z^{\left(i\right)}\right)=a^{\left(i\right)}.*\left(1-a^{\left(i\right)}\right)=g\left(z^{\left(i\right)}\right).*\left(1-g\left(z^{\left(i\right)}\right)\right)$
\end_inset

, and 
\begin_inset Formula $.*$
\end_inset

 indicates element-wise multiplication.
\end_layout

\begin_layout Itemize
It is possible to prove that the partial derivative terms we want are given
 by 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset

 (without regularization).
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
Backpropagation algorithm:
\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\left\{ \left(x^{\left(1\right)},y^{\left(1\right)}\right),...,\left(x^{\left(m\right)},y^{\left(m\right)}\right)\right\} $
\end_inset


\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}=0$
\end_inset

 for all 
\begin_inset Formula $l,i,j$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $i=1:m$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Set 
\begin_inset Formula $a^{\left(1\right)}=x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Perform forward propagation to compute 
\begin_inset Formula $a^{\left(l\right)}$
\end_inset

 for 
\begin_inset Formula $l=2,3,...,L$
\end_inset

.
\end_layout

\begin_layout Itemize
Using 
\begin_inset Formula $y^{\left(i\right)}$
\end_inset

, compute 
\begin_inset Formula $\delta^{\left(L\right)}=a^{\left(L\right)}-y^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Compute 
\begin_inset Formula $\delta^{\left(L-1\right)},\delta^{\left(L-2\right)},...,\delta^{\left(2\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}:=\Delta_{ij}^{\left(l\right)}+a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Vectorized form: 
\begin_inset Formula $\Delta^{\left(l\right)}:=\Delta^{\left(l\right)}+\delta^{\left(l+1\right)}\left(a^{\left(l\right)}\right)^{T}$
\end_inset


\end_layout

\begin_layout Itemize
After the for loop, we compute:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\frac{1}{m}\Delta_{ij}^{\left(l\right)}+\lambda\Theta_{ij}^{\left(l\right)}$
\end_inset

, if 
\begin_inset Formula $j\neq0$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\frac{1}{m}\Delta_{ij}^{\left(l\right)}$
\end_inset

, if 
\begin_inset Formula $j=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Can show that 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=D_{ij}^{\left(l\right)}$
\end_inset

, so we can use this in gradient descent or other optimization algorithms.
\end_layout

\end_deeper
\begin_layout Itemize
What is backpropagation doing?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\delta_{j}^{\left(l\right)}$
\end_inset

: 
\begin_inset Quotes eld
\end_inset

error
\begin_inset Quotes erd
\end_inset

 of cost for 
\begin_inset Formula $a_{j}^{\left(l\right)}$
\end_inset

 (unit 
\begin_inset Formula $j$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

).
 I.e., 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta_{j}^{\left(l\right)}=\frac{\partial}{\partial z_{j}^{\left(l\right)}}\text{cost}\left(i\right)$
\end_inset

 for 
\begin_inset Formula $j\geq0$
\end_inset

, where cost
\begin_inset Formula $\left(i\right)=y^{\left(i\right)}\log h_{\Theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log h_{\Theta}\left(x^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
These are a measure of how much we would like to change the neural network's
 weights so as to change the intermediate values of the computation, and
 thus change the final outputs, and the cost.
\end_layout

\end_deeper
\begin_layout Subsection*
Backpropagation in practice
\end_layout

\begin_layout Itemize
Unrolling parameters from matrices into vectors:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% Let's say you defined a function for computing the cost function
\end_layout

\begin_layout Plain Layout

% Here jVal is a number, and theta and grad are vectors in R^{n+1}
\end_layout

\begin_layout Plain Layout

function [jVal, grad] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

% You will pass it to fminunc as:
\end_layout

\begin_layout Plain Layout

optTheta = fminunc(@costFunction, initialTheta, options)
\end_layout

\begin_layout Plain Layout

% For a neural network, your parameters Theta are matrices.
\end_layout

\begin_layout Plain Layout

% The gradients D are also matrices.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Example: a neural net with 10 inputs, a hidden layer of 10 nodes, and 1
 output.
 I.e., 
\begin_inset Formula $s_{1}=10$
\end_inset

, 
\begin_inset Formula $s_{2}=10$
\end_inset

, 
\begin_inset Formula $s_{3}=1$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\Theta^{\left(1\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}\in R^{1\times11}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $D^{\left(1\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}\in R^{10\times11}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}\in R^{1\times11}$
\end_inset


\end_layout

\begin_layout Itemize
To unroll them, you can do: 
\family typewriter
thetaVec = [Theta1(:); Theta2(:); Theta3(:)];
\family default
, and the same for D.
\end_layout

\begin_layout Itemize
To go back to the matrix representation, you can do 
\family typewriter
Theta1 = reshape(thetaVec(1:110),10,11); Theta2 = reshape(thetaVec(111:220),10,1
1); Theta3 = reshape(thetaVec(221:231),1,11);
\end_layout

\end_deeper
\begin_layout Itemize
Example: have initial parameters 
\begin_inset Formula $\Theta^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Unroll to get 
\family typewriter
initialTheta
\family default
 to pass to 
\family typewriter
fminunc(@costFunction, initialTheta, options)
\end_layout

\begin_layout Itemize
Then we change our cost function:
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
function [jVal, gradientVec] = costFunction(thetaVec)
\end_layout

\begin_layout Itemize
From 
\family typewriter
thetaVec
\family default
, get 
\begin_inset Formula $\Theta^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $\Theta^{\left(3\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Use forward propagation and backpropagation to compute 
\begin_inset Formula $D^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}$
\end_inset

 and 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Unroll 
\begin_inset Formula $D^{\left(1\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(2\right)}$
\end_inset

, 
\begin_inset Formula $D^{\left(3\right)}$
\end_inset

 to get 
\family typewriter
gradientVec
\family default
.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Downside of backpropagation - easy to have many subtle bugs in your implementati
on.
\end_layout

\begin_deeper
\begin_layout Itemize
However, there is something called 
\begin_inset Quotes eld
\end_inset

gradient checking
\begin_inset Quotes erd
\end_inset

 which allows us to catch almost all of these bugs.
\end_layout

\end_deeper
\begin_layout Itemize
Gradient checking
\end_layout

\begin_deeper
\begin_layout Itemize
Numerical estimate of gradients: 
\begin_inset Formula $\frac{\partial}{\partial\theta}J\left(\theta\right)\approx\frac{J\left(\theta+\epsilon\right)-J\left(\theta-\epsilon\right)}{2\epsilon}$
\end_inset

.
 Usually, we take 
\begin_inset Formula $\epsilon$
\end_inset

 to be very small, something like 
\begin_inset Formula $10^{-4}$
\end_inset

.
\end_layout

\begin_layout Itemize
Implementation in Matlab: 
\family typewriter
gradApprox = (J(theta + eps) - J(theta - eps))/(2*eps);
\end_layout

\end_deeper
\begin_layout Itemize
Gradient checking with a parameter vector 
\begin_inset Formula $\theta\in R^{n}$
\end_inset

 (
\begin_inset Formula $\theta$
\end_inset

 is the unrolled version of 
\begin_inset Formula $\Theta^{\left(1\right)},\Theta^{\left(2\right)}$
\end_inset

, etc.).
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta=\left[\theta_{1},\theta_{2},...,\theta_{n}\right]$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{i}}J\left(\theta\right)\approx\frac{J\left(\theta_{1},...,\theta_{i}+\epsilon,...,\theta_{n}\right)-J\left(\theta_{1},...,\theta_{i}-\epsilon,...,\theta_{n}\right)}{2\epsilon}$
\end_inset


\end_layout

\begin_layout Itemize
These equations give a way to numerically approximate the gradient with
 respect to any of your parameters.
\end_layout

\begin_layout Itemize
Implementation in Matlab:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

for i=1:n
\end_layout

\begin_layout Plain Layout

	thetaPlus = theta;
\end_layout

\begin_layout Plain Layout

	thetaPlus(i) = thetaPlus(i) + epsilon;
\end_layout

\begin_layout Plain Layout

	thetaMinus = theta;
\end_layout

\begin_layout Plain Layout

	thetaMinus(i) = thetaMinus(i) - epsilon;
\end_layout

\begin_layout Plain Layout

	gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon);
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Implemenation:
\end_layout

\begin_deeper
\begin_layout Itemize
Implement backpropagation to compute 
\family typewriter
DVec
\family default
.
\end_layout

\begin_layout Itemize
Implement numerical gradient checking to compute 
\family typewriter
gradApprox
\family default
.
\end_layout

\begin_layout Itemize
Make sure they give similar values for a few test cases.
\end_layout

\begin_layout Itemize
Turn off gradient checking; just use backpropagation for future runs.
\end_layout

\end_deeper
\begin_layout Itemize
Important: make sure to disable your gradient checking code before training
 your classifier, otherwise your code will be very slow.
\end_layout

\end_deeper
\begin_layout Itemize
Random initialization: we need to pick some initial value for 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
For gradient descent, we previously set 
\family typewriter
initialTheta = zeros(n,1)
\family default
.
\end_layout

\begin_deeper
\begin_layout Itemize
This doesn't work for training a neural network; it will result in all nodes
 being the same and all deltas being the same.
 All of the partial derivatives will be equal to each other, as well.
\end_layout

\end_deeper
\begin_layout Itemize
So, for a neural network, we need to use a random choice for 
\family typewriter
initialTheta
\family default
 between 
\begin_inset Formula $\left[-\epsilon,\epsilon\right]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
Theta1 = rand(10,11)*(2*initEps) - initEps;
\end_layout

\begin_layout Itemize

\family typewriter
Theta2 = rand(1,11)*(2*initEps) - initEps;
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Putting it all together:
\end_layout

\begin_deeper
\begin_layout Itemize
First, pick some network architecture, or connectivity pattern between neurons.
\end_layout

\begin_deeper
\begin_layout Itemize
Number of input units should be the number of features.
\end_layout

\begin_layout Itemize
Number of output units should be the number of possible classifications.
\end_layout

\begin_layout Itemize
Reasonable default: one hidden layer, or if more than one hidden layer,
 have the same number of units in each hidden layer (usually the more, the
 better, although more units requires more computational power).
\end_layout

\end_deeper
\begin_layout Itemize
Next, we need to train a neural network.
\end_layout

\begin_deeper
\begin_layout Itemize
First, randomly initialize the weights.
\end_layout

\begin_layout Itemize
Then, implement forward propagation to get 
\begin_inset Formula $h_{\Theta}\left(x^{\left(i\right)}\right)$
\end_inset

 for any 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Implement code to compute the cost function 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Implement backpropagation to compute partial derivatives 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{jk}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Usually requires a for loop.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Use gradient checking to compare the partial derivatives computed using
 backpropagation to those computed numerically for a few examples.
\end_layout

\begin_layout Itemize
Disable gradient checking code if everything seems OK.
\end_layout

\begin_layout Itemize
Use gradient descent or advanced optimization methods with backpropagation
 to try to minimize 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 as a function of the parameters 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Note: 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 is not necessarily convex, so it is possible for the algorithms to get
 stuck in local optima.
\end_layout

\end_deeper
\end_deeper
\begin_layout Section*
Advice for applying machine learning
\end_layout

\begin_layout Subsection*
Evaluating a learning algorithm
\end_layout

\begin_layout Itemize
How do you decide what the most promising avenues to explore are?
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose you have implemented regularized linear regression to predict housing
 prices.
\end_layout

\begin_layout Itemize
However, when you test your hypothesis on a new set of houses, you find
 that it makes unacceptably large errors in its predictions.
\end_layout

\begin_layout Itemize
What should you try next?
\end_layout

\begin_deeper
\begin_layout Itemize
Get more training examples, although this doesn't always help.
\end_layout

\begin_layout Itemize
Spend time trying to carefully select a smaller set of features.
\end_layout

\begin_layout Itemize
Try getting additional features.
\end_layout

\begin_layout Itemize
Try adding polynomial features.
\end_layout

\begin_layout Itemize
Try increasing or decreasing the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Machine learning diagnostics: tests you can run to gain insight into what
 is/isn't working with a learning algorithm, and gain guidance as to how
 best to improve its performance.
\end_layout

\begin_deeper
\begin_layout Itemize
Diagnostics can take time to implement, but doing so can be a very good
 use of your time.
\end_layout

\end_deeper
\begin_layout Itemize
Evaluating your hypothesis:
\end_layout

\begin_deeper
\begin_layout Itemize
Split your data into two portions: training and testing datasets.
\end_layout

\begin_deeper
\begin_layout Itemize
Typical split is about 70% training, 30% testing.
\end_layout

\begin_layout Itemize
\begin_inset Formula $m_{test}$
\end_inset

: number of test examples.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left(x_{test}^{\left(i\right)},y_{test}^{\left(i\right)}\right)$
\end_inset

: 
\begin_inset Formula $i$
\end_inset

th example from test dataset.
\end_layout

\begin_layout Itemize
Should usually divide dataset by randomly choosing samples since there may
 sometimes be some order to the samples.
\end_layout

\end_deeper
\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Learn parameters 
\begin_inset Formula $\theta$
\end_inset

 from training data by minimizing training error 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Compute test set error 
\begin_inset Formula $J_{test}\left(\theta\right)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}\left(h_{\theta}\left(x_{test}^{\left(i\right)}\right)-y_{test}^{\left(i\right)}\right)^{2}$
\end_inset

 (this example is for linear regression).
\end_layout

\begin_layout Itemize
Alternate definition of test set error: 0/1 misclassification error.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $err\left(h_{\theta}\left(x\right),y\right)=\left\{ \begin{array}{c}
1,h_{\theta}\left(x\right)\geq0.5,y=0|h_{\theta}\left(x\right)<0.5,y=1\\
0,\text{otherwise}
\end{array}\right.$
\end_inset


\end_layout

\begin_layout Itemize
Test error: 
\begin_inset Formula $\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
When parameters are fit using a training set, the error of the parameters
 as measured on that data (training set error) is likely to be lower than
 when you measure the error on another, independent dataset (generalization
 error).
\end_layout

\begin_layout Itemize
Model selection
\end_layout

\begin_deeper
\begin_layout Itemize
Say you have a choice of models with different degrees of polynomials,.
\end_layout

\begin_layout Itemize
It's as if you have another parameter, 
\begin_inset Formula $d$
\end_inset

, which is the highest degree of the polynomial used in the model.
\end_layout

\begin_layout Itemize
You could fit each model to your training data and get a set of parameters
 for each model 
\begin_inset Formula $\theta^{\left(d\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Then you can look at the test set and compute the cost for each model: 
\begin_inset Formula $J_{test}\left(\theta^{\left(d\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, rind the model with the lowest cost.
 For this example, assume it's the model with parameters 
\begin_inset Formula $\theta^{\left(5\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Question: how well does the model generalize? Could report test set error
 
\begin_inset Formula $J_{test}\left(\theta^{\left(5\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Problem: 
\begin_inset Formula $J_{test}\left(\theta^{\left(5\right)}\right)$
\end_inset

 is likely to be optimistic because our extra parameter 
\begin_inset Formula $d$
\end_inset

 is fit to our test data set.
\end_layout

\begin_layout Itemize
Thus, we now need to split our dataset into three pieces:
\end_layout

\begin_deeper
\begin_layout Itemize
Training dataset (
\begin_inset Formula $\approx60\%$
\end_inset

).
\end_layout

\begin_layout Itemize
Cross-validation (CV) dataset, sometimes also called validation dataset
 (
\begin_inset Formula $\approx20\%$
\end_inset

).
 
\begin_inset Formula $m_{cv}$
\end_inset

 is the number of CV examples.
\end_layout

\begin_layout Itemize
Test dataset (
\begin_inset Formula $\approx20\%$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize
Can calculate the CV and test errors just as we have for the training dataset.
\end_layout

\begin_layout Itemize
For the best model selection strategy, calculate 
\begin_inset Formula $J_{cv}\left(\theta^{\left(d\right)}\right)$
\end_inset

.
 Then you can safely use 
\begin_inset Formula $J_{test}\left(\theta^{\left(d\right)}\right)$
\end_inset

 for the best model to determine the generalization error for that model.
\end_layout

\end_deeper
\begin_layout Subsection*
Bias and variance
\end_layout

\begin_layout Itemize
High bias is equivalent to underfitting the data.
\end_layout

\begin_layout Itemize
High variance is equivalent to overfitting the data.
\end_layout

\begin_layout Itemize
Idea for diagnosing bias/variance:
\end_layout

\begin_deeper
\begin_layout Itemize
Plot training error vs.
 degree of polynomial 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Plot cross-validation error vs.
 degree of polynomial 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Training error will probably decrease with 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Cross-validation error will most likely reach a minimum at a certain value
 of 
\begin_inset Formula $d$
\end_inset

, then start increasing again.
\end_layout

\begin_layout Itemize
High bias: high training error and high CV error.
 
\begin_inset Formula $J_{train}\left(\theta\right)\approx J_{cv}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
High variance: low training error, high CV error.
\end_layout

\end_deeper
\begin_layout Itemize
Regularization can help prevent overfitting, but how does it affect bias
 and variance?
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we are fitting a high-order polynomial.
\end_layout

\begin_layout Itemize
How do we choose the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

 to minimize bias and variance?
\end_layout

\begin_deeper
\begin_layout Itemize
Try a range of 
\begin_inset Formula $\lambda$
\end_inset

 values.
\end_layout

\begin_layout Itemize
For each 
\begin_inset Formula $\lambda$
\end_inset

 value, we find the set of parameters 
\begin_inset Formula $\theta$
\end_inset

 which minimizes 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, apply this set of parameters to the cross-validation dataset and pick
 whichever 
\begin_inset Formula $\lambda$
\end_inset

 ends up giving the lowest 
\begin_inset Formula $J_{cv}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then, apply this to the test set.
\end_layout

\begin_layout Itemize
This is model selection applied to the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Learning curves
\end_layout

\begin_deeper
\begin_layout Itemize
Plotting 
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $J_{cv}\left(\theta\right)$
\end_inset

 versus 
\begin_inset Formula $m$
\end_inset

 (training set size).
\end_layout

\begin_layout Itemize
Lets you see how your algorithm changes with more training examples.
\end_layout

\begin_layout Itemize
Average training error will grow with 
\begin_inset Formula $m$
\end_inset

 (easy to fit only a few data points, harder to fit many).
 Will usually start to plateau at a certain point.
\end_layout

\begin_layout Itemize
Cross-validation error will tend to decrease as 
\begin_inset Formula $m$
\end_inset

 increases.
\end_layout

\begin_layout Itemize
High bias (underfitting) case:
\end_layout

\begin_deeper
\begin_layout Itemize
Cross-validation error will initially decrease with 
\begin_inset Formula $m$
\end_inset

, but will plateau relatively quickly.
\end_layout

\begin_layout Itemize
Training error will start small and increase with 
\begin_inset Formula $m$
\end_inset

, and will eventually plateau and end up very close to the cross-validation
 error.
\end_layout

\begin_layout Itemize
If a learning algorithm has high bias, getting more training data will not
 help very much on its own.
\end_layout

\end_deeper
\begin_layout Itemize
High variance (overfitting) case:
\end_layout

\begin_deeper
\begin_layout Itemize
Training error will start small and increase very slowly with 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Itemize
Cross-validation error will start high and decrease very slowly with 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Itemize
There will be a big gap between the training and cross-validation error.
\end_layout

\begin_layout Itemize
If a learning algorithm has high variance, getting more training data is
 likely to help.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Debugging a learning algorithm
\end_layout

\begin_deeper
\begin_layout Itemize
Getting more training examples will help with high variance cases.
\end_layout

\begin_layout Itemize
Trying a smaller set of features will also help with high variance cases.
\end_layout

\begin_layout Itemize
Adding features will help with high bias problems.
\end_layout

\begin_layout Itemize
Adding polynomial features will also help with high bias problems.
\end_layout

\begin_layout Itemize
Decreasing 
\begin_inset Formula $\lambda$
\end_inset

 will help fix high bias problems.
\end_layout

\begin_layout Itemize
Increasing 
\begin_inset Formula $\lambda$
\end_inset

 will help fix high variance problems.
\end_layout

\end_deeper
\begin_layout Itemize
Neural networks and overfitting.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Small
\begin_inset Quotes erd
\end_inset

 neural network: fewer parameters, computationally cheaper, more prone to
 underfitting.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Large
\begin_inset Quotes erd
\end_inset

 neural network: more parameters, computationally more expensive, more prone
 to overfitting.
 Use regularization to address overfitting.
\end_layout

\end_deeper
\begin_layout Section*
Machine learning system design
\end_layout

\begin_layout Subsection*
Building a spam classifier
\end_layout

\begin_layout Itemize
Supervised learning: how do we want to define 
\begin_inset Formula $x$
\end_inset

, the set of features of the e-mail?
\end_layout

\begin_deeper
\begin_layout Itemize
Could choose 
\begin_inset Formula $\approx$
\end_inset

100 words indicative of spam/not spam.
 (e.g., deal, buy, discount, your name, etc.)
\end_layout

\begin_layout Itemize
Given an e-mail, parse it and record whether each word occurs or not.
\end_layout

\begin_layout Itemize
Another option is to look through a training set of spam e-mail and pick
 out the most frequently occurring words automatically, rather than choosing
 the features manually.
\end_layout

\end_deeper
\begin_layout Itemize
How to spend your time to make your algorithm have low error?
\end_layout

\begin_deeper
\begin_layout Itemize
Collect lots of data.
\end_layout

\begin_layout Itemize
Develop sophisticated features based on e-mail routing information from
 e-mail header.
\end_layout

\begin_layout Itemize
Develop sophisticated features for message body.
\end_layout

\begin_deeper
\begin_layout Itemize
Should 
\begin_inset Quotes eld
\end_inset

deal
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Dealer
\begin_inset Quotes erd
\end_inset

 be treated as the same word?
\end_layout

\begin_layout Itemize
Features about punctuation (spam uses more exclamation points).
\end_layout

\end_deeper
\begin_layout Itemize
Detect misspellings (m0rtgage, w4tches, etc.).
\end_layout

\end_deeper
\begin_layout Itemize
Recommended approach:
\end_layout

\begin_deeper
\begin_layout Itemize
Start with a simple algorithm that you can implement quickly.
\end_layout

\begin_layout Itemize
Implement it and test on your cross-validation data set.
\end_layout

\begin_layout Itemize
Plot learning curves to decide if more data, more features, etc.
 are likely to help.
\end_layout

\begin_layout Itemize
Error analysis: manually examine the examples (in the cross-validation data
 set) that your algorithm made errors on.
 See if you spot any systematic trend in what type of examples it is making
 errors on.
\end_layout

\end_deeper
\begin_layout Itemize
Example:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $m_{CV}=500$
\end_inset

 examples in CV data set.
\end_layout

\begin_layout Itemize
Algorithm misclassifies 100 e-mails.
\end_layout

\begin_layout Itemize
Manually examine the 100 errors and categorize them baesd on:
\end_layout

\begin_deeper
\begin_layout Itemize
What type of e-mail it is.
\end_layout

\begin_layout Itemize
What cues (features) you think would have helped the algorithm classify
 them correctly.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The importance of numerical evaluation
\end_layout

\begin_deeper
\begin_layout Itemize
Should discount/discounts/discounted/discounting be treated as the same
 word?
\end_layout

\begin_layout Itemize
In natural language processing, can use 
\begin_inset Quotes eld
\end_inset

stemming
\begin_inset Quotes erd
\end_inset

 software (e.g.
 
\begin_inset Quotes eld
\end_inset

porter stemmer
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Itemize
Difficult example: universe/university.
\end_layout

\begin_layout Itemize
Error analysis may not be helpful for deciding whether stemming would be
 useful.
 Best approach is to just try it and see if it works (i.e., is the cross-validatio
n error lower with or without stemming?).
\end_layout

\begin_layout Itemize
Need numerical evaluation (e.g., cross-validation error) of algorithm's performanc
e with/without stemming.
\end_layout

\begin_layout Itemize
Distinguish between upper/lower case.
\end_layout

\end_deeper
\begin_layout Subsection*
Handling skewed data
\end_layout

\begin_layout Itemize
Example: cancer classification.
\end_layout

\begin_deeper
\begin_layout Itemize
Train logistic regression model 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

; 
\begin_inset Formula $y=1$
\end_inset

 means that the patient has cancer.
\end_layout

\begin_layout Itemize
Find that we get 1% error on test set (99% correct diagnoses).
\end_layout

\begin_layout Itemize
But if only 0.50% of patients have cancer, our result is not particularly
 good.
\end_layout

\begin_layout Itemize
This is an example of skewed classes, where we have a lot more of one class
 than the other.
\end_layout

\begin_layout Itemize
In this case, classification accuracy is not very useful.
\end_layout

\end_deeper
\begin_layout Itemize
Precision/recall evaluation metric:
\end_layout

\begin_deeper
\begin_layout Itemize
Take 
\begin_inset Formula $y=1$
\end_inset

 in presence of rare class that we want to detect.
\end_layout

\begin_layout Itemize
True positive: predicted class is 1 and actual class is 1.
\end_layout

\begin_layout Itemize
True negative: predicted class is 0 and actual class is 0.
\end_layout

\begin_layout Itemize
False positive: predicted class is 1 and actual class is 0.
\end_layout

\begin_layout Itemize
False negative: predicted class is 0 and actual class is 1.
\end_layout

\begin_layout Itemize
Precision: of all patients where we predicted 
\begin_inset Formula $y=1$
\end_inset

, what fraction actually has cancer?
\end_layout

\begin_deeper
\begin_layout Itemize
Equal to number of true positives divided by the total number of predicted
 positives: 
\begin_inset Formula $N_{TP}/\left(N_{TP}+N_{FP}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Recall: of all patients that actually have cancer, what fraction did we
 correctly detect as having cancer?
\end_layout

\begin_deeper
\begin_layout Itemize
Equal to number of true positives divided by the number of actual positives:
 
\begin_inset Formula $N_{TP}/\left(N_{TP}+N_{FN}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Also called 
\series bold
sensitivity
\series default
.
\end_layout

\end_deeper
\begin_layout Itemize
This may be more useful in general for skewed classes than classification
 accuracy would be.
\end_layout

\end_deeper
\begin_layout Itemize
Example: suppose we want to predict 
\begin_inset Formula $y=1$
\end_inset

 (cancer) only if very confident.
\end_layout

\begin_deeper
\begin_layout Itemize
Could increase our prediction threshold:
\end_layout

\begin_deeper
\begin_layout Itemize
Predict 1 if 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.7$
\end_inset

.
\end_layout

\begin_layout Itemize
Predict 0 if 
\begin_inset Formula $h_{\theta}\left(x\right)<0.7$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
This will increase our precision - more of our predictions will be true.
\end_layout

\begin_layout Itemize
It will also lower the recall because we predict fewer people will have
 cancer.
\end_layout

\end_deeper
\begin_layout Itemize
Example: suppose we want to avoid missing too many cases of cancer (avoid
 false negatives).
\end_layout

\begin_deeper
\begin_layout Itemize
Could decrease our prediction threshold:
\end_layout

\begin_deeper
\begin_layout Itemize
Predict 1 if 
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.3$
\end_inset

.
\end_layout

\begin_layout Itemize
Predict 0 if 
\begin_inset Formula $h_{\theta}\left(x\right)<0.3$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
This will give higher recall - we will flag almost all patients who do have
 cancer.
\end_layout

\begin_layout Itemize
But the precision will be lower - a higher fraction of the patients who
 we predict to have cancer will not actually have cancer.
\end_layout

\end_deeper
\begin_layout Itemize
Main point: you can vary the value of your threshold and plot a curve which
 shows a precision vs.
 recall threshold.
\end_layout

\begin_layout Itemize
Question: is there a way to choose this threshold optimally?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $F_{1}$
\end_inset

 score: tells us how to compare/weight precision and recall.
 (also just called F score)
\end_layout

\begin_layout Itemize
\begin_inset Formula $F_{1}=2\frac{PR}{P+R}$
\end_inset

.
 Like taking an average, but gives the lower score more weight.
\end_layout

\begin_layout Subsection*
Data for machine learning
\end_layout

\begin_layout Itemize
Designing a high-accuracy learning system
\end_layout

\begin_deeper
\begin_layout Itemize
Example: classify between confusable words (to, two, too; then, than).
\end_layout

\begin_deeper
\begin_layout Itemize
Algorithms: Perceptron (logistic regression), Winnow, memory-based, naive
 Bayes.
\end_layout

\end_deeper
\begin_layout Itemize
Test all algorithms and vary training set size - as training set size increases,
 at a certain point, most algorithms end up getting very similar results.
\end_layout

\begin_layout Itemize
Saying: 
\begin_inset Quotes eld
\end_inset

It's not who has the best algorithm that wins, it's who has the most data.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Large data rationale
\end_layout

\begin_deeper
\begin_layout Itemize
Assume features 
\begin_inset Formula $x\in R^{n+1}$
\end_inset

 has sufficient information to predict 
\begin_inset Formula $y$
\end_inset

 accurately.
\end_layout

\begin_layout Itemize
Example: for breakfast I ate _____ eggs.
 (algorithm should put in 
\emph on
two
\emph default
)
\end_layout

\begin_layout Itemize
Counterexample: predict housing price from only size (square feet) and no
 other features.
\end_layout

\begin_layout Itemize
Useful test: given the input 
\begin_inset Formula $x$
\end_inset

, can a human expert confidently predict 
\begin_inset Formula $y$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Itemize
Can help determine if we have enough information for the learning algorithm
 to do a good job.
\end_layout

\end_deeper
\begin_layout Itemize
Use a learning algorithm with many parameters (e.g., logistic/linear regression
 with many features or a neural network with many hidden units).
\end_layout

\begin_deeper
\begin_layout Itemize
This should give low bias and 
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

 should be small.
\end_layout

\end_deeper
\begin_layout Itemize
Use a very large training set (unlikely to overfit).
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $J_{train}\left(\theta\right)\approx J_{test}\left(\theta\right)$
\end_inset

, then 
\begin_inset Formula $J_{test}\left(\theta\right)$
\end_inset

 will most likely be small.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Section*
Support vector machines
\end_layout

\begin_layout Subsection*
Large margin classification
\end_layout

\begin_layout Itemize
Alternative view of logistic regression:
\end_layout

\begin_deeper
\begin_layout Itemize
Think about what we want logistic regression to do.
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

, we want 
\begin_inset Formula $h_{\theta}\left(x\right)\approx1$
\end_inset

, 
\begin_inset Formula $\theta^{T}x\gg0$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y=0$
\end_inset

, we want 
\begin_inset Formula $h_{\theta}\left(x\right)\approx0$
\end_inset

, 
\begin_inset Formula $\theta^{T}x\ll0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Cost of example: 
\begin_inset Formula $-\left(y\log h_{\theta}\left(x\right)+\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $y=1$
\end_inset

, the cost is 
\begin_inset Formula $-\log\left(h_{\theta}\left(x\right)\right)=-\log\left(\frac{1}{1+\exp\left(-\theta^{T}x\right)}\right)$
\end_inset

; as 
\begin_inset Formula $\theta^{T}x$
\end_inset

 gets large, the cost becomes small.
\end_layout

\end_deeper
\begin_layout Itemize
To make a support vector machine, we use a 
\begin_inset Quotes eld
\end_inset

simpler
\begin_inset Quotes erd
\end_inset

 cost function: instead we use two lines.
\end_layout

\begin_deeper
\begin_layout Itemize
Cost = 0 for 
\begin_inset Formula $z=\theta^{T}x>1$
\end_inset

 (for 
\begin_inset Formula $y=1$
\end_inset

).
\end_layout

\begin_layout Itemize
Cost = linear function which follows curve for 
\begin_inset Formula $z<1$
\end_inset

 (for 
\begin_inset Formula $y=1$
\end_inset

).
\end_layout

\begin_layout Itemize
Do the same thing for the case where 
\begin_inset Formula $y=0$
\end_inset

.
\end_layout

\begin_layout Itemize
This will give us a computational advantage.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Support vector machine cost function: 
\begin_inset Formula $J\left(\theta\right)=C\sum_{i=1}^{m}y^{\left(i\right)}\text{cost}_{1}\left(\theta^{T}x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\text{cost}_{0}\left(\theta^{T}x^{\left(i\right)}\right)+\frac{1}{2}\sum_{i=0}^{n}\theta_{j}^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We ignore the factor of 
\begin_inset Formula $m$
\end_inset

 that was in previous cost functions because it's just a normalization factor.
\end_layout

\begin_layout Itemize
We also pull out 
\begin_inset Formula $\lambda$
\end_inset

from the regularization term.
 
\begin_inset Formula $C=\frac{1}{\lambda}$
\end_inset

 is now the coefficient of the first term.
\end_layout

\end_deeper
\begin_layout Itemize
We could have set our threshold at 
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

 for 
\begin_inset Formula $y=1$
\end_inset

 (and similarly for 
\begin_inset Formula $y=0$
\end_inset

), but putting at 
\begin_inset Formula $\theta^{T}x\geq1$
\end_inset

 means a stronger prediction.
\end_layout

\begin_layout Itemize
Consider a case where we set 
\begin_inset Formula $C$
\end_inset

 to be a very large value, like 
\begin_inset Formula $10^{5}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
When minimizing the cost function, it will try to set the first term to
 be 0.
\end_layout

\begin_layout Itemize
This requires we find 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\geq0$
\end_inset

 for 
\begin_inset Formula $y^{\left(i\right)}=1$
\end_inset

 and 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\leq-1$
\end_inset

 for 
\begin_inset Formula $y^{\left(i\right)}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
End result is a very interesting decision boundary.
 For a linearly separable case, the decision boundary will be close to what
 appears optimal by eye.
\end_layout

\begin_deeper
\begin_layout Itemize
This 
\begin_inset Quotes eld
\end_inset

distance
\begin_inset Quotes erd
\end_inset

 between the two classes is the 
\emph on
margin
\emph default
, the SVM tries to separate the two classes with as large of a margin as
 possible.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Large margin classifiers can be sensitive to outliers.
\end_layout

\begin_deeper
\begin_layout Itemize
Can use an intermediate value for 
\begin_inset Formula $C$
\end_inset

 instead of a very large one in order to reduce the effect of outliers.
\end_layout

\end_deeper
\begin_layout Itemize
Vector stuff
\end_layout

\begin_deeper
\begin_layout Itemize
Magnitude: 
\begin_inset Formula $\left|u\right|=\sqrt{u_{1}^{2}+u_{2}^{2}}$
\end_inset


\end_layout

\begin_layout Itemize
Inner product: 
\begin_inset Formula $u\cdot v=\sum_{i}u_{i}v_{i}=u^{T}v=v^{T}u=p\left|u\right|$
\end_inset

.
 This is equal to the length of the projection of 
\begin_inset Formula $v$
\end_inset

 onto 
\begin_inset Formula $u$
\end_inset

 (
\begin_inset Formula $p$
\end_inset

) multiplied by the magnitude of 
\begin_inset Formula $u$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
SVM decision boundary
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\text{min}\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

, such that 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\geq1$
\end_inset

 if 
\begin_inset Formula $y^{\left(i\right)}=1$
\end_inset

 and 
\begin_inset Formula $\theta^{T}x^{\left(i\right)}\leq-1$
\end_inset

 if 
\begin_inset Formula $y^{\left(i\right)}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
Can think of 
\begin_inset Formula $\sum_{j=1}^{n}\theta_{j}^{2}=\left(\sqrt{\sum_{j=1}^{n}\theta_{j}^{2}}\right)^{2}$
\end_inset

, which is like the magnitude of the parameter vector theta:
\begin_inset Formula $\left|\theta\right|^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Note: we make a simplification and take 
\begin_inset Formula $\theta_{0}$
\end_inset

 to be 0 (this means that the decision boundary will pass through the origin).
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\theta^{T}x=p^{\left(i\right)}\cdot\left|\theta\right|$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Kernels
\end_layout

\begin_layout Itemize
Question: is there a different or better way to choose features?
\end_layout

\begin_layout Itemize
Example: given 
\begin_inset Formula $x$
\end_inset

, compute new features based on proximity to landmarks 
\begin_inset Formula $l^{1}$
\end_inset

, 
\begin_inset Formula $l^{2}$
\end_inset

, 
\begin_inset Formula $l^{3}$
\end_inset

 (points randomly chosen to be 
\begin_inset Quotes eld
\end_inset

important
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

landmarks
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
Given 
\begin_inset Formula $x$
\end_inset

: 
\begin_inset Formula $f_{1}=similarity\left(x,l^{1}\right)=\exp\left(-\frac{\left|x-l^{1}\right|^{2}}{2\sigma^{2}}\right)$
\end_inset

, similarly for 
\begin_inset Formula $f_{2},f_{3}$
\end_inset

.
\end_layout

\begin_layout Itemize
Here, the formula for similarity is called the 
\emph on
kernel
\emph default
.
 In this case, we use a Gaussian kernel.
 Can also be written as 
\begin_inset Formula $k\left(x,l^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Kernels and similarity
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $f_{1}=similarity\left(x,l^{1}\right)=\exp\left(-\frac{\left|x-l^{1}\right|^{2}}{2\sigma^{2}}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $x\approx l^{1}$
\end_inset

, then 
\begin_inset Formula $f_{1}\approx1$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $x$
\end_inset

 is far from 
\begin_inset Formula $l^{1}$
\end_inset

, then 
\begin_inset Formula $f_{1}\approx0$
\end_inset

.
\end_layout

\begin_layout Itemize
This feature 
\begin_inset Formula $f_{1}$
\end_inset

 measures how close 
\begin_inset Formula $x$
\end_inset

 is to the first landmark, 
\begin_inset Formula $l^{1}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma$
\end_inset

 is a parameter of the kernel.
\end_layout

\begin_deeper
\begin_layout Itemize
Larger 
\begin_inset Formula $\sigma$
\end_inset

 means a more spread out Gaussian distribution.
\end_layout

\end_deeper
\begin_layout Itemize
How do we choose the landmarks?
\end_layout

\begin_deeper
\begin_layout Itemize
Start by choosing the positions of actual samples as landmarks (1 landmark
 per sample).
\end_layout

\begin_layout Itemize
This says your features will measure how close an example is to something
 from the training set.
\end_layout

\begin_layout Itemize
For each new example, there will be 
\begin_inset Formula $m$
\end_inset

 features, one for each landmark.
\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis: given 
\begin_inset Formula $x$
\end_inset

, compute features 
\begin_inset Formula $f\in R^{m+1}$
\end_inset

, predict 
\begin_inset Formula $y=1$
\end_inset

 if 
\begin_inset Formula $\theta^{T}f\geq0$
\end_inset

.
 (
\begin_inset Formula $\theta\in R^{m+1}$
\end_inset

)
\end_layout

\begin_layout Itemize
Training: minimize cost function 
\begin_inset Formula $\text{min}C\sum_{i=1}^{m}y^{\left(i\right)}cost_{1}\left(\theta^{T}f^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)cost_{0}\left(\theta^{T}f^{\left(i\right)}\right)+\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2}$
\end_inset

 (here, 
\begin_inset Formula $n=m$
\end_inset

, since the number of features is equal to the number of samples).
\end_layout

\begin_deeper
\begin_layout Itemize
Can write last term as 
\begin_inset Formula $\sum_{j=1}^{n}\theta_{j}^{2}=\theta^{T}\theta$
\end_inset

 (ignoring 
\begin_inset Formula $\theta_{0}$
\end_inset

).
\end_layout

\begin_layout Itemize
Most SVMs replace this with 
\begin_inset Formula $\theta^{T}M\theta$
\end_inset

, where 
\begin_inset Formula $M$
\end_inset

 is a matrix generally chosen based on your kernel.
 This is a mathematical detail that allows the algorithm to run much more
 efficiently.
\end_layout

\end_deeper
\begin_layout Itemize
Can apply kernel method for things like logistic regression, but it doesn't
 work quite as well.
 SVMs use some computational tricks that logistic regression doesn't.
\end_layout

\begin_layout Itemize
Don't recommend to write software to minimize the SVM cost function, use
 stuff that has already been developed.
\end_layout

\begin_layout Itemize
SVM parameters:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $C=1/\lambda$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Large 
\begin_inset Formula $C$
\end_inset

: lower bias, higher variance.
\end_layout

\begin_layout Itemize
Small 
\begin_inset Formula $C$
\end_inset

: higher bias, lower variance.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Large 
\begin_inset Formula $\sigma^{2}$
\end_inset

: features 
\begin_inset Formula $f_{i}$
\end_inset

 vary more smoothly.
 Higher bias, lower variance.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Using an SVM
\end_layout

\begin_layout Itemize
Use SVM software package (liblinear, libsvm, etc.) to solve for parameters
 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
Need to specify:
\end_layout

\begin_deeper
\begin_layout Itemize
Choice of parameter 
\begin_inset Formula $C$
\end_inset

.
\end_layout

\begin_layout Itemize
Choice of kernel or similarity function.
\end_layout

\begin_deeper
\begin_layout Itemize
No kernel (linear kernel): predict 
\begin_inset Formula $y=1$
\end_inset

 if 
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Useful if number of features is large and number of examples is small.
\end_layout

\end_deeper
\begin_layout Itemize
Gaussian kernel: 
\begin_inset Formula $f_{i}=\exp\left(-\frac{\left|x-l^{\left(i\right)}\right|^{2}}{2\sigma^{2}}\right)$
\end_inset

, where 
\begin_inset Formula $l^{\left(i\right)}=x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Need to choose 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Useful when number of features is small and/or number of examples is large.
\end_layout

\begin_layout Itemize
Note: need to perform feature scaling before using the Gaussian kernel if
 your features are on very different scales, this is because 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is defined the same way for all features.
\end_layout

\end_deeper
\begin_layout Itemize
Polynomial kernel: 
\begin_inset Formula $k\left(x,l\right)=\left(x^{T}l+C\right)^{d}$
\end_inset

, 
\begin_inset Formula $C$
\end_inset

 is a constant and 
\begin_inset Formula $d$
\end_inset

 is the degree of the polynomial.
 Usually worse than a Gaussian kernel but can be useful in some situations.
\end_layout

\begin_layout Itemize
Others: string kernel, chi-square kernel, histogram intersection kernel,
 etc.
\end_layout

\begin_layout Itemize
Not all similarity functions make valid kernels.
 They technically need to satisfy Mercer's theorem to make sure that SVM
 packages' optimizations run correctly and do not diverge.
\end_layout

\end_deeper
\begin_layout Itemize
You may need to implement the kernel or similarity function on your own.
\end_layout

\end_deeper
\begin_layout Itemize
Multi-class classification
\end_layout

\begin_deeper
\begin_layout Itemize
Many SVM packages already have this functionality build in.
\end_layout

\begin_layout Itemize
Otherwise, use the one-vs-all method.
 Train 
\begin_inset Formula $K$
\end_inset

 SVMs, one to distinguish 
\begin_inset Formula $y=i$
\end_inset

 from the rest for 
\begin_inset Formula $i=1,2,...,K$
\end_inset

, get 
\begin_inset Formula $\theta^{\left(1\right)},\theta^{\left(2\right)},...,\theta^{\left(K\right)}$
\end_inset

, and pick the class 
\begin_inset Formula $i$
\end_inset

 with the largest 
\begin_inset Formula $\left(\theta^{\left(i\right)}\right)^{T}x$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Logistic regression vs.
 SVMs
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

: number of features.
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

: number of training examples.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $n$
\end_inset

 is large relative to 
\begin_inset Formula $m$
\end_inset

, use logistic regression, or an SVM with a linear kernel.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $n$
\end_inset

 is small (1 - 1000 or so) and 
\begin_inset Formula $m$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

intermediate
\begin_inset Quotes erd
\end_inset

 (10 - 10000 or so), use an SVM with a Gaussian kernel.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $n$
\end_inset

 is small and 
\begin_inset Formula $m$
\end_inset

 is large (50000+), create/add more features, then use logistic regression
 or an SVM with a linear kernel.
\end_layout

\begin_layout Itemize
A neural network will likely work well for most of these cases, but may
 be slower to train.
\end_layout

\end_deeper
\begin_layout Section*
Unsupervised learning
\end_layout

\begin_layout Subsection*
Clustering
\end_layout

\begin_layout Itemize
Unsupervised learning - our data does not have any labels associated with
 it!
\end_layout

\begin_deeper
\begin_layout Itemize
Training set: 
\begin_inset Formula $\left\{ x^{\left(1\right)},x^{\left(2\right)},...,x^{\left(m\right)}\right\} $
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
We want the algorithm to find some structure in our data, often by clustering.
\end_layout

\begin_layout Itemize
Uses: market segmentation, social network analysis, organization of computing
 clusters, astronomical data analysis.
\end_layout

\begin_layout Itemize
K-means clustering: most widely used clustering algorithm.
\end_layout

\begin_deeper
\begin_layout Itemize
An iterative algorithm:
\end_layout

\begin_deeper
\begin_layout Itemize
First: assign data to a cluster based on cluster centroid locations.
\end_layout

\begin_layout Itemize
Second: move cluster centroids to average position of data points in each
 cluster.
\end_layout

\begin_layout Itemize
Repeat until the cluster centroids stop moving.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
K-means inputs:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

: number of clusters you want to find in the data (will talk more later
 about how to choose 
\begin_inset Formula $K$
\end_inset

).
\end_layout

\begin_layout Itemize
Training set 
\begin_inset Formula $\left\{ x^{\left(1\right)},x^{\left(2\right)},...,x^{\left(m\right)}\right\} $
\end_inset

, where 
\begin_inset Formula $x^{\left(i\right)}\in R^{n}$
\end_inset

 (drop 
\begin_inset Formula $x_{0}=1$
\end_inset

 convention).
\end_layout

\end_deeper
\begin_layout Itemize
K-means algorithm:
\end_layout

\begin_deeper
\begin_layout Itemize
Randomly initialize 
\begin_inset Formula $K$
\end_inset

 cluster centroids 
\begin_inset Formula $\mu_{1},\mu_{2},...,\mu_{k}\in R^{n}$
\end_inset

.
\end_layout

\begin_layout Itemize
Repeat: 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

for i=1:m
\end_layout

\begin_layout Plain Layout

	ci = index (from 1 to K) of cluster centroid closest to xi
\end_layout

\begin_layout Plain Layout

for k=1:K
\end_layout

\begin_layout Plain Layout

	mu_k = average of points assigned to cluster k
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
If a cluster centroid has no points assigned to it, usually you just want
 to eliminate it - then you'll have 
\begin_inset Formula $K-1$
\end_inset

 clusters.
 Otherwise, if you really need 
\begin_inset Formula $K$
\end_inset

 clusters, you can randomly re-initialize that cluster centroid.
\end_layout

\end_deeper
\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $c^{\left(i\right)}$
\end_inset

: index of cluster (1,2,...,K) to which example 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

 is currently assigned.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mu_{k}$
\end_inset

: cluster centroid 
\begin_inset Formula $k$
\end_inset

 (
\begin_inset Formula $\mu_{k}\in R^{n}$
\end_inset

).
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mu_{c^{\left(i\right)}}:$
\end_inset

cluster centroid of cluster to which example 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

 has been assigned.
\end_layout

\end_deeper
\begin_layout Itemize
K-means optimization objective:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(c^{\left(1\right)},...,c^{\left(m\right)},\mu_{1},...,\mu_{K}\right)=\frac{1}{m}\sum_{i=1}^{m}\left|x^{\left(i\right)}-\mu_{c^{\left(i\right)}}\right|^{2}$
\end_inset


\end_layout

\begin_layout Itemize
I.e., we want to minimize the total squared distances between data points
 and their corresponding cluster centroid.
\end_layout

\begin_layout Itemize
We minimize the cost function by moving the cluster centroids.
\end_layout

\begin_layout Itemize
The cost function should never increase with more iterations.
\end_layout

\begin_layout Itemize
Note: the cost function is also called the 
\emph on
distortion
\emph default
 function for k-means clustering.
\end_layout

\end_deeper
\begin_layout Itemize
Random initialization of cluster centroids
\end_layout

\begin_deeper
\begin_layout Itemize
Number of cluster centroids 
\begin_inset Formula $K$
\end_inset

 should be less than number of training examples 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Itemize
Can end up with different solutions depending on your initialization; sometimes
 the algorithm can get caught in local optima.
\end_layout

\begin_layout Itemize
One method for initialization: randomly pick 
\begin_inset Formula $K$
\end_inset

 training examples, set 
\begin_inset Formula $\mu_{1},...,\mu_{K}$
\end_inset

 equal to these training examples.
\end_layout

\begin_layout Itemize
Can also repeat k-means clustering many times with many different random
 initializations, then pick the case with the overall lowest cost.
\end_layout

\begin_deeper
\begin_layout Itemize
If you're using a small number of clusters, 
\begin_inset Formula $2\lessapprox K\lessapprox10$
\end_inset

, many initializations can help make sure you find a better optimum.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $K>10$
\end_inset

, your first initialization will probably be pretty good already, more initializ
ations will help some, but not very much.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Choosing the number of clusters
\end_layout

\begin_deeper
\begin_layout Itemize
Not always clear how many clusters that actually are in the data.
\end_layout

\begin_layout Itemize
Elbow method:
\end_layout

\begin_deeper
\begin_layout Itemize
Run k-means with several different values of 
\begin_inset Formula $K$
\end_inset

 and plot the cost function 
\begin_inset Formula $J$
\end_inset

 vs.
 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_layout Itemize
You may see curve that looks like a bent arm.
 Try to choose the value of 
\begin_inset Formula $K$
\end_inset

 that is at the 
\begin_inset Quotes eld
\end_inset

elbow.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Not used that often - you usually don't see a nice, clear elbow; it's usually
 a smooth curve.
\end_layout

\end_deeper
\begin_layout Itemize
Another method: evaluate k-means based on a metric for how well it performs
 for its actual, downstream purpose.
\end_layout

\begin_deeper
\begin_layout Itemize
Example: picking T-shirt sizes (S, M, L vs.
 XS, S, M, L, XL).
 Think about how number of sizes might impact sales and manufacturing costs.
\end_layout

\end_deeper
\end_deeper
\begin_layout Section*
Dimensionality reduction
\end_layout

\begin_layout Subsection*
Motivation
\end_layout

\begin_layout Itemize
Can be useful when you have some features which are partly or fully correlated.
\end_layout

\begin_layout Itemize
Want to define a new feature which encapsulates one or more other features.
\end_layout

\begin_layout Itemize
Example: reduce data from 2D to 1D:
\end_layout

\begin_deeper
\begin_layout Itemize
Originally have two examples 
\begin_inset Formula $x^{\left(1\right)}\in R^{2}$
\end_inset

 and 
\begin_inset Formula $x^{\left(2\right)}\in R^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
If the two features are highly correlated, dimensionality reduction may
 let us convert to 
\begin_inset Formula $z^{\left(1\right)}\in R$
\end_inset

 and 
\begin_inset Formula $z^{\left(2\right)}\in R$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Dimensionality reduction may allow our algorithms to run more quickly -
 this is one of the main reasons we pursue it.
\end_layout

\begin_layout Itemize
Dimensionality reduction also helps us to visualize the data better.
\end_layout

\begin_deeper
\begin_layout Itemize
Hard to plot and visualize 50-dimensional data, but 2D or 3D data is easier.
\end_layout

\end_deeper
\begin_layout Subsection*
Principal component analysis (PCA)
\end_layout

\begin_layout Itemize
Tries to find a lower-dimensional surface to project the data onto such
 that it minimizes the orthogonal projection error (least-squares error).
\end_layout

\begin_layout Itemize
Should perform feature scaling before doing PCA.
\end_layout

\begin_layout Itemize
Problem formulation for reducing from 2D to 1D: find a direction (a vector
 
\begin_inset Formula $u^{\left(1\right)}\in R$
\end_inset

) onto which to project the data so as to minimize the projection error.
\end_layout

\begin_deeper
\begin_layout Itemize
Can generalize this to going from 
\begin_inset Formula $n$
\end_inset

 dimensions to 
\begin_inset Formula $k$
\end_inset

 dimensions.
\end_layout

\end_deeper
\begin_layout Itemize
How does PCA relate to linear regression?
\end_layout

\begin_deeper
\begin_layout Itemize
They are not the same!
\end_layout

\begin_layout Itemize
Linear regression: minimize vertical distance between points and regression
 line.
 Trying to use 
\begin_inset Formula $x_{1}$
\end_inset

 to predict 
\begin_inset Formula $x_{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
PCA: minimize 
\emph on
orthogonal
\emph default
 distance between points and projection.
\end_layout

\end_deeper
\begin_layout Itemize
Implementation:
\end_layout

\begin_deeper
\begin_layout Itemize
Perform feature scaling and mean normalization: 
\begin_inset Formula $\mu_{j}=\sum_{i=1}^{m}x_{j}^{\left(i\right)}$
\end_inset

; replace each 
\begin_inset Formula $x_{j}^{\left(i\right)}$
\end_inset

 with 
\begin_inset Formula $x_{j}-\mu_{j}$
\end_inset

.
 Then scale by standard deviation.
\end_layout

\begin_layout Itemize
Example: reduce data from 
\begin_inset Formula $n$
\end_inset

 dimensions to 
\begin_inset Formula $k$
\end_inset

 dimensions.
\end_layout

\begin_layout Itemize
Compute covariance matrix: 
\begin_inset Formula $\Sigma=\frac{1}{m}\sum_{i=1}^{n}\left(x^{\left(i\right)}\right)\left(x^{\left(i\right)}\right)^{T}$
\end_inset


\end_layout

\begin_layout Itemize
Compute eigenvectors of matrix 
\begin_inset Formula $\Sigma$
\end_inset

: 
\family typewriter
[U,S,V] = svd(Sigma);
\family default
 (singular value decomposition).
 Can also use 
\family typewriter
eig(Sigma)
\family default
.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\Sigma$
\end_inset

 is 
\begin_inset Formula $n\times n$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $U$
\end_inset

 is 
\begin_inset Formula $n\times n$
\end_inset

; each column is a vector corresponding to a principal component.
\end_layout

\begin_deeper
\begin_layout Itemize
Take the first 
\begin_inset Formula $k$
\end_inset

 columns of 
\begin_inset Formula $U$
\end_inset

.
 Call this 
\begin_inset Formula $U_{reduce}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $z=U_{reduce}^{T}\cdot x$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Reconstruction from compressed representation: given a point 
\begin_inset Formula $z$
\end_inset

 in the compressed space, can we map it back to the full representation
 
\begin_inset Formula $x$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x_{approx}=U_{reduce}\cdot z$
\end_inset


\end_layout

\begin_layout Itemize
Variance retained: 
\begin_inset Formula $\sum_{i=1}^{k}S_{ii}/\sum_{i=1}^{n}S_{ii}\leq1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Choosing the number of principal components to use:
\end_layout

\begin_deeper
\begin_layout Itemize
PCA tries to minimize the average squared projection error: 
\begin_inset Formula $\frac{1}{m}\sum_{i=1}^{m}\left|x^{\left(i\right)}-x_{approx}^{\left(i\right)}\right|^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Total variation in the data: 
\begin_inset Formula $\frac{1}{m}\sum_{i=1}^{m}\left|x^{\left(i\right)}\right|^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Typically, choose 
\begin_inset Formula $k$
\end_inset

 so that the average squared projection error divided by the total variation
 is 
\begin_inset Formula $\leq0.01$
\end_inset

.
 
\begin_inset Quotes eld
\end_inset

99% of variance is retained.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Can use second output of 
\family typewriter
svd()
\family default
: this is a diagonal matrix; the quantity we want to calculate is 
\begin_inset Formula $1-\frac{\sum_{i=1}^{k}S_{ii}}{\sum_{i=1}^{n}S_{ii}}\leq0.01$
\end_inset

 OR 
\begin_inset Formula $\frac{\sum_{i=1}^{k}S_{ii}}{\sum_{i=1}^{n}S_{ii}}\geq0.99$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Advice for applying PCA to speed up supervised learning:
\end_layout

\begin_deeper
\begin_layout Itemize
Example: 100 by 100 images.
\end_layout

\begin_layout Itemize
Extract inputs: unlabeled dataset 
\begin_inset Formula $x^{\left(1\right)},...,x^{\left(m\right)}\in R^{10000}$
\end_inset


\end_layout

\begin_layout Itemize
Use PCA to get 
\begin_inset Formula $z^{\left(1\right)},...,z^{\left(m\right)}\in R^{1000}$
\end_inset

.
\end_layout

\begin_layout Itemize
Pair 
\begin_inset Formula $z$
\end_inset

s with 
\begin_inset Formula $y$
\end_inset

s.
\end_layout

\begin_layout Itemize
Note: the mapping from 
\begin_inset Formula $x$
\end_inset

 to 
\begin_inset Formula $z$
\end_inset

 should be defined by running PCA on the training set.
 Then you can use the same mapping for the cross-validation and test sets.
\end_layout

\end_deeper
\begin_layout Itemize
Bad use of PCA: to prevent overfitting.
\end_layout

\begin_deeper
\begin_layout Itemize
The idea: if we use PCA to go to 
\begin_inset Formula $k<n$
\end_inset

 features, it should be less likely to overfit.
\end_layout

\begin_layout Itemize
Sometimes, this might work OK, but regularization is a much better way to
 address overfitting.
\end_layout

\end_deeper
\begin_layout Itemize
PCA is sometimes used where it shouldn't be.
\end_layout

\begin_deeper
\begin_layout Itemize
Example: design of ML system.
\end_layout

\begin_deeper
\begin_layout Itemize
Get training set.
\end_layout

\begin_layout Itemize
Run PCA to reduce training set in dimension.
\end_layout

\begin_layout Itemize
Train logistic regression.
\end_layout

\begin_layout Itemize
Test on test set: map test set to lower dimension, use hypothesis function
 to test algorithm.
\end_layout

\end_deeper
\begin_layout Itemize
But why not just try it without PCA?
\end_layout

\begin_layout Itemize
Should first try it without PCA; if it doesn't do what you want, then implement
 PCA and see what the results are.
\end_layout

\end_deeper
\begin_layout Section*
Anomaly detection
\end_layout

\begin_layout Subsection*
Density estimation
\end_layout

\begin_layout Itemize
What is anomaly detection?
\end_layout

\begin_deeper
\begin_layout Itemize
Trying to decide if a new example fits in with the other examples.
\end_layout

\begin_layout Itemize
Is the new example 
\begin_inset Formula $x_{test}$
\end_inset

 anomalous?
\end_layout

\begin_layout Itemize
Can define a model 
\begin_inset Formula $p\left(x\right)$
\end_inset

 which gives the probability that an example is OK.
\end_layout

\begin_deeper
\begin_layout Itemize
We also define a threshold 
\begin_inset Formula $\epsilon$
\end_inset

, where if 
\begin_inset Formula $p\left(x\right)\geq\epsilon$
\end_inset

, we mark the example as OK.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Uses:
\end_layout

\begin_deeper
\begin_layout Itemize
Fraud detection:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: features of user 
\begin_inset Formula $i$
\end_inset

's activity.
\end_layout

\begin_layout Itemize
Model 
\begin_inset Formula $p\left(x\right)$
\end_inset

 from data.
\end_layout

\begin_layout Itemize
Identify unusual users by checking which have 
\begin_inset Formula $p\left(x\right)<\epsilon$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Manufacturing example: monitoring computers in a data center.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: features of machine 
\begin_inset Formula $i$
\end_inset

.
 Examples: memory use, number of disk accesses, CPU load, network traffic,
 etc.
\end_layout

\begin_layout Itemize
Model 
\begin_inset Formula $p\left(x\right)$
\end_inset

 from data and identify unusual machines.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Gaussian distribution: 
\begin_inset Formula $p\left(x;\mu,\sigma^{2}\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Can estimate for a data set:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\mu=\frac{1}{m}\sum_{i=1}^{m}x^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\sigma^{2}=\frac{1}{m-1}\sum_{i=1}^{m}\left(x^{\left(i\right)}-\mu\right)^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In machine learning, people tend to use 
\begin_inset Formula $1/m$
\end_inset

 instead of 
\begin_inset Formula $1/\left(m-1\right)$
\end_inset

 (not sure why....).
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
Developing an anomaly detection algorithm
\end_layout

\begin_deeper
\begin_layout Itemize
Training set with 
\begin_inset Formula $m$
\end_inset

 examples: 
\begin_inset Formula $\left\{ x^{\left(1\right)},...,x^{\left(m\right)}\right\} $
\end_inset

, where each example is 
\begin_inset Formula $x\in R^{n}$
\end_inset

.
\end_layout

\begin_layout Itemize
Assume each feature 
\begin_inset Formula $x_{i}$
\end_inset

 is Gaussian distributed with mean 
\begin_inset Formula $\mu_{i}$
\end_inset

 and variance 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 - this corresponds to assuming the features are all independent.
\end_layout

\begin_deeper
\begin_layout Itemize
Should we do PCA to make this assumption more robust?
\end_layout

\end_deeper
\begin_layout Itemize
Model 
\begin_inset Formula $p\left(x\right)=p\left(x_{1};\mu_{1},\sigma_{1}^{2}\right)p\left(x_{2};\mu_{2},\sigma_{2}^{2}\right)...p\left(x_{n};\mu_{n},\sigma_{n}^{2}\right)=\prod_{j=1}^{n}p\left(x_{j};\mu_{j},\sigma_{j}^{2}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Choose features that you think might be indicative of anomalous examples.
\end_layout

\begin_layout Itemize
Fit parameters 
\begin_inset Formula $\mu_{j}$
\end_inset

 and 
\begin_inset Formula $\sigma_{j}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Given a new example, compute 
\begin_inset Formula $p\left(x\right).$
\end_inset


\end_layout

\begin_layout Itemize
Mark as anomalous if 
\begin_inset Formula $p\left(x\right)<\epsilon$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Reasonable choice for 
\begin_inset Formula $\epsilon$
\end_inset

: something like 0.02?
\end_layout

\begin_layout Subsection*
Building an anomaly detection system
\end_layout

\begin_layout Itemize
When developing a learning algorithm, making decisions is much easier if
 we have a way of evaluating the algorithm.
\end_layout

\begin_deeper
\begin_layout Itemize
Assume we have some labeled data with anomalous and non-anomalous examples.
\end_layout

\begin_layout Itemize
Training set: large number of examples, no anomalous cases.
\end_layout

\begin_layout Itemize
Cross-validation and test sets should include anomalous examples.
\end_layout

\end_deeper
\begin_layout Itemize
Algorithm evaluation
\end_layout

\begin_deeper
\begin_layout Itemize
Fit model 
\begin_inset Formula $p\left(x\right)$
\end_inset

 on training set.
\end_layout

\begin_layout Itemize
Make predictions on a test set.
\end_layout

\begin_layout Itemize
Possible evaluation metrics:
\end_layout

\begin_deeper
\begin_layout Itemize
True positive, false positive, false negative, true negative.
\end_layout

\begin_layout Itemize
Precision/recall.
\end_layout

\begin_layout Itemize
\begin_inset Formula $F_{1}$
\end_inset

 score.
\end_layout

\begin_layout Itemize
Classification accuracy is not good because there are a small number of
 anomalies; an algorithm that always predicts 
\begin_inset Formula $y=0$
\end_inset

 will do well.
\end_layout

\end_deeper
\begin_layout Itemize
Can also use cross-validation set to choose parameter 
\begin_inset Formula $\epsilon$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Anomaly detection vs.
 supervised learning
\end_layout

\begin_deeper
\begin_layout Itemize
Anomaly detection
\end_layout

\begin_deeper
\begin_layout Itemize
Very small number of positive examples (
\begin_inset Formula $y=1$
\end_inset

).
 0-20 positive examples is common.
\end_layout

\begin_layout Itemize
Large number of negative (
\begin_inset Formula $y=0$
\end_inset

) examples.
\end_layout

\begin_layout Itemize
Many different types of anomalies, hard for any algorithm to learn from
 positive examples what the anomalies might look like; future anomalies
 may look very different from examples seen so far.
\end_layout

\begin_layout Itemize
Examples: fraud detection, manufacturing, monitoring machines in a data
 center.
\end_layout

\end_deeper
\begin_layout Itemize
Supervised learning
\end_layout

\begin_deeper
\begin_layout Itemize
Large number of positive and negative examples.
\end_layout

\begin_layout Itemize
Enough positive examples for the algorithm to get a sense of what positive
 examples are like; future positive examples are likely to be similar to
 ones in the training set.
\end_layout

\begin_layout Itemize
Examples: e-mail spam classification, weather prediction, cancer classification.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Choosing features
\end_layout

\begin_deeper
\begin_layout Itemize
First, plot histogram of the features, check for significant non-Gaussianity.
 Usually OK if it's somewhat close to Gaussian.
\end_layout

\begin_deeper
\begin_layout Itemize
Can also test if the log of a feature is Gaussian; if so, replace that feature
 with its log.
\end_layout

\begin_layout Itemize
Other transforms: 
\begin_inset Formula $x\rightarrow x^{\alpha}$
\end_inset

, usually 
\begin_inset Formula $\alpha<1$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Want 
\begin_inset Formula $p\left(x\right)$
\end_inset

 to be large for normal examples and small for anomalous examples.
\end_layout

\begin_deeper
\begin_layout Itemize
Common problem: 
\begin_inset Formula $p\left(x\right)$
\end_inset

 is comparable (say, both large) for normal and anomalous examples.
\end_layout

\end_deeper
\begin_layout Subsection*
Multivariate Gaussian distribution
\end_layout

\begin_layout Itemize
In some cases, we need to account for correlations between variables - our
 assumption of independence may not be valid.
\end_layout

\begin_layout Itemize
Then, we can't model 
\begin_inset Formula $p\left(x\right)$
\end_inset

 assuming the variables are independent.
 We need to do it all in one go.
\end_layout

\begin_deeper
\begin_layout Itemize
Parameters: 
\begin_inset Formula $\mu\in R^{n}$
\end_inset

, 
\begin_inset Formula $\Sigma\in R^{n\times n}$
\end_inset

 (covariance matrix).
\end_layout

\begin_layout Itemize
\begin_inset Formula $p\left(x;\mu,\Sigma\right)=\frac{1}{\left(2\pi\right)^{n/2}\det\left(\Sigma\right)}\exp\left(-\frac{1}{2}\left(x-\mu\right)^{T}\Sigma^{-1}\left(x-\mu\right)\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Anomaly detection with the multivariate Gaussian
\end_layout

\begin_deeper
\begin_layout Itemize
First, fit model 
\begin_inset Formula $p\left(x\right)$
\end_inset

by setting:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\mu=\frac{1}{m}\sum_{i=1}^{m}x^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\Sigma=\frac{1}{m}\sum_{i=1}^{m}\left(x^{\left(i\right)}-\mu\right)\left(x^{\left(i\right)}-\mu\right)^{T}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Given a new example 
\begin_inset Formula $x$
\end_inset

, compute 
\begin_inset Formula $p\left(x\right)$
\end_inset

 and flag as anomalous if 
\begin_inset Formula $p\left(x\right)<\epsilon$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Our original model (assuming independent features) corresponds to a multivariate
 Gaussian where the contours of the multivariate PDF are 
\begin_inset Quotes eld
\end_inset

axis-aligned.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is equivalent to saying that the covariance matrix is diagonal.
\end_layout

\end_deeper
\begin_layout Itemize
When should we use the independent model and when should we use the multivariate
 model?
\end_layout

\begin_deeper
\begin_layout Itemize
Independent:
\end_layout

\begin_deeper
\begin_layout Itemize
Manually create features to capture anomalies where 
\begin_inset Formula $x_{1},x_{2}$
\end_inset

 take unusual combinations of values.
\end_layout

\begin_layout Itemize
Computationally cheaper, scales better to large 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Itemize
Works OK even if 
\begin_inset Formula $m$
\end_inset

 (training set size) is small.
\end_layout

\end_deeper
\begin_layout Itemize
Multivariate:
\end_layout

\begin_deeper
\begin_layout Itemize
Automatically captures correlations between features.
\end_layout

\begin_layout Itemize
Computationally more expensive because we need to invert the 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $\Sigma$
\end_inset

.
\end_layout

\begin_layout Itemize
Must have 
\begin_inset Formula $m>n$
\end_inset

, or else 
\begin_inset Formula $\Sigma$
\end_inset

 is not invertible.
 (preferable to have 
\begin_inset Formula $m\gg n$
\end_inset

)
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
If you fit a multivariate model and find that 
\begin_inset Formula $\Sigma$
\end_inset

 is singular or non-invertible:
\end_layout

\begin_deeper
\begin_layout Itemize
Check if 
\begin_inset Formula $m>n$
\end_inset

.
\end_layout

\begin_layout Itemize
You may have highly redundant (linearly dependent) features (
\begin_inset Formula $x_{3}=Cx_{4}$
\end_inset

, 
\begin_inset Formula $x_{7}=x_{8}+x_{9}$
\end_inset

, etc.).
\end_layout

\end_deeper
\begin_layout Section*
Recommender systems
\end_layout

\begin_layout Itemize
When you buy a product online, most websites automatically recommend other
 products that you may like.
 Recommender systems look at patterns of activity between different users
 and different products to produce these recommendations.
\end_layout

\begin_layout Subsection*
Predicting movie ratings
\end_layout

\begin_layout Itemize
User rates movies using 0 to 5 stars.
\end_layout

\begin_layout Itemize
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="5">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Movie
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Alice (1)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Bob (2)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Carol (3)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Dave (4)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Love at last
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Romance forever
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cute puppies of love
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
?
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nonstop car chases
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Swords vs.
 karate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
?
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Itemize
Notation:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n_{u}$
\end_inset

: number of users (4 in this example).
\end_layout

\begin_layout Itemize
\begin_inset Formula $n_{m}$
\end_inset

: number of movies (5 in this example).
\end_layout

\begin_layout Itemize
\begin_inset Formula $r\left(i,j\right)$
\end_inset

: 1 if user 
\begin_inset Formula $j$
\end_inset

 has rated movie 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $y^{\left(i,j\right)}$
\end_inset

: rating given by user 
\begin_inset Formula $j$
\end_inset

 to movie 
\begin_inset Formula $i$
\end_inset

 (defined only if 
\begin_inset Formula $r\left(i,j\right)=1$
\end_inset

).
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

: parameter vector for user 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: feature vector for movie 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $m^{\left(j\right)}$
\end_inset

: number of movies rated by user 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Goal: based on a user's rating for some movies, predict how they would rate
 other movies.
\end_layout

\begin_layout Itemize
What if we had features for each movies?
\end_layout

\begin_deeper
\begin_layout Itemize
In this example, we would want features which measure the degree of romance
 and the degree of action in each movie.
\end_layout

\begin_layout Itemize
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Movie
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{1}$
\end_inset

 (romance)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{2}$
\end_inset

 (action)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Love at last
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Romance forever
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cute puppies of love
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.99
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nonstop car chases
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Swords vs.
 karate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.9
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
For each user 
\begin_inset Formula $j$
\end_inset

, learn a parameter 
\begin_inset Formula $\theta^{\left(j\right)}\in R^{3}$
\end_inset

.
 Predict user 
\begin_inset Formula $j$
\end_inset

 as rating movie 
\begin_inset Formula $i$
\end_inset

 with 
\begin_inset Formula $\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Problem formulation:
\end_layout

\begin_deeper
\begin_layout Itemize
For user 
\begin_inset Formula $j$
\end_inset

 and movie 
\begin_inset Formula $i$
\end_inset

, predicted rating: 
\begin_inset Formula $\left(\theta^{\left(j\right)}\right)^{T}\left(x^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
To learn 
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

 (parameter for user 
\begin_inset Formula $j$
\end_inset

): 
\begin_inset Formula $\text{min}_{\theta^{\left(j\right)}}\frac{1}{2m^{\left(j\right)}}\sum_{i:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}\left(x^{\left(i\right)}\right)-y^{\left(i,j\right)}\right)^{2}+\frac{\lambda}{2m^{\left(j\right)}}\sum_{k=1}^{n}\left(\theta_{k}^{\left(j\right)}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize
To learn 
\begin_inset Formula $\theta$
\end_inset

 for all users: 
\begin_inset Formula $J\left(\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}\right)\text{min}_{\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}}\frac{1}{2}\sum_{j=1}^{n_{u}}\sum_{i:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}\left(x^{\left(i\right)}\right)-y^{\left(i,j\right)}\right)^{2}+\frac{\lambda}{2}\sum_{j=1}^{n_{u}}\sum_{k=1}^{n}\left(\theta_{k}^{\left(j\right)}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Gradient descent update:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{k}^{\left(j\right)}:=\theta_{k}^{\left(j\right)}-\alpha\sum_{i:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}-y^{\left(i,j\right)}\right)x_{k}^{\left(i\right)}$
\end_inset

 for 
\begin_inset Formula $k=0$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{k}^{\left(j\right)}:=\theta_{k}^{\left(j\right)}-\alpha\left(\sum_{i:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}-y^{\left(i,j\right)}\right)x_{k}^{\left(i\right)}+\lambda\theta_{k}^{\left(j\right)}\right)$
\end_inset

 for 
\begin_inset Formula $k\neq0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The term multiplying 
\begin_inset Formula $\alpha$
\end_inset

 is equal to 
\begin_inset Formula $\frac{\partial}{\partial\theta_{k}^{\left(j\right)}}J\left(\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Subsection*
Collaborative filtering
\end_layout

\begin_layout Itemize
Sometimes, it might be hard to get features.
 Example: previous study with movie ratings - lots of work to get people
 to watch movies and rate the amount of romance and action.
 What if we let the people rating the movies define their own 
\begin_inset Formula $\theta$
\end_inset

 which describes how much they like each type of movie?
\end_layout

\begin_deeper
\begin_layout Itemize
I.e., someone who likes romantic movies and hates action movies would have
 a parameter vector 
\begin_inset Formula $\theta^{\left(i\right)}=\left[0,5,0\right]$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Optimization algorithm to learn the features:
\end_layout

\begin_deeper
\begin_layout Itemize
Given 
\begin_inset Formula $\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}$
\end_inset

, to learn 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

: 
\begin_inset Formula $\text{min}_{x^{\left(i\right)}}\frac{1}{2}\sum_{j:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}-y^{\left(i,j\right)}\right)^{2}+\frac{\lambda}{2}\sum_{k=1}^{n}\left(x_{k}^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}$
\end_inset

, to learn 
\begin_inset Formula $x^{\left(1\right)},...,x^{\left(n_{m}\right)}$
\end_inset

: 
\begin_inset Formula $\text{min}_{x^{\left(1\right)},...,x^{\left(n_{m}\right)}}\frac{1}{2}\sum_{i=1}^{n_{m}}\sum_{j:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}-y^{\left(i,j\right)}\right)^{2}+\frac{\lambda}{2}\sum_{i=1}^{n_{m}}\sum_{k=1}^{n}\left(x_{k}^{\left(i\right)}\right)^{2}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Also, if we know the features 
\begin_inset Formula $x^{\left(1\right)},...,x^{\left(n_{m}\right)}$
\end_inset

, can estimate the parameter 
\begin_inset Formula $\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}$
\end_inset

 (this is what we did before).
\end_layout

\begin_layout Itemize
Can also iterate: 
\begin_inset Formula $\theta\rightarrow x\rightarrow\theta\rightarrow x$
\end_inset

, and so on.
 Will allow you to estimate parameters better.
\end_layout

\begin_layout Itemize
Full algorithm description:
\end_layout

\begin_deeper
\begin_layout Itemize
Objective: given 
\begin_inset Formula $x^{\left(1\right)},...x^{\left(n_{m}\right)}$
\end_inset

, estimate 
\begin_inset Formula $\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}$
\end_inset

 (or vice versa).
\end_layout

\begin_layout Itemize
Actually, we can do both at the same time!
\end_layout

\begin_layout Itemize
Minimizing 
\begin_inset Formula $x^{\left(1\right)},...x^{\left(n_{m}\right)}$
\end_inset

 and 
\begin_inset Formula $\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}$
\end_inset

 simultaneously:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J\left(x^{\left(1\right)},...,x^{\left(n_{m}\right)},\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}\right)=\frac{1}{2}\sum_{\left(i,j\right):r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}-y^{\left(i,j\right)}\right)^{2}+\frac{\lambda}{2}\sum_{i=1}^{n_{m}}\sum_{k=1}^{n}\left(x_{k}^{\left(i\right)}\right)^{2}+\frac{\lambda}{2}\sum_{j=1}^{n_{u}}\sum_{k=1}^{n}\left(\theta_{k}^{\left(j\right)}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Here we minimize 
\begin_inset Formula $J$
\end_inset

 over all features 
\begin_inset Formula $x^{\left(1\right)},...,x^{\left(n_{m}\right)}$
\end_inset

 and parameters 
\begin_inset Formula $\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Note that we don't minimize over 
\begin_inset Formula $x_{0}$
\end_inset

 because we are now learning all of the features
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Initialize 
\begin_inset Formula $x^{\left(1\right)},...,x^{\left(n_{m}\right)},\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}$
\end_inset

 to small, random values.
\end_layout

\begin_layout Itemize
Minimize 
\begin_inset Formula $J\left(x^{\left(1\right)},...,x^{\left(n_{m}\right)},\theta^{\left(1\right)},...,\theta^{\left(n_{u}\right)}\right)$
\end_inset

 using gradient descent or an advanced optimization algorithm.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x_{k}^{\left(i\right)}:=x_{k}^{\left(i\right)}-\alpha\left(\sum_{j:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}-y^{\left(i,j\right)}\right)\theta_{k}^{\left(j\right)}+\lambda x_{k}^{\left(i\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{k}^{\left(j\right)}:=\theta_{k}^{\left(j\right)}-\alpha\left(\sum_{i:r\left(i,j\right)=1}\left(\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}-y^{\left(i,j\right)}\right)x_{k}^{\left(i\right)}+\lambda\theta_{k}^{\left(j\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Term in parentheses is equal to 
\begin_inset Formula $\frac{\partial}{\partial x_{k}^{\left(i\right)}}J\left(...\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
For a user with parameters 
\begin_inset Formula $\theta$
\end_inset

 and a movie with learned features 
\begin_inset Formula $x$
\end_inset

, predict a star rating of 
\begin_inset Formula $\theta^{T}x$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection*
Low rank matrix factorization
\end_layout

\begin_layout Itemize
Put all ratings from all users in a matrix.
\end_layout

\begin_layout Itemize
Using movies example: 
\begin_inset Formula $n_{u}=5$
\end_inset

, 
\begin_inset Formula $n_{m}=4$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y=\left[\begin{array}{cccc}
5 & 5 & 0 & 0\\
5 & ? & ? & 0\\
? & 4 & 0 & ?\\
0 & 0 & 5 & 4\\
0 & 0 & 5 & 0
\end{array}\right]$
\end_inset


\end_layout

\begin_layout Itemize
Predicted ratings: 
\begin_inset Formula $Y_{ij}=\left(\theta^{\left(j\right)}\right)^{T}x^{\left(i\right)}$
\end_inset

.
 We can calculate this element-by-element, or we can vectorize it!
\end_layout

\begin_layout Itemize
Define a vector of feature vectors 
\begin_inset Formula $X=\left[\begin{array}{c}
\left(x^{\left(1\right)}\right)^{T}\\
\left(x^{\left(2\right)}\right)^{T}\\
.\\
.\\
\left(x^{\left(n_{m}\right)}\right)^{T}
\end{array}\right]$
\end_inset

 and a vector of parameter vectors 
\begin_inset Formula $\Theta=\left[\begin{array}{c}
\left(\Theta^{\left(1\right)}\right)^{T}\\
\left(\Theta^{\left(2\right)}\right)^{T}\\
.\\
.\\
\left(\Theta^{\left(n_{u}\right)}\right)^{T}
\end{array}\right]$
\end_inset

; then we can calculate 
\begin_inset Formula $Y=X\Theta^{T}$
\end_inset

.
\end_layout

\begin_layout Itemize
This is called low rank matrix factorization.
\end_layout

\end_deeper
\begin_layout Itemize
Can use learned features to find related movies.
\end_layout

\begin_deeper
\begin_layout Itemize
For each product 
\begin_inset Formula $i$
\end_inset

, we learn a feature vector 
\begin_inset Formula $x^{\left(i\right)}\in R^{n}$
\end_inset

.
\end_layout

\begin_layout Itemize
How to find movies 
\begin_inset Formula $j$
\end_inset

 related to movie 
\begin_inset Formula $i$
\end_inset

? (can be used to make recommendations)
\end_layout

\begin_deeper
\begin_layout Itemize
Find a movie 
\begin_inset Formula $j$
\end_inset

 such that the distance between the two feature vectors is small.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left|x^{\left(i\right)}-x^{\left(j\right)}\right|=\text{small}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
What if we have a user who hasn't rated any movies?
\end_layout

\begin_deeper
\begin_layout Itemize
First term in cost function plays no role since 
\begin_inset Formula $r\left(i,j\right)$
\end_inset

 is 0 for all movies.
\end_layout

\begin_layout Itemize
The only term that affects 
\begin_inset Formula $\theta$
\end_inset

 for the new user is the regularization term, so we'll get a parameter vector
 of all zeros.
\end_layout

\begin_layout Itemize
Thus, we'll predict ratings of 0 for all movies for the new user - this
 doesn't seem very useful!
\end_layout

\begin_layout Itemize
To fix this, we use mean normalization:
\end_layout

\begin_deeper
\begin_layout Itemize
Subtract the mean rating for each movie so that each movie has an average
 rating of zero.
\end_layout

\begin_layout Itemize
Adjust your 
\begin_inset Formula $Y$
\end_inset

 matrix to account for this and use this matrix to do collaborative filtering.
\end_layout

\begin_layout Itemize
For user 
\begin_inset Formula $j$
\end_inset

 on movie 
\begin_inset Formula $i$
\end_inset

, predict 
\begin_inset Formula $\left(\theta^{\left(j\right)}\right)^{T}\left(x^{\left(i\right)}\right)+\mu_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
This results in the predicted ratings for the new user being the mean ratings
 for each movie.
\end_layout

\end_deeper
\end_deeper
\begin_layout Section*
Large scale machine learning
\end_layout

\begin_layout Subsection*
Gradient descent with large datasets
\end_layout

\begin_layout Itemize
Example: a training set size: 
\begin_inset Formula $m=1e8$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
A single step of gradient descent requires a sum over all of these examples!
\end_layout

\begin_layout Itemize
Question: why not just test on a subset 
\begin_inset Formula $m=1000$
\end_inset

 before investing the effort into training on the full dataset?
\end_layout

\begin_layout Itemize
Can test if more data is useful by plotting 
\begin_inset Formula $J_{CV}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

 vs.
 
\begin_inset Formula $m$
\end_inset

 and see how they compare as 
\begin_inset Formula $m$
\end_inset

 increases.
\end_layout

\end_deeper
\begin_layout Itemize
Linear regression with gradient descent:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\sum_{j=0}^{n}\theta_{j}x_{j}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $J_{train}\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Gradient descent: repeat 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 for every 
\begin_inset Formula $j=0,...,n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $m$
\end_inset

 is large, this step takes a long time!
\end_layout

\begin_layout Itemize
This is also called 
\emph on
batch
\emph default
 gradient descent.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Stochastic gradient descent:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\text{cost}\left(\theta,\left(x^{\left(i\right)},y^{\left(i\right)}\right)\right)=\frac{1}{2}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Measures how well the cost function does on a single example.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $J_{train}\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\text{cost}\left(\theta,\left(x^{\left(i\right)},y^{\left(i\right)}\right)\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is the average of the cost function over all training examples.
\end_layout

\end_deeper
\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Randomly shuffle the dataset.
\end_layout

\begin_layout Itemize
Repeat:
\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $i=1,...,m$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
(
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

, for 
\begin_inset Formula $j=0,...,n$
\end_inset

)
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
How many times to repeat this whole thing? Usually 1 to 10 passes is most
 common, but it depends on the size of your training set.
\end_layout

\begin_layout Itemize
Note: the term multiplying 
\begin_inset Formula $\alpha$
\end_inset

 is equal to 
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}\text{cost}\left(\theta\left(x^{\left(i\right)},y^{\left(i\right)}\right)\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Main difference between this and batch gradient descent is that we take
 a small step after looking at each example rather than having to sum over
 all examples before taking a step.
\end_layout

\begin_layout Itemize
However, we end up taking a less 
\begin_inset Quotes eld
\end_inset

direct
\begin_inset Quotes erd
\end_inset

 route to the minimum of the cost function.
\end_layout

\end_deeper
\begin_layout Itemize
Mini-batch gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
In between batch and stochastic gradient descent.
\end_layout

\begin_layout Itemize
We use 
\begin_inset Formula $b$
\end_inset

 examples in each iteration, where 
\begin_inset Formula $b$
\end_inset

 is the mini-batch size.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $b$
\end_inset

 is typically chosen to be something like 10, may be between 2 and 100.
\end_layout

\end_deeper
\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Get 
\begin_inset Formula $b$
\end_inset

 randomly chosen examples from the training set.
\end_layout

\begin_layout Itemize
Run batch gradient descent once on these examples.
\end_layout

\begin_layout Itemize
Repeat for each
\end_layout

\end_deeper
\begin_layout Itemize
Written out for 
\begin_inset Formula $b=10$
\end_inset

, 
\begin_inset Formula $m=1000$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Repeat:
\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $i=1,11,21,31,...,991$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{10}\sum_{k=1}^{i+9}\left(h_{\theta}\left(x^{\left(k\right)}\right)-y^{\left(k\right)}\right)x_{j}^{\left(k\right)}$
\end_inset


\end_layout

\begin_layout Itemize
(for every 
\begin_inset Formula $j=0,...,n$
\end_inset

)
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Usually want to use 1-10 passes.
\end_layout

\end_deeper
\begin_layout Itemize
Why is it useful?
\end_layout

\begin_deeper
\begin_layout Itemize
Only outperforms stochastic gradient descent if we have a good vectorized
 implementation for handling the 
\begin_inset Formula $b$
\end_inset

 examples on each iteration.
\end_layout

\end_deeper
\begin_layout Itemize
Disadvantage: new parameter 
\begin_inset Formula $b$
\end_inset

 which you have to study.
\end_layout

\end_deeper
\begin_layout Itemize
Checking for convergence:
\end_layout

\begin_deeper
\begin_layout Itemize
Batch gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Plot 
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

 as a function of the number of iterations of gradient descent.
\end_layout

\begin_layout Itemize
This should decrease as the number of iterations increases.
\end_layout

\end_deeper
\begin_layout Itemize
Stochastic gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
During learning, compute 
\begin_inset Formula $\text{cost}\left(\theta,\left(x^{\left(i\right)},y^{\left(i\right)}\right)\right)$
\end_inset

 before updating 
\begin_inset Formula $\theta$
\end_inset

 using 
\begin_inset Formula $\left(x^{\left(i\right)},y^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Every 1000 iterations or so, average the cost function over the last 1000
 examples and plot it.
\end_layout

\begin_layout Itemize
If the algorithm appears to be diverging, use a smaller 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Itemize
If the cost function seems to be flat vs.
 the number of iterations, you may need to change something - learning rate,
 features, etc.
\end_layout

\begin_layout Itemize
Can slowly decrease 
\begin_inset Formula $\alpha$
\end_inset

 over time if we want 
\begin_inset Formula $\theta$
\end_inset

 to converge.
\end_layout

\begin_deeper
\begin_layout Itemize
Ex: 
\begin_inset Formula $\alpha=\frac{const1}{iterationNumber+const2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Subsection*
Online learning
\end_layout

\begin_layout Itemize
Online learning algorithm: problems where we have a continuous stream of
 data coming in and we want an algorithm to learn from that.
\end_layout

\begin_layout Itemize
Example: shipping service website where user comes, specifies origin and
 destination, you offer to ship their package for some asking price, and
 users sometimes choose to use your shipping service (
\begin_inset Formula $y=1$
\end_inset

) and sometimes not (
\begin_inset Formula $y=0$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
Features 
\begin_inset Formula $x$
\end_inset

 capture properties of the user, the origin and destination, and the asking
 price.
\end_layout

\begin_layout Itemize
We want to learn 
\begin_inset Formula $p\left(y=1|x;\theta\right)$
\end_inset

 to optimize the price.
\end_layout

\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Repeat forever:
\end_layout

\begin_deeper
\begin_layout Itemize
Get 
\begin_inset Formula $\left(x,y\right)$
\end_inset

 corresponding to user.
\end_layout

\begin_layout Itemize
Update parameters 
\begin_inset Formula $\theta$
\end_inset

 using just this example 
\begin_inset Formula $\left(x,y\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\left(h_{\theta}\left(x\right)-y\right)x_{j}$
\end_inset

 (for 
\begin_inset Formula $j=0,...,n$
\end_inset

)
\end_layout

\begin_layout Itemize
(this is for logistic regression)
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
After using each data point, we throw it out and never use it again.
\end_layout

\begin_layout Itemize
This type of algorithm can adapt to changing user preferences.
\end_layout

\end_deeper
\begin_layout Itemize
Another application - product search.
\end_layout

\begin_deeper
\begin_layout Itemize
User searches for 
\begin_inset Quotes eld
\end_inset

Android phone 1080p camera
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Itemize
Have 100 phones in store, will return 10 results.
\end_layout

\begin_layout Itemize
Want to have a learning algorithm figure out which 10 phones we should show
 to the user based on their search query.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x$
\end_inset

: features of phone, how many words in user query match name of phone, how
 many words in user search query match phone description, etc.
\end_layout

\begin_layout Itemize
\begin_inset Formula $y=1$
\end_inset

 if user clicks on link, 
\begin_inset Formula $y=0$
\end_inset

 otherwise.
\end_layout

\begin_layout Itemize
Learn 
\begin_inset Formula $p\left(y=1|x;\theta\right)$
\end_inset

 and use this to show the user the 10 phones that they are most likely to
 click on.
\end_layout

\end_deeper
\begin_layout Itemize
Other examples: choosing special offers to show the user, customized selection
 of news articles, product recommendation, etc.
\end_layout

\begin_layout Subsection*
MapReduce and data parallelism
\end_layout

\begin_layout Itemize
Example with 
\begin_inset Formula $m=400$
\end_inset

 data points:
\end_layout

\begin_deeper
\begin_layout Itemize
Batch gradient descent: 
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{400}\sum_{i=1}^{400}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Itemize
Machine 1: Use 
\begin_inset Formula $\left(x^{\left(1\right)},y^{\left(1\right)}\right),...,\left(x^{\left(100\right)},y^{\left(100\right)}\right)$
\end_inset

 and compute the summation for the first 100 examples.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $temp_{j}^{\left(1\right)}=\sum_{i=1}^{100}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Machine 2: same thing with examples 101 to 200.
\end_layout

\begin_layout Itemize
Machine 3: same thing with examples 201 to 300.
\end_layout

\begin_layout Itemize
Machine 4: same thing with examples 301 to 400.
\end_layout

\begin_layout Itemize
After all of these computation are done, we send them to a master server,
 which combines the results and updates 
\begin_inset Formula $\theta_{j}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{1}{400}\left(temp_{j}^{\left(1\right)}+temp_{j}^{\left(2\right)}+temp_{j}^{\left(3\right)}+temp_{j}^{\left(4\right)}\right)$
\end_inset

 (for 
\begin_inset Formula $j=0,...,n$
\end_inset

).
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Many learning algorithms can be expressed as computing sums of functions
 over the training set.
\end_layout

\begin_layout Itemize
Example: advanced optimization with logistic regression.
\end_layout

\begin_deeper
\begin_layout Itemize
We need:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J_{train}\left(\theta\right)=-1\frac{1}{m}\sum_{i=1}^{m}y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)-\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J_{train}\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
You can use MapReduce on computers with multiple cores as well!
\end_layout

\begin_deeper
\begin_layout Itemize
No network latency for communication between computers.
\end_layout

\end_deeper
\begin_layout Section*
Photo optical character recognition (OCR)
\end_layout

\begin_layout Itemize
Recognizing and reading text in digital images.
\end_layout

\begin_layout Itemize
Very useful to modularize the full procedure!
\end_layout

\begin_layout Itemize
Photo OCR pipeline overview:
\end_layout

\begin_deeper
\begin_layout Itemize
Text detection: define a rectangle around the text.
\end_layout

\begin_layout Itemize
Character segmentation: separate each character.
\end_layout

\begin_layout Itemize
Character classification: classify each character as a letter.
\end_layout

\begin_layout Itemize
May need a 
\begin_inset Quotes eld
\end_inset

correction
\begin_inset Quotes erd
\end_inset

 step.
 Example: 
\begin_inset Quotes eld
\end_inset

c1eaning
\begin_inset Quotes erd
\end_inset

 should be corrected to 
\begin_inset Quotes eld
\end_inset

cleaning
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Sliding window: define a window size (82 by 50 pixels, for example) and
 slide the window around in your image, performing logistic regression to
 determine if what you are looking for (pedestrians, text, etc.) is contained
 in each window.
\end_layout

\begin_deeper
\begin_layout Itemize
Use some step size of something like 5-10 pixels to slide the window after
 each iteration.
\end_layout

\begin_layout Itemize
Can use larger window size to select parts of the image, then resize those
 to 82 by 50 pixels before passing to your logistic regression classifier.
\end_layout

\end_deeper
\begin_layout Itemize
Text detection:
\end_layout

\begin_deeper
\begin_layout Itemize
Want to define rectangles which contain text.
 It's tricky because different text blocks may have different heights, widths,
 and aspect ratios.
\end_layout

\begin_layout Itemize
Get training examples of positive and negative cases.
\end_layout

\begin_layout Itemize
Run a small window on the image (of the size that will detect single letters).
\end_layout

\begin_layout Itemize
Then apply an expansion classifier: if a pixel is within X pixels of a pixel
 that is expected to have text, color that pixel white as well.
\end_layout

\begin_deeper
\begin_layout Itemize
This helps us to define our bounding rectangles.
\end_layout

\begin_layout Itemize
We define bounding rectangles around groups of white pixels which have reasonabl
e aspect ratios.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
1D sliding window for character segmentation
\end_layout

\begin_deeper
\begin_layout Itemize
Positive examples should be focused on spaces between two characters.
\end_layout

\begin_layout Itemize
Negative examples should have blanks and single letters.
\end_layout

\begin_layout Itemize
We slide a window in one dimension through the text rectangle and let the
 learning algorithm determine where there are gaps.
\end_layout

\end_deeper
\begin_layout Section*
Artificial data synthesis
\end_layout

\begin_layout Itemize
Using a small dataset, doing random operations to generate new data.
\end_layout

\begin_layout Itemize
Can take data you already have and distort it.
 Example: take a character and apply distortions or blurs to make the algorithm
 more robust.
\end_layout

\begin_layout Itemize
Can also generate completely new data yourself.
\end_layout

\begin_layout Itemize
Photo OCR: take random fonts and paste a few characters on a random background,
 applying distortions.
 Then include these new examples in your training data set.
\end_layout

\begin_layout Itemize
Usually, it doesn't help to add purely random/meaningless noise to your
 data.
 The distortion introduced should be representative of the type of distortions
 you might expect to see in your test data set.
\end_layout

\begin_layout Itemize
Discussion on getting more data:
\end_layout

\begin_deeper
\begin_layout Itemize
Make sure you have a low bias classifier before expending the effort (plot
 learning curves, keep increasing the number of features or hidden units
 (for a neural network) until you have a low bias classifier).
\end_layout

\begin_layout Itemize
Consider: how much work would it be to get 10 times as much data as we currently
 have?
\end_layout

\begin_deeper
\begin_layout Itemize
Artificial data synthesis.
\end_layout

\begin_layout Itemize
Collect/label it yourself.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Crowdsourcing
\begin_inset Quotes erd
\end_inset

, e.g.
 Amazon Mechanical Turk
\end_layout

\end_deeper
\end_deeper
\begin_layout Section*
Ceiling analysis: which part of the pipeline to work on next?
\end_layout

\begin_layout Itemize
Ceiling analysis: estimating the error due to each component of your pipeline.
 What part of the pipeline should you spend the most time trying to improve?
\end_layout

\begin_layout Itemize
Example: image -> text detection -> character segmentation -> character
 recognition.
\end_layout

\begin_deeper
\begin_layout Itemize
Overall system has accuracy 72%.
\end_layout

\begin_layout Itemize
Test and find that text detection has 89% accuracy.
\end_layout

\begin_layout Itemize
Now manually label text in images to simulate 100% text detection accuracy.
\end_layout

\begin_layout Itemize
Then check character segmentation accuracy: 90%.
\end_layout

\end_deeper
\end_body
\end_document
