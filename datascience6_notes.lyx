#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Statistical Inference - Notes
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 6/1/2015 - 6/28/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Subsection*

\series bold
Introduction
\end_layout

\begin_layout Itemize
Statistical inference: generating conclusions about a population from a
 noisy sample.
\end_layout

\begin_layout Subsection*
Probability
\end_layout

\begin_layout Itemize
Given a random experiement, a probability meausre is a population quantity
 that summarizes the randomness.
\end_layout

\begin_layout Itemize
Specifically, probability takes a possible outcome from the experiment and:
\end_layout

\begin_deeper
\begin_layout Itemize
Assigns it a number betwee 0 and 1.
\end_layout

\begin_layout Itemize
The probability that something happens should be 1.
\end_layout

\begin_layout Itemize
The probability of the union of any two sets of outcomes that are mutually
 exclusive is the sum of their respective probabilities.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $P(A\cup B)=P(A)+P(B)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Rules that probability must follow:
\end_layout

\begin_deeper
\begin_layout Itemize
The probability that nothing occurs is 0.
\end_layout

\begin_layout Itemize
The probability that something occurs is 1.
\end_layout

\begin_layout Itemize
The probability of something is 1 minus the probability that the opposite
 occurs.
\end_layout

\begin_layout Itemize
The probability of at least one of two or more things that cannot simultaneously
 occur is the sum of their respective probabilities.
\end_layout

\begin_layout Itemize
If an event A implies the occurrence of event B, then the probability of
 A occurring is less than the probability of event B.
\end_layout

\begin_layout Itemize
For any two events, the probability that at least one occurs is the sum
 of their probabilities minus the sum of their intersection.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probability densities and mass functions for random variables are useful
 for modeling and thinking about probabilities for the numeric outcome of
 experiments.
\end_layout

\begin_layout Itemize
A random variable is the numerical outcome of an experiment.
\end_layout

\begin_deeper
\begin_layout Itemize
Can be discrete or continuous.
\end_layout

\end_deeper
\begin_layout Itemize
Probability mass function: a probability mass function evaluated at a value
 corresponds to the probability that a random variable takes that value.
 To be a valid PMF, a function must satisfy:
\end_layout

\begin_deeper
\begin_layout Itemize
It must always be larger than or equal to zero.
\end_layout

\begin_layout Itemize
The sum of the probabilities for all possible values of the random variable
 has to add up to one
\end_layout

\end_deeper
\begin_layout Itemize
Example: Bernoulli distribution for a coin flip
\end_layout

\begin_deeper
\begin_layout Itemize
X = 0 represents tails and X = 1 represents heads.
 Probability of getting heads is 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $p\left(x\right)=\theta^{x}\left(1-\theta\right)^{1-x}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
A probability density function (PDF) is a function associated with a continuous
 random variable.
\end_layout

\begin_deeper
\begin_layout Itemize
Must be larger than or equal to zero.
\end_layout

\begin_layout Itemize
The total area under it must be one.
\end_layout

\begin_layout Itemize
Areas under PDFs correspond to probabilities for that random variable.
\end_layout

\end_deeper
\begin_layout Itemize
CDF and survival function.
\end_layout

\begin_deeper
\begin_layout Itemize
The cumulative distribution function of a random variable X returns the
 probability that the random variable is less than or equal to the value
 x.
\end_layout

\begin_layout Itemize
\begin_inset Formula $F\left(x\right)=P\left(X\leq x\right)$
\end_inset


\end_layout

\begin_layout Itemize
The survival function is just 
\begin_inset Formula $1-F\left(x\right)$
\end_inset

 and gives the probability that X is larger than or equal to x.
\end_layout

\end_deeper
\begin_layout Itemize
Quantiles
\end_layout

\begin_deeper
\begin_layout Itemize
Sample quantiles - 95th percentile means that 95% people did worse and 5%
 did better.
\end_layout

\begin_layout Itemize
Population quantiles: The 
\begin_inset Formula $\alpha$
\end_inset

th quantile of a distribution with CDF 
\begin_inset Formula $F\left(x\right)$
\end_inset

 is the point 
\begin_inset Formula $x_{\alpha}$
\end_inset

 such that 
\begin_inset Formula $F\left(x_{\alpha}\right)=\alpha$
\end_inset

.
\end_layout

\begin_layout Itemize
A percentile is simple a quantile with the quantile expressed as a percent.
\end_layout

\begin_layout Itemize
The median is the 50th percentile.
\end_layout

\begin_layout Itemize
R can approximate quantiles for you for common distributions.
\end_layout

\begin_deeper
\begin_layout Itemize
Use the 
\begin_inset Quotes eld
\end_inset

q
\begin_inset Quotes erd
\end_inset

 commands: 
\family typewriter
qbeta, qnorm, qpois
\family default
, etc.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Conditional probability
\end_layout

\begin_layout Itemize
Let B be an event such that 
\begin_inset Formula $P\left(B\right)>0$
\end_inset

.
\end_layout

\begin_layout Itemize
Then the conditional probability of an event A occurring is 
\begin_inset Formula $P\left(A|B\right)=\frac{P\left(A\cap B\right)}{P(B)}$
\end_inset

.
\end_layout

\begin_layout Itemize
If A and B are independent, 
\begin_inset Formula $P\left(A|B\right)=P\left(A\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Bayes' theorem: 
\begin_inset Formula $P\left(A|B\right)P\left(B\right)=P\left(B|A\right)P\left(A\right)$
\end_inset


\end_layout

\begin_layout Itemize
Definitions: consider an example where B means you have a disease and A
 is a positive test result.
\end_layout

\begin_deeper
\begin_layout Itemize
Sensitivity: 
\begin_inset Formula $P\left(A|B\right)$
\end_inset


\end_layout

\begin_layout Itemize
Specificity: 
\begin_inset Formula $P\left(\text{not A}|\text{not B}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Positive predictive value: 
\begin_inset Formula $P\left(B|A\right)$
\end_inset


\end_layout

\begin_layout Itemize
Negative predictive value: 
\begin_inset Formula $P\left(\text{not B}|\text{not A}\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Using Bayes' theorem: 
\begin_inset Formula $P\left(A|B\right)=\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)}$
\end_inset


\end_layout

\begin_layout Itemize
Independence: event A is independent of event B if 
\begin_inset Formula $P\left(A\cap B\right)=P\left(A\right)P\left(B\right)$
\end_inset


\end_layout

\begin_layout Itemize
IID random variables: independent and identically distributed.
\end_layout

\begin_deeper
\begin_layout Itemize
Independent: statistically unrelated from one to another.
\end_layout

\begin_layout Itemize
Identically distributed: all having been drawn from the same population
 model.
\end_layout

\end_deeper
\begin_layout Subsection*
Expected values
\end_layout

\begin_layout Itemize
The mean is a characterization of the center of a distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[X\right]=\sum_{x}xp\left(x\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The variance and standard deviation are characteristics of how spread out
 a distribution is.
\end_layout

\begin_layout Itemize
For a continuous random variable, the expected value is again exactly the
 center of mass of the density.
\end_layout

\begin_layout Itemize
The average of random variables is itself a random variable and its associated
 distribution has an expected value, but the center of this distribution
 is the same as that of the original distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
The sample mean is 
\series bold
unbiased
\series default
 because its distribution is centered at what it's trying to estimate.
\end_layout

\begin_layout Itemize
To put it another way: the distribution of averages of samples will have
 the same mean as that of the random variable sample itself.
\end_layout

\begin_layout Itemize
The more data that goes into the sample mean, the more concentrated its
 density will be around the population mean.
\end_layout

\end_deeper
\begin_layout Subsection*
Introduction to variability
\end_layout

\begin_layout Itemize
Variance is a measure of the spread of a distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Var\left(X\right)=E\left[\left(X-\mu\right)^{2}\right]=E\left[X^{2}\right]-E\left[X\right]^{2}$
\end_inset


\end_layout

\begin_layout Itemize
The square root of the variance is the standard deviation.
\end_layout

\end_deeper
\begin_layout Itemize
Sample variance: average squared distance of the observations from the sample
 mean
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $S^{2}=\frac{\sum_{i-1}\left(X_{i}-\bar{X}\right)^{2}}{n-1}$
\end_inset


\end_layout

\begin_layout Itemize
The expected value of the sample variance is the population variance.
\end_layout

\begin_layout Itemize
Dividing by 
\begin_inset Formula $n-1$
\end_inset

 instead of 
\begin_inset Formula $n$
\end_inset

 is what makes this an unbiased estimate of the population variance.
\end_layout

\end_deeper
\begin_layout Itemize
Standard error on the mean
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that the average of random samples from a population is itself a
 random variable.
\end_layout

\begin_layout Itemize
Expected value of sample mean: 
\begin_inset Formula $E\left[\bar{X}\right]=\mu$
\end_inset


\end_layout

\begin_layout Itemize
Variance of sample mean: 
\begin_inset Formula $Var\left(\bar{X}\right)=Var\left(\frac{1}{n}\sum_{i}X_{i}\right)=\frac{1}{n^{2}}Var\left(\sum_{i}X_{i}\right)=\frac{1}{n^{2}}\sum_{i}\sigma^{2}=\sigma^{2}/n$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The standard deviation talks about how variable the population is.
\end_layout

\begin_layout Itemize
The standard error 
\begin_inset Formula $S/\sqrt{n}$
\end_inset

 talks about how variable averages of random samples of size 
\begin_inset Formula $n$
\end_inset

 from the population are.
\end_layout

\begin_layout Itemize
The variance of a sample mean is 
\begin_inset Formula $\sigma^{2}/n$
\end_inset

, and we estimate it with 
\begin_inset Formula $S^{2}/n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The standard error of the sample mean is 
\begin_inset Formula $s/\sqrt{n}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Chebyshev's inequality: the probability that a random variable 
\begin_inset Formula $X$
\end_inset

 is at least 
\begin_inset Formula $k$
\end_inset

 standard deviations from its mean is less than 
\begin_inset Formula $1/k^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Pr\left(\left|X-\mu\right|\ge k\sigma\right)\le\frac{1}{k^{2}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Some common distributions
\end_layout

\begin_layout Itemize
Binomial distribution
\end_layout

\begin_deeper
\begin_layout Itemize
Comes from the Bernoulli distribution - result of a binary outcome like
 a coin flip.
 
\begin_inset Formula $P\left(X=x\right)=p^{x}\left(1-p\right)^{1-x}$
\end_inset


\end_layout

\begin_layout Itemize
A binomial variable is the sum of many IID Bernoulli trials.
\end_layout

\begin_deeper
\begin_layout Itemize
With 
\begin_inset Formula $n$
\end_inset

 trials, 
\begin_inset Formula $P\left(X=x\right)=\frac{n!}{x!\left(n-x\right)!}p^{x}\left(1-p\right)^{1-x}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal distribution
\end_layout

\begin_deeper
\begin_layout Itemize
With expected value 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

, the distribution is given by: 
\begin_inset Formula $\left(2\pi\sigma^{2}\right)^{_{1/2}}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}$
\end_inset


\end_layout

\begin_layout Itemize
Standard normal distribution has 
\begin_inset Formula $\mu=0$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
We can convert to a standard normal distribution from another distribution
 by taking 
\begin_inset Formula $X\rightarrow\frac{X-\mu}{\sigma}$
\end_inset

.
\end_layout

\begin_layout Itemize
We can go the other way by taking 
\begin_inset Formula $X\rightarrow\mu+\sigma X$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Poisson distribution
\end_layout

\begin_deeper
\begin_layout Itemize
Used to model counts, event time data, survival data, contingency tables,
 and more.
\end_layout

\begin_layout Itemize
\begin_inset Formula $P\left(X=x;\lambda\right)=\frac{\lambda^{x}e^{-\lambda}}{x!}$
\end_inset


\end_layout

\begin_layout Itemize
Mean = variance = 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Itemize
Poisson approximation to the binomial:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $X\sim Binomial(n,p)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda=np$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

 gets large, 
\begin_inset Formula $p$
\end_inset

 gets small.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Asymptotics
\end_layout

\begin_layout Itemize
Term for the behavior of statistics as the sample size or some other relevant
 quantity goes to infinity or zero.
\end_layout

\begin_layout Itemize
Very useful for simple statistical inference and approximations.
\end_layout

\begin_layout Itemize
Also form the basis for frequency interpretation of probabilities (the long
 run proportion of times an event occurs).
\end_layout

\begin_layout Itemize
The Law of Large Numbers (LLN)
\end_layout

\begin_deeper
\begin_layout Itemize
The sample mean converges to the population mean in the limit of infinite
 trials.
\end_layout

\end_deeper
\begin_layout Itemize
An estimator is 
\series bold
consistent
\series default
 if it converges to what you want to estimate.
\end_layout

\begin_layout Itemize
The LLN says that the sample mean of iid samples is consistent for the populatio
n mean.
\end_layout

\begin_layout Itemize
Typically, good estimators are consistent - we should expect to get the
 right answer if we collect infinite data.
\end_layout

\begin_layout Itemize
The Central Limit Theorem (CLT)
\end_layout

\begin_deeper
\begin_layout Itemize
The distribution of averages of iid variables (properly normalized) becomes
 that of a standard normal distribution as the sample size increases.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\bar{X}_{n}-\mu}{\sigma/\sqrt{n}}=\frac{\text{Estimate - Mean of estimate}}{\text{Std. Err. of estimate}}$
\end_inset


\end_layout

\begin_layout Itemize
The useful way to think about the CLT is that 
\begin_inset Formula $\bar{X}_{n}$
\end_inset

 is approximately 
\begin_inset Formula $N\left(\mu,\sigma^{2}/n\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Standard deviation is equal to the standard error on the mean.
\end_layout

\begin_layout Itemize
No guarantee that 
\begin_inset Formula $n$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

big enough.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Confidence intervals
\end_layout

\begin_deeper
\begin_layout Itemize
For a sample mean of a normally distributed random variable, 
\begin_inset Formula $\bar{X}\pm2\sigma/\sqrt{n}$
\end_inset

 is called a 95% confidence interval for 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Itemize
Sometimes, if your number of trials isn't large enough for the CLT to be
 applicable, you can form the interval with 
\begin_inset Formula $\left(X+2\right)/\left(n+4\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Taking the mean and adding and subtracting the relevant normal quantile
 times the standard error yields a confidence interval for the mean.
\end_layout

\begin_layout Itemize
Confidence intervals get wider as the coverage increases.
\end_layout

\end_deeper
\begin_layout Subsection*
T confidence intervals
\end_layout

\begin_layout Itemize
Using T quantiles rather than Z quantiles.
\end_layout

\begin_layout Itemize
Tails will be a little wider than for Z intervals.
\end_layout

\begin_layout Itemize
T intervals are useful for small sample sizes, will become like the Z intervals
 in the limit of lots of data.
\end_layout

\begin_layout Itemize
The T distribution is indexed by the degrees of freedom; it gets more like
 a standard normal as df gets larger.
\end_layout

\begin_layout Itemize
The T distribution assumes that the underlying data are IID Gaussian with
 the result that 
\begin_inset Formula $\frac{\bar{X}-\mu}{S/\sqrt{n}}$
\end_inset

 follows Gosset's T distribution with 
\begin_inset Formula $n-1$
\end_inset

 degrees of freedom.
\end_layout

\begin_layout Itemize
The T intervals are 
\begin_inset Formula $\bar{X}\pm t_{n-1}S/\sqrt{n}$
\end_inset

, where 
\begin_inset Formula $t_{n-1}$
\end_inset

 is the relevant quantile.
\end_layout

\begin_layout Itemize
This distribution works well whenever the distribution of the data is roughly
 symmetric and mound-shaped.
\end_layout

\begin_layout Itemize
Paired observations are often analyzed using the T interval by taking difference
s.
\end_layout

\begin_layout Itemize
The spirit of the T interval assumptions are violated for skewed distributions.
\end_layout

\begin_layout Itemize
Other intervals are more useful for highly discrete data.
\end_layout

\begin_layout Itemize
Independent group T confidence intervals - comparing means between two different
 groups in a randomized trial.
\end_layout

\begin_deeper
\begin_layout Itemize
Can't use a paired T test because the groups are independent and may have
 different sample sizes.
\end_layout

\begin_layout Itemize
Standard confidence interval to use in this situation: 
\begin_inset Formula $\bar{Y}-\bar{X}\pm t_{n_{x}+n_{y}-2,1-\alpha/2}S_{p}\left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)^{1/2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $t_{n_{x}+n_{y}-2,1-\alpha/2}$
\end_inset

 is the relevant T quantile, where 
\begin_inset Formula $n_{i}$
\end_inset

 is the number of observations in group 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)^{1/2}$
\end_inset

 is the standard error of the difference.
 Gets smaller as we collect more data.
\end_layout

\begin_layout Itemize
\begin_inset Formula $S_{p}^{2}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

pooled variance.
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $S_{p}^{2}=\left[\left(n_{x}-1\right)S_{x}^{2}+\left(n_{y}-1\right)S_{y}^{2}\right]/\left(n_{x}+n_{y}-2\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
This interval assumes a constant variance across the two groups!
\end_layout

\begin_layout Itemize
If there is some doubt, there is a method for using a different variance
 per group, which we will discuss later.
\end_layout

\end_deeper
\begin_layout Itemize
Unequal variances: 
\begin_inset Formula $\bar{Y}-\bar{X}\pm t_{df}\left(\frac{s_{x}^{2}}{n_{x}}+\frac{s_{y}^{2}}{n_{y}}\right)^{1/2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In this case, the distribution does not actually follow a T distribution!
\end_layout

\begin_layout Itemize
We can approximate it with a T distribution with degrees of freedom 
\begin_inset Formula $df=\frac{\left(S_{x}^{2}/n_{x}+S_{y}^{2}/n_{y}\right)^{2}}{\left(\frac{S_{x}^{2}}{n_{x}}\right)^{2}/\left(n_{x}-1\right)+\left(\frac{S_{y}^{2}}{n_{y}}\right)^{2}/\left(n_{y}-1\right)}$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Degrees of freedom may be fractional, but it's OK.
\end_layout

\end_deeper
\begin_layout Itemize
To do this in R, use 
\family typewriter
t.test()
\family default
, but set 
\family typewriter
var.equal=FALSE
\family default
.
\end_layout

\end_deeper
\begin_layout Subsection*
Hypothesis testing
\end_layout

\begin_layout Itemize
Hypothesis testing is concerned with making decisions using data.
\end_layout

\begin_layout Itemize
A null hypothesis is specified that represents the status quo, usually labeled
 
\begin_inset Formula $H_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
The null hypothesis is assumed to be true and statistical evidence is required
 to reject it in favor or a research or alternative hypothesis, 
\begin_inset Formula $H_{a}$
\end_inset

.
\end_layout

\begin_layout Itemize
There are four possible outcomes of our statistical decision process:
\end_layout

\begin_deeper
\begin_layout Itemize
Correctly accept null.
\end_layout

\begin_layout Itemize
Type I error: select 
\begin_inset Formula $H_{a}$
\end_inset

 when the null hypothesis is true.
\end_layout

\begin_layout Itemize
Correctly reject null.
\end_layout

\begin_layout Itemize
Type II error: select null hypothesis when 
\begin_inset Formula $H_{a}$
\end_inset

 is true.
\end_layout

\begin_layout Itemize
As the type I error rate increases, the type II error rate decreases, and
 vice versa.
\end_layout

\end_deeper
\begin_layout Itemize
Typical way to do decision making: reject the null hypothesis if 
\begin_inset Formula $\bar{X}$
\end_inset

 is larger than some constant 
\begin_inset Formula $C$
\end_inset

, chosen such that the probability of the type I error is 0.05.
\end_layout

\begin_layout Itemize
Two-sided tests
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose that we would reject the null hypothesis if in fact the mean was
 too large or too small.
\end_layout

\begin_layout Itemize
We will reject if the test statistic is either too large or too small.
\end_layout

\begin_layout Itemize
Then we want the probability of rejecting the null hypothesis to be 5%,
 split equally as 2.5% in the upper tail and 2.5% in the lower tail.
\end_layout

\begin_layout Itemize
Thus, we reject if our test statistic if larger than 
\family typewriter
qt(0.975)
\family default
 or smaller than 
\family typewriter
qt(0.025)
\family default
.
\end_layout

\begin_layout Itemize
In general, if you fail to reject the one-sided test, you should fail to
 reject the two-sided test as well.
\end_layout

\end_deeper
\begin_layout Subsection*
P-values
\end_layout

\begin_layout Itemize
Most common measure of statistical significance.
\end_layout

\begin_layout Itemize
Somewhat controversial among statisticians because they are used often and
 are commonly misinterpreted.
\end_layout

\begin_layout Itemize
What is a p-value?
\end_layout

\begin_deeper
\begin_layout Itemize
Assume that nothing is going on (null hypothesis) - how unusual is the result
 we got?
\end_layout

\end_deeper
\begin_layout Itemize
Approach:
\end_layout

\begin_deeper
\begin_layout Itemize
Define the hypothetical distribution of a statistic when 
\begin_inset Quotes eld
\end_inset

nothing
\begin_inset Quotes erd
\end_inset

 is going on (null distribution).
\end_layout

\begin_layout Itemize
Calculate the statistic with the data we have (test statistic).
\end_layout

\begin_layout Itemize
Compare what we calculated to our hypothetical distribution and see if the
 value is 
\begin_inset Quotes eld
\end_inset

extreme
\begin_inset Quotes erd
\end_inset

 (p-value).
\end_layout

\end_deeper
\begin_layout Itemize
Formal definition: probability under the null hypothesis of obtaining evidence
 as extreme or more extreme than that obtained.
\end_layout

\begin_deeper
\begin_layout Itemize
If the p-value is small, then either 
\begin_inset Formula $H_{0}$
\end_inset

 is true and we have observed a rare event, or 
\begin_inset Formula $H_{0}$
\end_inset

 is false.
\end_layout

\end_deeper
\begin_layout Itemize
Example: getting a T statistic of 2.5 for 15 degrees of freedom.
\end_layout

\begin_deeper
\begin_layout Itemize
What's the probability of getting a T statistic as large as 2.5?
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
pt(2.5, 15, lower.tail=FALSE)
\family default
 gives 0.01225.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Can also think of the p-value as the 
\begin_inset Quotes eld
\end_inset

attained significance level.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Smallest value of 
\begin_inset Formula $\alpha$
\end_inset

 for which we would reject the null hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
By reporting a p-value, the reader can perform the hypothesis test at whatever
 
\begin_inset Formula $\alpha$
\end_inset

 level they want.
\end_layout

\begin_deeper
\begin_layout Itemize
If the p-value is less than 
\begin_inset Formula $\alpha$
\end_inset

, you reject the null hypothesis.
\end_layout

\begin_layout Itemize
For a two-sided hypothesis test, double the smaller of the two one-sided
 hypothesis test p-values.
\end_layout

\end_deeper
\end_body
\end_document
