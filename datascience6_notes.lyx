#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Statistical Inference - Notes
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 6/1/2015 - 6/28/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Subsection*

\series bold
Introduction
\end_layout

\begin_layout Itemize
Statistical inference: generating conclusions about a population from a
 noisy sample.
\end_layout

\begin_layout Subsection*
Probability
\end_layout

\begin_layout Itemize
Given a random experiement, a probability meausre is a population quantity
 that summarizes the randomness.
\end_layout

\begin_layout Itemize
Specifically, probability takes a possible outcome from the experiment and:
\end_layout

\begin_deeper
\begin_layout Itemize
Assigns it a number betwee 0 and 1.
\end_layout

\begin_layout Itemize
The probability that something happens should be 1.
\end_layout

\begin_layout Itemize
The probability of the union of any two sets of outcomes that are mutually
 exclusive is the sum of their respective probabilities.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $P(A\cup B)=P(A)+P(B)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Rules that probability must follow:
\end_layout

\begin_deeper
\begin_layout Itemize
The probability that nothing occurs is 0.
\end_layout

\begin_layout Itemize
The probability that something occurs is 1.
\end_layout

\begin_layout Itemize
The probability of something is 1 minus the probability that the opposite
 occurs.
\end_layout

\begin_layout Itemize
The probability of at least one of two or more things that cannot simultaneously
 occur is the sum of their respective probabilities.
\end_layout

\begin_layout Itemize
If an event A implies the occurrence of event B, then the probability of
 A occurring is less than the probability of event B.
\end_layout

\begin_layout Itemize
For any two events, the probability that at least one occurs is the sum
 of their probabilities minus the sum of their intersection.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probability densities and mass functions for random variables are useful
 for modeling and thinking about probabilities for the numeric outcome of
 experiments.
\end_layout

\begin_layout Itemize
A random variable is the numerical outcome of an experiment.
\end_layout

\begin_deeper
\begin_layout Itemize
Can be discrete or continuous.
\end_layout

\end_deeper
\begin_layout Itemize
Probability mass function: a probability mass function evaluated at a value
 corresponds to the probability that a random variable takes that value.
 To be a valid PMF, a function must satisfy:
\end_layout

\begin_deeper
\begin_layout Itemize
It must always be larger than or equal to zero.
\end_layout

\begin_layout Itemize
The sum of the probabilities for all possible values of the random variable
 has to add up to one
\end_layout

\end_deeper
\begin_layout Itemize
Example: Bernoulli distribution for a coin flip
\end_layout

\begin_deeper
\begin_layout Itemize
X = 0 represents tails and X = 1 represents heads.
 Probability of getting heads is 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $p\left(x\right)=\theta^{x}\left(1-\theta\right)^{1-x}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
A probability density function (PDF) is a function associated with a continuous
 random variable.
\end_layout

\begin_deeper
\begin_layout Itemize
Must be larger than or equal to zero.
\end_layout

\begin_layout Itemize
The total area under it must be one.
\end_layout

\begin_layout Itemize
Areas under PDFs correspond to probabilities for that random variable.
\end_layout

\end_deeper
\begin_layout Itemize
CDF and survival function.
\end_layout

\begin_deeper
\begin_layout Itemize
The cumulative distribution function of a random variable X returns the
 probability that the random variable is less than or equal to the value
 x.
\end_layout

\begin_layout Itemize
\begin_inset Formula $F\left(x\right)=P\left(X\leq x\right)$
\end_inset


\end_layout

\begin_layout Itemize
The survival function is just 
\begin_inset Formula $1-F\left(x\right)$
\end_inset

 and gives the probability that X is larger than or equal to x.
\end_layout

\end_deeper
\begin_layout Itemize
Quantiles
\end_layout

\begin_deeper
\begin_layout Itemize
Sample quantiles - 95th percentile means that 95% people did worse and 5%
 did better.
\end_layout

\begin_layout Itemize
Population quantiles: The 
\begin_inset Formula $\alpha$
\end_inset

th quantile of a distribution with CDF 
\begin_inset Formula $F\left(x\right)$
\end_inset

 is the point 
\begin_inset Formula $x_{\alpha}$
\end_inset

 such that 
\begin_inset Formula $F\left(x_{\alpha}\right)=\alpha$
\end_inset

.
\end_layout

\begin_layout Itemize
A percentile is simple a quantile with the quantile expressed as a percent.
\end_layout

\begin_layout Itemize
The median is the 50th percentile.
\end_layout

\begin_layout Itemize
R can approximate quantiles for you for common distributions.
\end_layout

\begin_deeper
\begin_layout Itemize
Use the 
\begin_inset Quotes eld
\end_inset

q
\begin_inset Quotes erd
\end_inset

 commands: 
\family typewriter
qbeta, qnorm, qpois
\family default
, etc.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Conditional probability
\end_layout

\begin_layout Itemize
Let B be an event such that 
\begin_inset Formula $P\left(B\right)>0$
\end_inset

.
\end_layout

\begin_layout Itemize
Then the conditional probability of an event A occurring is 
\begin_inset Formula $P\left(A|B\right)=\frac{P\left(A\cap B\right)}{P(B)}$
\end_inset

.
\end_layout

\begin_layout Itemize
If A and B are independent, 
\begin_inset Formula $P\left(A|B\right)=P\left(A\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Bayes' theorem: 
\begin_inset Formula $P\left(A|B\right)P\left(B\right)=P\left(B|A\right)P\left(A\right)$
\end_inset


\end_layout

\begin_layout Itemize
Definitions: consider an example where B means you have a disease and A
 is a positive test result.
\end_layout

\begin_deeper
\begin_layout Itemize
Sensitivity: 
\begin_inset Formula $P\left(A|B\right)$
\end_inset


\end_layout

\begin_layout Itemize
Specificity: 
\begin_inset Formula $P\left(\text{not A}|\text{not B}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Positive predictive value: 
\begin_inset Formula $P\left(B|A\right)$
\end_inset


\end_layout

\begin_layout Itemize
Negative predictive value: 
\begin_inset Formula $P\left(\text{not B}|\text{not A}\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Using Bayes' theorem: 
\begin_inset Formula $P\left(A|B\right)=\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)}$
\end_inset


\end_layout

\begin_layout Itemize
Independence: event A is independent of event B if 
\begin_inset Formula $P\left(A\cap B\right)=P\left(A\right)P\left(B\right)$
\end_inset


\end_layout

\begin_layout Itemize
IID random variables: independent and identically distributed.
\end_layout

\begin_deeper
\begin_layout Itemize
Independent: statistically unrelated from one to another.
\end_layout

\begin_layout Itemize
Identically distributed: all having been drawn from the same population
 model.
\end_layout

\end_deeper
\begin_layout Subsection*
Expected values
\end_layout

\begin_layout Itemize
The mean is a characterization of the center of a distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[X\right]=\sum_{x}xp\left(x\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The variance and standard deviation are characteristics of how spread out
 a distribution is.
\end_layout

\begin_layout Itemize
For a continuous random variable, the expected value is again exactly the
 center of mass of the density.
\end_layout

\begin_layout Itemize
The average of random variables is itself a random variable and its associated
 distribution has an expected value, but the center of this distribution
 is the same as that of the original distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
The sample mean is 
\series bold
unbiased
\series default
 because its distribution is centered at what it's trying to estimate.
\end_layout

\begin_layout Itemize
To put it another way: the distribution of averages of samples will have
 the same mean as that of the random variable sample itself.
\end_layout

\begin_layout Itemize
The more data that goes into the sample mean, the more concentrated its
 density will be around the population mean.
\end_layout

\end_deeper
\begin_layout Subsection*
Introduction to variability
\end_layout

\begin_layout Itemize
Variance is a measure of the spread of a distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Var\left(X\right)=E\left[\left(X-\mu\right)^{2}\right]=E\left[X^{2}\right]-E\left[X\right]^{2}$
\end_inset


\end_layout

\begin_layout Itemize
The square root of the variance is the standard deviation.
\end_layout

\end_deeper
\begin_layout Itemize
Sample variance: average squared distance of the observations from the sample
 mean
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $S^{2}=\frac{\sum_{i-1}\left(X_{i}-\bar{X}\right)^{2}}{n-1}$
\end_inset


\end_layout

\begin_layout Itemize
The expected value of the sample variance is the population variance.
\end_layout

\begin_layout Itemize
Dividing by 
\begin_inset Formula $n-1$
\end_inset

 instead of 
\begin_inset Formula $n$
\end_inset

 is what makes this an unbiased estimate of the population variance.
\end_layout

\end_deeper
\begin_layout Itemize
Standard error on the mean
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that the average of random samples from a population is itself a
 random variable.
\end_layout

\begin_layout Itemize
Expected value of sample mean: 
\begin_inset Formula $E\left[\bar{X}\right]=\mu$
\end_inset


\end_layout

\begin_layout Itemize
Variance of sample mean: 
\begin_inset Formula $Var\left(\bar{X}\right)=Var\left(\frac{1}{n}\sum_{i}X_{i}\right)=\frac{1}{n^{2}}Var\left(\sum_{i}X_{i}\right)=\frac{1}{n^{2}}\sum_{i}\sigma^{2}=\sigma^{2}/n$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The standard deviation talks about how variable the population is.
\end_layout

\begin_layout Itemize
The standard error 
\begin_inset Formula $S/\sqrt{n}$
\end_inset

 talks about how variable averages of random samples of size 
\begin_inset Formula $n$
\end_inset

 from the population are.
\end_layout

\begin_layout Itemize
The variance of a sample mean is 
\begin_inset Formula $\sigma^{2}/n$
\end_inset

, and we estimate it with 
\begin_inset Formula $S^{2}/n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The standard error of the sample mean is 
\begin_inset Formula $s/\sqrt{n}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Chebyshev's inequality: the probability that a random variable 
\begin_inset Formula $X$
\end_inset

 is at least 
\begin_inset Formula $k$
\end_inset

 standard deviations from its mean is less than 
\begin_inset Formula $1/k^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Pr\left(\left|X-\mu\right|\ge k\sigma\right)\le\frac{1}{k^{2}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Some common distributions
\end_layout

\begin_layout Itemize
Binomial distribution
\end_layout

\begin_deeper
\begin_layout Itemize
Comes from the Bernoulli distribution - result of a binary outcome like
 a coin flip.
 
\begin_inset Formula $P\left(X=x\right)=p^{x}\left(1-p\right)^{1-x}$
\end_inset


\end_layout

\begin_layout Itemize
A binomial variable is the sum of many IID Bernoulli trials.
\end_layout

\begin_deeper
\begin_layout Itemize
With 
\begin_inset Formula $n$
\end_inset

 trials, 
\begin_inset Formula $P\left(X=x\right)=\frac{n!}{x!\left(n-x\right)!}p^{x}\left(1-p\right)^{1-x}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Normal distribution
\end_layout

\begin_deeper
\begin_layout Itemize
With expected value 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

, the distribution is given by: 
\begin_inset Formula $\left(2\pi\sigma^{2}\right)^{_{1/2}}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}$
\end_inset


\end_layout

\begin_layout Itemize
Standard normal distribution has 
\begin_inset Formula $\mu=0$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
We can convert to a standard normal distribution from another distribution
 by taking 
\begin_inset Formula $X\rightarrow\frac{X-\mu}{\sigma}$
\end_inset

.
\end_layout

\begin_layout Itemize
We can go the other way by taking 
\begin_inset Formula $X\rightarrow\mu+\sigma X$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Poisson distribution
\end_layout

\begin_deeper
\begin_layout Itemize
Used to model counts, event time data, survival data, contingency tables,
 and more.
\end_layout

\begin_layout Itemize
\begin_inset Formula $P\left(X=x;\lambda\right)=\frac{\lambda^{x}e^{-\lambda}}{x!}$
\end_inset


\end_layout

\begin_layout Itemize
Mean = variance = 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Itemize
Poisson approximation to the binomial:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $X\sim Binomial(n,p)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda=np$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

 gets large, 
\begin_inset Formula $p$
\end_inset

 gets small.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Asymptotics
\end_layout

\begin_layout Itemize
Term for the behavior of statistics as the sample size or some other relevant
 quantity goes to infinity or zero.
\end_layout

\begin_layout Itemize
Very useful for simple statistical inference and approximations.
\end_layout

\begin_layout Itemize
Also form the basis for frequency interpretation of probabilities (the long
 run proportion of times an event occurs).
\end_layout

\begin_layout Itemize
The Law of Large Numbers (LLN)
\end_layout

\begin_deeper
\begin_layout Itemize
The sample mean converges to the population mean in the limit of infinite
 trials.
\end_layout

\end_deeper
\begin_layout Itemize
An estimator is 
\series bold
consistent
\series default
 if it converges to what you want to estimate.
\end_layout

\begin_layout Itemize
The LLN says that the sample mean of iid samples is consistent for the populatio
n mean.
\end_layout

\begin_layout Itemize
Typically, good estimators are consistent - we should expect to get the
 right answer if we collect infinite data.
\end_layout

\begin_layout Itemize
The Central Limit Theorem (CLT)
\end_layout

\begin_deeper
\begin_layout Itemize
The distribution of averages of iid variables (properly normalized) becomes
 that of a standard normal distribution as the sample size increases.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\bar{X}_{n}-\mu}{\sigma/\sqrt{n}}=\frac{\text{Estimate - Mean of estimate}}{\text{Std. Err. of estimate}}$
\end_inset


\end_layout

\begin_layout Itemize
The useful way to think about the CLT is that 
\begin_inset Formula $\bar{X}_{n}$
\end_inset

 is approximately 
\begin_inset Formula $N\left(\mu,\sigma^{2}/n\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Standard deviation is equal to the standard error on the mean.
\end_layout

\begin_layout Itemize
No guarantee that 
\begin_inset Formula $n$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

big enough.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Confidence intervals
\end_layout

\begin_deeper
\begin_layout Itemize
For a sample mean of a normally distributed random variable, 
\begin_inset Formula $\bar{X}\pm2\sigma/\sqrt{n}$
\end_inset

 is called a 95% confidence interval for 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Itemize
Sometimes, if your number of trials isn't large enough for the CLT to be
 applicable, you can form the interval with 
\begin_inset Formula $\left(X+2\right)/\left(n+4\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Taking the mean and adding and subtracting the relevant normal quantile
 times the standard error yields a confidence interval for the mean.
\end_layout

\begin_layout Itemize
Confidence intervals get wider as the coverage increases.
\end_layout

\end_deeper
\begin_layout Subsection*
T confidence intervals
\end_layout

\begin_layout Itemize
Using T quantiles rather than Z quantiles.
\end_layout

\begin_layout Itemize
Tails will be a little wider than for Z intervals.
\end_layout

\begin_layout Itemize
T intervals are useful for small sample sizes, will become like the Z intervals
 in the limit of lots of data.
\end_layout

\begin_layout Itemize
The T distribution is indexed by the degrees of freedom; it gets more like
 a standard normal as df gets larger.
\end_layout

\begin_layout Itemize
The T distribution assumes that the underlying data are IID Gaussian with
 the result that 
\begin_inset Formula $\frac{\bar{X}-\mu}{S/\sqrt{n}}$
\end_inset

 follows Gosset's T distribution with 
\begin_inset Formula $n-1$
\end_inset

 degrees of freedom.
\end_layout

\begin_layout Itemize
The T intervals are 
\begin_inset Formula $\bar{X}\pm t_{n-1}S/\sqrt{n}$
\end_inset

, where 
\begin_inset Formula $t_{n-1}$
\end_inset

 is the relevant quantile.
\end_layout

\begin_layout Itemize
This distribution works well whenever the distribution of the data is roughly
 symmetric and mound-shaped.
\end_layout

\begin_layout Itemize
Paired observations are often analyzed using the T interval by taking difference
s.
\end_layout

\begin_deeper
\begin_layout Itemize
Get mean of differences: 
\begin_inset Formula $\bar{Z}$
\end_inset


\end_layout

\begin_layout Itemize
Get sigma of differences: 
\begin_inset Formula $s^{2}=\sum_{i=1}^{n}\frac{\left(Z_{i}-\bar{Z}\right)^{2}}{n-1}$
\end_inset


\end_layout

\begin_layout Itemize
T statistic: 
\begin_inset Formula $\frac{\bar{Z}-H_{a}}{s/\sqrt{n}}$
\end_inset

, where 
\begin_inset Formula $H_{a}$
\end_inset

 is the hypothesis (taken to be 0 for the null hypothesis).
\end_layout

\end_deeper
\begin_layout Itemize
The spirit of the T interval assumptions are violated for skewed distributions.
\end_layout

\begin_layout Itemize
Other intervals are more useful for highly discrete data.
\end_layout

\begin_layout Itemize
Independent group T confidence intervals - comparing means between two different
 groups in a randomized trial.
\end_layout

\begin_deeper
\begin_layout Itemize
Can't use a paired T test because the groups are independent and may have
 different sample sizes.
\end_layout

\begin_layout Itemize
Standard confidence interval to use in this situation: 
\begin_inset Formula $\bar{Y}-\bar{X}\pm t_{n_{x}+n_{y}-2,1-\alpha/2}S_{p}\left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)^{1/2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $t_{n_{x}+n_{y}-2,1-\alpha/2}$
\end_inset

 is the relevant T quantile, where 
\begin_inset Formula $n_{i}$
\end_inset

 is the number of observations in group 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)^{1/2}$
\end_inset

 is the standard error of the difference.
 Gets smaller as we collect more data.
\end_layout

\begin_layout Itemize
\begin_inset Formula $S_{p}^{2}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

pooled variance.
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $S_{p}^{2}=\left[\left(n_{x}-1\right)S_{x}^{2}+\left(n_{y}-1\right)S_{y}^{2}\right]/\left(n_{x}+n_{y}-2\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
This interval assumes a constant variance across the two groups!
\end_layout

\begin_layout Itemize
If there is some doubt, there is a method for using a different variance
 per group, which we will discuss later.
\end_layout

\end_deeper
\begin_layout Itemize
Unequal variances: 
\begin_inset Formula $\bar{Y}-\bar{X}\pm t_{df}\left(\frac{s_{x}^{2}}{n_{x}}+\frac{s_{y}^{2}}{n_{y}}\right)^{1/2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In this case, the distribution does not actually follow a T distribution!
\end_layout

\begin_layout Itemize
We can approximate it with a T distribution with degrees of freedom 
\begin_inset Formula $df=\frac{\left(S_{x}^{2}/n_{x}+S_{y}^{2}/n_{y}\right)^{2}}{\left(\frac{S_{x}^{2}}{n_{x}}\right)^{2}/\left(n_{x}-1\right)+\left(\frac{S_{y}^{2}}{n_{y}}\right)^{2}/\left(n_{y}-1\right)}$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Degrees of freedom may be fractional, but it's OK.
\end_layout

\end_deeper
\begin_layout Itemize
To do this in R, use 
\family typewriter
t.test()
\family default
, but set 
\family typewriter
var.equal=FALSE
\family default
.
\end_layout

\begin_layout Itemize
T-statistic: 
\begin_inset Formula $t=\frac{\bar{Y}-\bar{X}}{\sqrt{S_{x}^{2}/n_{x}+S_{y}^{2}/n_{y}}}$
\end_inset


\end_layout

\begin_layout Itemize
Calculate p-value: 
\family typewriter
pt(t, df=df, lower.tail=FALSE)
\family default
.
\end_layout

\begin_deeper
\begin_layout Itemize
Multiply by 2 if doing a two-sided test.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Hypothesis testing
\end_layout

\begin_layout Itemize
Hypothesis testing is concerned with making decisions using data.
\end_layout

\begin_layout Itemize
A null hypothesis is specified that represents the status quo, usually labeled
 
\begin_inset Formula $H_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
The null hypothesis is assumed to be true and statistical evidence is required
 to reject it in favor or a research or alternative hypothesis, 
\begin_inset Formula $H_{a}$
\end_inset

.
\end_layout

\begin_layout Itemize
There are four possible outcomes of our statistical decision process:
\end_layout

\begin_deeper
\begin_layout Itemize
Correctly accept null.
\end_layout

\begin_layout Itemize
Type I error: select 
\begin_inset Formula $H_{a}$
\end_inset

 when the null hypothesis is true.
\end_layout

\begin_layout Itemize
Correctly reject null.
\end_layout

\begin_layout Itemize
Type II error: select null hypothesis when 
\begin_inset Formula $H_{a}$
\end_inset

 is true.
\end_layout

\begin_layout Itemize
As the type I error rate increases, the type II error rate decreases, and
 vice versa.
\end_layout

\end_deeper
\begin_layout Itemize
Typical way to do decision making: reject the null hypothesis if 
\begin_inset Formula $\bar{X}$
\end_inset

 is larger than some constant 
\begin_inset Formula $C$
\end_inset

, chosen such that the probability of the type I error is 0.05.
\end_layout

\begin_layout Itemize
Two-sided tests
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose that we would reject the null hypothesis if in fact the mean was
 too large or too small.
\end_layout

\begin_layout Itemize
We will reject if the test statistic is either too large or too small.
\end_layout

\begin_layout Itemize
Then we want the probability of rejecting the null hypothesis to be 5%,
 split equally as 2.5% in the upper tail and 2.5% in the lower tail.
\end_layout

\begin_layout Itemize
Thus, we reject if our test statistic if larger than 
\family typewriter
qt(0.975)
\family default
 or smaller than 
\family typewriter
qt(0.025)
\family default
.
\end_layout

\begin_layout Itemize
In general, if you fail to reject the one-sided test, you should fail to
 reject the two-sided test as well.
\end_layout

\end_deeper
\begin_layout Subsection*
P-values
\end_layout

\begin_layout Itemize
Most common measure of statistical significance.
\end_layout

\begin_layout Itemize
Somewhat controversial among statisticians because they are used often and
 are commonly misinterpreted.
\end_layout

\begin_layout Itemize
What is a p-value?
\end_layout

\begin_deeper
\begin_layout Itemize
Assume that nothing is going on (null hypothesis) - how unusual is the result
 we got?
\end_layout

\end_deeper
\begin_layout Itemize
Approach:
\end_layout

\begin_deeper
\begin_layout Itemize
Define the hypothetical distribution of a statistic when 
\begin_inset Quotes eld
\end_inset

nothing
\begin_inset Quotes erd
\end_inset

 is going on (null distribution).
\end_layout

\begin_layout Itemize
Calculate the statistic with the data we have (test statistic).
\end_layout

\begin_layout Itemize
Compare what we calculated to our hypothetical distribution and see if the
 value is 
\begin_inset Quotes eld
\end_inset

extreme
\begin_inset Quotes erd
\end_inset

 (p-value).
\end_layout

\end_deeper
\begin_layout Itemize
Formal definition: probability under the null hypothesis of obtaining evidence
 as extreme or more extreme than that obtained.
\end_layout

\begin_deeper
\begin_layout Itemize
If the p-value is small, then either 
\begin_inset Formula $H_{0}$
\end_inset

 is true and we have observed a rare event, or 
\begin_inset Formula $H_{0}$
\end_inset

 is false.
\end_layout

\end_deeper
\begin_layout Itemize
Example: getting a T statistic of 2.5 for 15 degrees of freedom.
\end_layout

\begin_deeper
\begin_layout Itemize
What's the probability of getting a T statistic as large as 2.5?
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
pt(2.5, 15, lower.tail=FALSE)
\family default
 gives 0.01225.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Can also think of the p-value as the 
\begin_inset Quotes eld
\end_inset

attained significance level.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Smallest value of 
\begin_inset Formula $\alpha$
\end_inset

 for which we would reject the null hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
By reporting a p-value, the reader can perform the hypothesis test at whatever
 
\begin_inset Formula $\alpha$
\end_inset

 level they want.
\end_layout

\begin_deeper
\begin_layout Itemize
If the p-value is less than 
\begin_inset Formula $\alpha$
\end_inset

, you reject the null hypothesis.
\end_layout

\begin_layout Itemize
For a two-sided hypothesis test, double the smaller of the two one-sided
 hypothesis test p-values.
\end_layout

\end_deeper
\begin_layout Subsection*
Power
\end_layout

\begin_layout Itemize
Power is the probability of rejecting the null hypothesis when it is false.
 Power is a good thing!
\end_layout

\begin_deeper
\begin_layout Itemize
Comes into play more when you fail to reject the null hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
A type II error is failing to reject the null hypothesis when it is false.
 The probability of a type II error is usually called 
\begin_inset Formula $\beta$
\end_inset

, and power is calculated as 
\begin_inset Formula $Power=1-\beta$
\end_inset

.
\end_layout

\begin_layout Itemize
Example:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $H_{0}:\mu=30$
\end_inset

 vs.
 
\begin_inset Formula $H_{a}:\mu>30$
\end_inset

.
\end_layout

\begin_layout Itemize
T-statistic: 
\begin_inset Formula $\frac{\bar{X}-30}{s/\sqrt{n}}$
\end_inset


\end_layout

\begin_layout Itemize
Power: 
\begin_inset Formula $P\left(\frac{\bar{X}-30}{s/\sqrt{n}}>t_{1-\alpha,n-1};\mu=\mu_{a}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is equal to 
\begin_inset Formula $\alpha$
\end_inset

 for 
\begin_inset Formula $\mu_{a}=30$
\end_inset

 (null hypothesis).
\end_layout

\begin_layout Itemize
It's not equal to 
\begin_inset Formula $\alpha$
\end_inset

 for 
\begin_inset Formula $\mu_{a}>30$
\end_inset

, but it approaches 
\begin_inset Formula $\alpha$
\end_inset

 as 
\begin_inset Formula $\mu_{a}$
\end_inset

 goes to 30.
\end_layout

\end_deeper
\begin_layout Itemize
We assume the statistic follows a t-distribution under the null hypothesis.
\end_layout

\end_deeper
\begin_layout Itemize
If we calculate a power of 0.64 for a mean of 32 when the null hypothesis
 is 30, that means we have a 64% chance of detecting a mean as large as
 32.
\end_layout

\begin_layout Itemize
Calculating power for Gaussian data
\end_layout

\begin_deeper
\begin_layout Itemize
We reject if 
\begin_inset Formula $\frac{\bar{X}-30}{\sigma/\sqrt{n}}h>z_{1-\alpha}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Equivalently, we reject if 
\begin_inset Formula $\bar{X}>30+Z_{1-\alpha}\frac{\sigma}{\sqrt{n}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Under 
\begin_inset Formula $H_{0}$
\end_inset

: 
\begin_inset Formula $\bar{X}\sim N\left(\mu_{0},\sigma^{2}/n\right)$
\end_inset


\end_layout

\begin_layout Itemize
Under 
\begin_inset Formula $H_{a}$
\end_inset

: 
\begin_inset Formula $\bar{X}\sim N\left(\mu_{a},\sigma^{2}/n\right)$
\end_inset


\end_layout

\begin_layout Itemize
R code: 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

z <- qnorm(1-alpha)
\end_layout

\begin_layout Plain Layout

pnorm(mu0 + z*sigma/sqrt(n), mean=mua, sd=sigma/sqrt(n), lower.tail=false)
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
When testing the alternative hypothesis, notice that if power is 
\begin_inset Formula $1-\beta$
\end_inset

, then 
\begin_inset Formula $1-\beta=P\left(\bar{X}>\mu_{0}+z_{1-\alpha}\frac{\sigma}{\sqrt{n}};\mu=\mu_{a}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
where 
\begin_inset Formula $\bar{X}\sim N\left(\mu_{a},\sigma^{2}/n\right).$
\end_inset


\end_layout

\begin_layout Itemize
Unknowns: 
\begin_inset Formula $\mu_{a},\sigma,n,\beta$
\end_inset

.
\end_layout

\begin_layout Itemize
Knowns: 
\begin_inset Formula $\mu_{0},\alpha$
\end_inset

.
\end_layout

\begin_layout Itemize
Specify any 3 of the unknowns and you can solve for the remaining one.
\end_layout

\end_deeper
\begin_layout Itemize
Other notes
\end_layout

\begin_deeper
\begin_layout Itemize
The calculation for 
\begin_inset Formula $H_{a}:\mu<\mu_{0}$
\end_inset

 is similar to what we've already done.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $H_{a}:\mu\neq\mu_{0}$
\end_inset

, calculate the one-sided power using 
\begin_inset Formula $\alpha/2$
\end_inset

 (this is only approximately right, it excludes the possibility of getting
 a large t-statistic in the opposite direction of the truth, but this is
 only meaningful if 
\begin_inset Formula $\mu_{0}$
\end_inset

 and 
\begin_inset Formula $\mu_{a}$
\end_inset

 are close to each other).
\end_layout

\begin_layout Itemize
Power goes up as 
\begin_inset Formula $\alpha$
\end_inset

 gets larger.
\end_layout

\begin_layout Itemize
Power of a one-sided test is greater than the power of the associated two-sided
 test.
\end_layout

\begin_layout Itemize
Power goes up as 
\begin_inset Formula $\mu_{a}$
\end_inset

 gets further away from 
\begin_inset Formula $\mu_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Power goes up as 
\begin_inset Formula $n$
\end_inset

 goes up.
\end_layout

\begin_layout Itemize
Power doesn't need 
\begin_inset Formula $\mu_{a},\sigma,n$
\end_inset

; instead it only needs 
\begin_inset Formula $\frac{\sqrt{n}\left(\mu_{a}-\mu_{0}\right)}{\sigma}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The quantity 
\begin_inset Formula $\frac{\mu_{a}-\mu_{0}}{\sigma}$
\end_inset

 is called the effect size, the difference in the means in units of standard
 deviation.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
T-test power
\end_layout

\begin_deeper
\begin_layout Itemize
The power is 
\begin_inset Formula $P\left(\frac{\bar{X}-\mu_{0}}{S/\sqrt{n}}>t_{1-\alpha,n-1};\mu=\mu_{a}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculating this requires the non-central t-distribution.
\end_layout

\begin_layout Itemize

\family typewriter
power.t.test
\family default
 does this very well.
\end_layout

\begin_deeper
\begin_layout Itemize
If you omit one of the arguments, it will solve for it.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
T-test power example
\end_layout

\begin_deeper
\begin_layout Itemize
Calculating power:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

power.t.test(n=16, delta=2/4, sd=1, type="one.sample", alt="one.sided")$power
\end_layout

\begin_layout Plain Layout

power.t.test(n=16, delta=2, sd=4, type="one.sample", alt="one.sided")$power
\end_layout

\begin_layout Plain Layout

power.t.text(n=16, delta=100, sd=200, type="one.sample", alt="one.sided")$power
\end_layout

\begin_layout Plain Layout

## all examples give the same result
\end_layout

\begin_layout Plain Layout

## this is because they all have the same ratio of delta to sd.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Calculating sample size when we give power:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

power.t.test(power=0.8, delta=2/4, sd=1, type="one.sample", alt="one.sided")$n
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Multiple testing
\end_layout

\begin_layout Itemize
Key ideas:
\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis testing and significance analysis are commonly overuse.
\end_layout

\begin_layout Itemize
Correcting for multiple testing avoids false positives or discoveries.
\end_layout

\begin_deeper
\begin_layout Itemize
Example: do two tests for p-values on the same experiment but only report
 the smallest.
\end_layout

\end_deeper
\begin_layout Itemize
Two key components:
\end_layout

\begin_deeper
\begin_layout Itemize
Error measure
\end_layout

\begin_layout Itemize
Correction
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Main reason for multiple testing: lots of data!
\end_layout

\begin_layout Itemize
Why correct for multiple tests?
\end_layout

\begin_deeper
\begin_layout Itemize
A p-value of 0.05 doesn't mean much if you did 20 different tests!
\end_layout

\end_deeper
\begin_layout Itemize
Types of errors
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose you are testing a hypothesis that a parameter 
\begin_inset Formula $\beta$
\end_inset

 equals zero versus the alternative (not equal to zero).
\end_layout

\begin_layout Itemize
The possible outcomes are:
\end_layout

\begin_layout Itemize
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta=0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta\neq0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Hypotheses
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Claim 
\begin_inset Formula $\beta=0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
U
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
T
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $m-R$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Claim 
\begin_inset Formula $\beta\neq0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
V
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
S
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $R$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Claims
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $m_{0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $m-m_{0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $m$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Itemize
Type I error (false positive, V): say that the parameter is not equal to
 zero when it really is (false alarm rate).
\end_layout

\begin_layout Itemize
Type II error (false negative, T): say that the parameter is zero when it
 isn't (false dismissal rate).
\end_layout

\end_deeper
\begin_layout Itemize
Error rates:
\end_layout

\begin_deeper
\begin_layout Itemize
False positive rate: the rate at which false results (
\begin_inset Formula $\beta=0$
\end_inset

) are called significant.
 
\begin_inset Formula $E\left[\frac{V}{m_{0}}\right]$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is closely related to the type I error rate.
\end_layout

\end_deeper
\begin_layout Itemize
Family-wise error rate (FWER): probability of at least one false positive.
 
\begin_inset Formula $Pr\left(V\geq1\right)$
\end_inset


\end_layout

\begin_layout Itemize
False discovery rate (FDR): the rate at which claims of significance are
 false.
 
\begin_inset Formula $E\left[\frac{V}{R}\right]$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Controlling the false positive rate
\end_layout

\begin_deeper
\begin_layout Itemize
If p-values are correctly calculated, calling all 
\begin_inset Formula $P<\alpha$
\end_inset

 significant will control the false positive rate at level 
\begin_inset Formula $\alpha$
\end_inset

 on average.
\end_layout

\begin_layout Itemize
Problem: suppose that you perform 10000 tests and 
\begin_inset Formula $\beta=0$
\end_inset

 for all of them.
\end_layout

\begin_deeper
\begin_layout Itemize
If you call 
\begin_inset Formula $P<0.05$
\end_inset

 significant, the expected number of false positives is 500.
\end_layout

\begin_layout Itemize
How do we avoid so many false positives?
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Controlling the family-wise error rate
\end_layout

\begin_deeper
\begin_layout Itemize
The Bonferroni correction is the oldest multiple testing correction.
\end_layout

\begin_layout Itemize
Basic idea:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose you do 
\begin_inset Formula $m$
\end_inset

 tests.
\end_layout

\begin_layout Itemize
You want to control FWER at level 
\begin_inset Formula $\alpha$
\end_inset

 such that 
\begin_inset Formula $Pr\left(V\geq1\right)<\alpha$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculate p-values normally.
\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $\alpha_{FWER}=\alpha/m$
\end_inset

 and call all p-values less than 
\begin_inset Formula $\alpha_{FWER}$
\end_inset

 significant.
\end_layout

\end_deeper
\begin_layout Itemize
Pros: easy to calculate, conservative.
\end_layout

\begin_layout Itemize
Cons: may be very conservative!
\end_layout

\end_deeper
\begin_layout Itemize
Controlling the false discovery rate
\end_layout

\begin_deeper
\begin_layout Itemize
This is the most popular correction when performing lots of tests.
\end_layout

\begin_layout Itemize
Basic idea:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose you do 
\begin_inset Formula $m$
\end_inset

 tests.
\end_layout

\begin_layout Itemize
You want to control FDR at level 
\begin_inset Formula $\alpha$
\end_inset

 so 
\begin_inset Formula $E\left[\frac{V}{R}\right]$
\end_inset


\end_layout

\begin_layout Itemize
Calculate p-values normally.
\end_layout

\begin_layout Itemize
Order the p-values from smallest to largest.
\end_layout

\begin_layout Itemize
Call any 
\begin_inset Formula $p_{i}\leq\alpha\frac{i}{m}$
\end_inset

 significant.
\end_layout

\end_deeper
\begin_layout Itemize
Pros: easy to calculate, less conservative.
\end_layout

\begin_layout Itemize
Cons: allows for more false positives, may behave strangely under dependence.
\end_layout

\end_deeper
\begin_layout Itemize
Adjusted p-values
\end_layout

\begin_deeper
\begin_layout Itemize
One approach is to adjust the threshold 
\begin_inset Formula $\alpha$
\end_inset

, but another approach is to calculate 
\begin_inset Quotes eld
\end_inset

adjusted p-values.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In this case, they are not technically p-values any more, so they don't
 have the same properties of classically defined p-values.
\end_layout

\begin_layout Itemize
But they can be used to control error parameters directly without adjust
 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Example:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose p-values are 
\begin_inset Formula $p_{1},...,p_{m}$
\end_inset

.
\end_layout

\begin_layout Itemize
You could adjust them by taking 
\begin_inset Formula $p_{i}^{FWER}=max\left(m\cdot p_{i},1\right)$
\end_inset

, for each p-value.
\end_layout

\begin_layout Itemize
Then if you call all 
\begin_inset Formula $p_{i}^{FWER}<\alpha$
\end_inset

 significant, you will control the FWER.
\end_layout

\end_deeper
\begin_layout Itemize
R examples:
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
p.adjust(pValues, method=
\begin_inset Quotes erd
\end_inset

Bonferroni
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Itemize

\family typewriter
p.adjust(pValues, method=
\begin_inset Quotes erd
\end_inset

BH
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Note: if there is strong dependence between different tests, there may be
 problems.
\end_layout

\begin_deeper
\begin_layout Itemize
Can try 
\family typewriter
method=
\begin_inset Quotes erd
\end_inset

BY
\begin_inset Quotes erd
\end_inset


\family default
 if this is the case.
\end_layout

\end_deeper
\begin_layout Subsection*
Bootstrapping
\end_layout

\begin_layout Itemize
The bootstrap is a tremendously useful tool for constructing confidence
 intervals and calculating standard errors for difficult statistics.
\end_layout

\begin_layout Itemize
Example: how would you derive a confidence interval for the median?
\end_layout

\begin_deeper
\begin_layout Itemize
Can do complicated math, but a bootstrap is an easier solution.
\end_layout

\end_deeper
\begin_layout Itemize
Example: rolling a dice 50 times and calculating the average roll.
\end_layout

\begin_deeper
\begin_layout Itemize
What if we only have one sample (of 50 rolls)?
\end_layout

\begin_layout Itemize
How can we get a distribution of averages for 50 rolls if we only have one
 sample of 50 rolls?
\end_layout

\begin_layout Itemize
Bootstrapping says that we should take our one sample of 50 rolls and use
 the distribution of single dice rolls to generate a population of averages
 of 50 rolls.
\end_layout

\begin_deeper
\begin_layout Itemize
Using our observed data to construct an 
\series bold
estimated
\series default
 population distribution.
\end_layout

\begin_layout Itemize
Using that population distribution to simulate the statistic we are interested
 in.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The bootstrap principle
\end_layout

\begin_deeper
\begin_layout Itemize
If you have a statistic that estimates some population parameter, but don't
 know its sampling distribution, then you can use the distribution defined
 by the data to approximate its sampling distribution.
\end_layout

\end_deeper
\begin_layout Itemize
The bootstrap in practice:
\end_layout

\begin_deeper
\begin_layout Itemize
Always carried out using simulation.
\end_layout

\begin_layout Itemize
Procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
Simulate complete data sets from the observed data (with replacement).
\end_layout

\begin_deeper
\begin_layout Itemize
This is approximately drawing from the sampling distribution of that statistic,
 at least as far as the data is able to approximate the true population
 distribution.
\end_layout

\end_deeper
\begin_layout Itemize
Calculate the statistic for each simulated data set.
\end_layout

\begin_layout Itemize
Use the simulated statistics to either define a confidence interval or take
 the standard deviation to calculate a standard error.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example: calculating confidence interval for the median of a data set of
 
\begin_inset Formula $n$
\end_inset

 observations.
\end_layout

\begin_deeper
\begin_layout Itemize
Sample 
\begin_inset Formula $n$
\end_inset

 observations 
\series bold
with replacement
\series default
 from the observed data resulting in one simulated complete data set.
\end_layout

\begin_layout Itemize
Take the median of the simulated data set.
\end_layout

\begin_layout Itemize
Repeat these steps 
\begin_inset Formula $B$
\end_inset

 times, resulting in 
\begin_inset Formula $B$
\end_inset

 simulated medians.
 (
\begin_inset Formula $B$
\end_inset

 should be large.)
\end_layout

\begin_layout Itemize
These medians are approximately drawn from the sampling distribution of
 the median of 
\begin_inset Formula $n$
\end_inset

 observations; therefore we can:
\end_layout

\begin_deeper
\begin_layout Itemize
Make a histogram of them.
\end_layout

\begin_layout Itemize
Calculate their standard deviation to estimate the standard error of the
 median.
\end_layout

\begin_layout Itemize
Take the 2.5th and 97.5th percentiles as a confidence interval for the median.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example code:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

library(UsingR)
\end_layout

\begin_layout Plain Layout

data(father.son)
\end_layout

\begin_layout Plain Layout

x <- father.son$sheight
\end_layout

\begin_layout Plain Layout

n <- length(x)
\end_layout

\begin_layout Plain Layout

B <- 10000
\end_layout

\begin_layout Plain Layout

## Make a matrix where each row is a sample with n observations.
\end_layout

\begin_layout Plain Layout

resamples <- matrix(sample(x, n*B, replace=TRUE), B, n)
\end_layout

\begin_layout Plain Layout

## Take the median of each row.
\end_layout

\begin_layout Plain Layout

medians <- apply(resamples, 1, median)
\end_layout

\begin_layout Plain Layout

## Estimated standard error on the median.
\end_layout

\begin_layout Plain Layout

sd(medians)
\end_layout

\begin_layout Plain Layout

## Estimate a confidence interval for the median.
\end_layout

\begin_layout Plain Layout

quantile(medians, c(0.025, 0.975))
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The bootstrap is non-parametric: it makes no assumptions about the probability
 distributions of the variables being assessed.
\end_layout

\begin_layout Itemize
Better percentile bootstrap confidence intervals correct for bias.
\end_layout

\begin_deeper
\begin_layout Itemize
Use the 
\begin_inset Quotes eld
\end_inset

BCA
\begin_inset Quotes erd
\end_inset

 interval instead.
 (???)
\end_layout

\end_deeper
\begin_layout Subsection*
Permutation tests
\end_layout

\begin_layout Itemize
Used for group comparisons.
\end_layout

\begin_layout Itemize
Permutation tests are very powerful and there are several variations:
\end_layout

\begin_deeper
\begin_layout Itemize
Rank sum test, Fisher's exact test, etc.
\end_layout

\end_deeper
\begin_layout Itemize
Permutation tests work very well in multivariate settings.
\end_layout

\begin_layout Itemize
Example: consider comparing two independent groups using InsectSprays data
 set.
\end_layout

\begin_deeper
\begin_layout Itemize
Consider the null hypothesis - that the distribution of the observations
 from each group is the same.
\end_layout

\begin_layout Itemize
Consider a data frame with count and spray.
\end_layout

\begin_layout Itemize
Permute the spray (group) labels and recalculate the statistic.
\end_layout

\begin_deeper
\begin_layout Itemize
Mean difference in counts.
\end_layout

\begin_layout Itemize
Geometric means.
\end_layout

\begin_layout Itemize
T-statistic.
\end_layout

\end_deeper
\begin_layout Itemize
Calculate the percentage of simulations where the simulated statistic was
 more extreme (toward the alternative) than the observed.
\end_layout

\begin_layout Itemize
This yields a permutation-based p-value.
\end_layout

\end_deeper
\begin_layout Itemize
Code example:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

subdata <- InsectSprays[InsectSprays$spray %in% c("B","C"),]
\end_layout

\begin_layout Plain Layout

y <- subdata$count
\end_layout

\begin_layout Plain Layout

group <- as.character(subdata$spray)
\end_layout

\begin_layout Plain Layout

testStat <- function(w, g) mean(w[g=="B"])- mean(w[g=="C"])
\end_layout

\begin_layout Plain Layout

observedStat <- testStat(y, group)
\end_layout

\begin_layout Plain Layout

permutations <- sapply(1:10000, function(i) testStat(y, sample(group)))
\end_layout

\begin_layout Plain Layout

observedStat
\end_layout

\begin_layout Plain Layout

## [1] 13.25
\end_layout

\begin_layout Plain Layout

mean(permutations > observedStat)
\end_layout

\begin_layout Plain Layout

## [1] 0
\end_layout

\begin_layout Plain Layout

## Using 10000 permutations, we couldn't find a reconfiguration of the group
 labels
\end_layout

\begin_layout Plain Layout

## that led to such an extreme difference.
\end_layout

\end_inset


\end_layout

\end_body
\end_document
