#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Regression Models - Notes
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 6/1/2015 - 6/28/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Subsection*

\series bold
Introduction
\end_layout

\begin_layout Itemize
Questions we may want to answer for this class (considering an example of
 children's heights and their parents' heights):
\end_layout

\begin_deeper
\begin_layout Itemize
Use the parents' heights to predict children's heights.
\end_layout

\begin_layout Itemize
To try to find an easily described mean relationship between parent and
 children heights.
\end_layout

\begin_layout Itemize
To investigate the variation in children's heights that appears unrelated
 to parents' heights (residual variation).
\end_layout

\begin_layout Itemize
To quantify what impact genotype information has beyond parental height
 in explaining child height.
\end_layout

\begin_layout Itemize
To figure out how and what assumptions are needed to generalize findings
 beyond the data in question.
\end_layout

\begin_layout Itemize
Why do children of very tall parents tend to be tall, but a little shorter
 than their parents, and why do children of very short parents tend to be
 short, but a little taller than their parents? (regression to the mean)
\end_layout

\end_deeper
\begin_layout Subsection*
Background and notation
\end_layout

\begin_layout Itemize
Define the empirical (sample) mean as: 
\begin_inset Formula $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
If we subtract the mean from all data points, we are 
\series bold
centering
\series default
 the random variables: 
\begin_inset Formula $\tilde{X}_{i}=X_{i}-\bar{X}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The mean of 
\begin_inset Formula $\tilde{X}_{i}$
\end_inset

 is zero.
\end_layout

\end_deeper
\begin_layout Itemize
The empirical (sample) variance is: 
\begin_inset Formula $S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\frac{1}{n-1}$
\end_inset


\begin_inset Formula $\left(\sum_{i=1}^{n}X_{i}^{2}-n\bar{X}^{2}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The data defined by 
\begin_inset Formula $X_{i}/S$
\end_inset

 have empirical standard deviation 1.
 This is called 
\begin_inset Quotes eld
\end_inset

scaling
\begin_inset Quotes erd
\end_inset

 the data.
\end_layout

\end_deeper
\begin_layout Itemize
Normalization: centering and scaling the data to have empirical mean 0 and
 empirical standard deviation 1.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Z_{i}=\frac{X_{i}-\bar{X}}{S}$
\end_inset


\end_layout

\begin_layout Itemize
Normalized data have units equal to standard deviations of the original
 data.
\end_layout

\end_deeper
\begin_layout Itemize
Empirical covariance
\end_layout

\begin_deeper
\begin_layout Itemize
Consider a pair of data 
\begin_inset Formula $\left(X_{i},Y_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Their covariance is 
\begin_inset Formula $Cov\left(X,Y\right)=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)=\frac{1}{n-1}\left(\sum_{i=1}^{n}X_{i}Y_{i}-n\bar{X}\bar{Y}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Their correlation is defined as 
\begin_inset Formula $Cor\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{S_{x}S_{y}}$
\end_inset

, where 
\begin_inset Formula $S_{i}$
\end_inset

 is the estimate of the standard deviation for variable 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)=Cor\left(Y,X\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $-1\leq Cor\left(X,Y\right)\leq1$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)=1$
\end_inset

 and 
\begin_inset Formula $Cor\left(X,Y\right)=-1$
\end_inset

 only when the observations fall perfectly on a positively or negatively
 sloped line, respectively.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)$
\end_inset

 measures the strength of the linear relationship between the 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 data, with stronger relationships as 
\begin_inset Formula $Cor\left(X,Y\right)$
\end_inset

 goes to -1 or +1.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)$
\end_inset

 implies no linear relationship.
\end_layout

\end_deeper
\begin_layout Subsection*
Basic least squares
\end_layout

\begin_layout Itemize
Example using children's and parents' heights.
\end_layout

\begin_layout Itemize
Consider only the children's heights - how could one describe the 
\begin_inset Quotes eld
\end_inset

middle
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Itemize
One definition: let 
\begin_inset Formula $Y_{i}$
\end_inset

 be the height of child 
\begin_inset Formula $i$
\end_inset

, for 
\begin_inset Formula $i=1,..,n$
\end_inset

 , then define the middle as the value of 
\begin_inset Formula $\mu$
\end_inset

 that minimizes 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-\mu\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
This is the physical 
\begin_inset Quotes eld
\end_inset

center of mass
\begin_inset Quotes erd
\end_inset

 of the histogram.
\end_layout

\begin_layout Itemize
The end result is 
\begin_inset Formula $\mu=\bar{Y}$
\end_inset

, the mean (and we can do a proof of this).
\end_layout

\end_deeper
\begin_layout Itemize
Regression through the origin:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose the 
\begin_inset Formula $X_{i}$
\end_inset

 are the parents' heights.
\end_layout

\begin_layout Itemize
Consider picking the slope 
\begin_inset Formula $\beta$
\end_inset

 that minimizes 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-X_{i}\beta\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta=\frac{\sum_{i=1}^{n}X_{i}Y_{i}}{\sum_{i=1}^{n}X_{i}^{2}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Linear least squares
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $Y_{i}$
\end_inset

 be the 
\begin_inset Formula $i$
\end_inset

th child's height and 
\begin_inset Formula $X_{i}$
\end_inset

 be the 
\begin_inset Formula $i$
\end_inset

th (average over the pair of) parents' heights.
\end_layout

\begin_layout Itemize
Consider finding the 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 line: child's height = 
\begin_inset Formula $\beta_{0}$
\end_inset

 + (parent's height)*
\begin_inset Formula $\beta_{1}$
\end_inset


\end_layout

\begin_layout Itemize
Use least squares:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\sum_{i=1}^{n}\left[Y_{i}-\left(\beta_{0}+\beta_{1}X_{i}\right)\right]^{2}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}=Cor\left(Y,X\right)\frac{S_{y}}{S_{x}}$
\end_inset

 (hat indicates that it is an estimator)
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}$
\end_inset

 has units of 
\begin_inset Formula $Y/X$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{0}=\bar{Y}-\hat{\beta}_{1}\bar{X}$
\end_inset

 (hat indicates that it is an estimator)
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{0}$
\end_inset

 has units of 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
The line passes through the point 
\begin_inset Formula $\left(\bar{X},\bar{Y}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The slope is the same as what you would get if you centered the data and
 did regression through the origin.
\end_layout

\begin_layout Itemize
If you normalized the data, the slope is 
\begin_inset Formula $Cor\left(Y,X\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection*
Regression to the mean
\end_layout

\begin_layout Itemize
Examples:
\end_layout

\begin_deeper
\begin_layout Itemize
Why are children of tall parents tall, but usually not as tall as their
 parents?
\end_layout

\begin_layout Itemize
Why do the best-performing athletes from last year usually not perform quite
 as well this year?
\end_layout

\end_deeper
\begin_layout Itemize
Imagine simulated pairs of random normal variables.
\end_layout

\begin_deeper
\begin_layout Itemize
The largest first ones would be the largest by chance, and the probability
 that there are smaller ones for the second simulation is high.
\end_layout

\begin_layout Itemize
In other words, 
\begin_inset Formula $P\left(Y<x|X=X\right)$
\end_inset

 gets bigger as 
\begin_inset Formula $x$
\end_inset

 heads into the very large values.
\end_layout

\begin_layout Itemize
Similarly, 
\begin_inset Formula $P\left(Y>x|X=x\right)$
\end_inset

 gets bigger as 
\begin_inset Formula $x$
\end_inset

 heads to very small values.
\end_layout

\end_deeper
\begin_layout Itemize
Suppose that we normalize 
\begin_inset Formula $X$
\end_inset

 (child's height) and 
\begin_inset Formula $Y$
\end_inset

 (parent's height) so that they both have mean 0 and variance 1.
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that our regression line will pass through 
\begin_inset Formula $\left(0,0\right)$
\end_inset

, the mean of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Itemize
The slope of the gression line is 
\begin_inset Formula $Cor\left(Y,X\right)$
\end_inset

, regardless of which variable is the outcome.
\end_layout

\end_deeper
\begin_layout Subsection*
Statistical linear regression models
\end_layout

\begin_layout Itemize
Basic regression model with additive Gaussian errors:
\end_layout

\begin_deeper
\begin_layout Itemize
Least squares is an estimation tool, how do we do inference?
\end_layout

\begin_layout Itemize
Consider developing a probabilistic model for linear regression: 
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\epsilon_{i}$
\end_inset

 are random Gaussian errors, which are assumed to be IID and drawn from
 
\begin_inset Formula $N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Note: 
\begin_inset Formula $E\left[Y_{i}|X_{i}=x_{i}\right]=\mu_{i}=\beta_{0}+\beta_{1}x_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Note: 
\begin_inset Formula $Var\left(Y_{i}|X_{i}=x_{i}\right)=\sigma^{2}$
\end_inset

.
 This is the variance around the regression line.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Interpreting regression coefficients:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\beta_{0}$
\end_inset

 is the expected value of the response when the predictor is 0.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[Y|X=0\right]=\beta_{0}$
\end_inset


\end_layout

\begin_layout Itemize
Note: this isn't always of interest, for example when 
\begin_inset Formula $X=0$
\end_inset

 is far outside the range of the data, or physically impossible (X is height,
 blood pressure, etc.).
\end_layout

\begin_layout Itemize
However, you can shift your 
\begin_inset Formula $X$
\end_inset

 values by some factor 
\begin_inset Formula $a$
\end_inset

; this will change the intercept, but not the slope.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}=\beta_{0}+a\beta_{1}+\beta_{1}\left(X_{i}-a\right)+\epsilon_{i}=\tilde{\beta}_{0}+\beta_{1}\left(X_{i}-a\right)+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Often, 
\begin_inset Formula $a$
\end_inset

 is set to 
\begin_inset Formula $\bar{X}$
\end_inset

 so that the intercept is interpreted as the expected response at the average
 
\begin_inset Formula $X$
\end_inset

 value.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
\begin_inset Formula $\beta_{1}$
\end_inset

 is the expected change in response for a 1 unit change in the predictor.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[Y|X=x+1\right]-E\left[Y|X=x\right]=\beta_{0}+\beta_{1}\left(x+1\right)-\left(\beta_{0}+\beta_{1}x\right)=\beta_{1}$
\end_inset


\end_layout

\begin_layout Itemize
Consider the impact of changing the units of 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}=\beta_{0}+\frac{\beta_{1}}{a}\left(X_{i}a\right)+\epsilon_{i}=\beta_{0}+\tilde{\beta}_{1}\left(X_{i}a\right)+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Multiplication of 
\begin_inset Formula $X$
\end_inset

 by a factor 
\begin_inset Formula $a$
\end_inset

 results in dividing the slope by a factor of 
\begin_inset Formula $a$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Subsection*
R code for regression
\end_layout

\begin_layout Itemize
Linear regression: 
\family typewriter
fit <- lm(outcome ~ predictor, data = name_of_data_frame)
\end_layout

\begin_deeper
\begin_layout Itemize
To just get the coefficients: 
\family typewriter
coef(fit)
\end_layout

\end_deeper
\begin_layout Itemize
Plotting a fit in ggplot: 
\family typewriter
g + geom_smooth(method=
\begin_inset Quotes eld
\end_inset

lm
\begin_inset Quotes erd
\end_inset

, color=
\begin_inset Quotes eld
\end_inset

black
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Itemize
If you want to do arithmetic operations inside the 
\family typewriter
lm
\family default
 function, use the 
\family typewriter
I()
\family default
 function:
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
fit2 <- lm(outcome ~ I(predictor - mean(predictor)), data=name_of_data_frame)
\end_layout

\end_deeper
\begin_layout Itemize
Using a fit to do a prediction: 
\family typewriter
predict(fit, newdata=data.frame(carat=newx))
\end_layout

\begin_layout Subsection*
Residuals
\end_layout

\begin_layout Itemize
Examples here are using the 
\family typewriter
diamond
\family default
 dataset from 
\family typewriter
UsingR
\family default
.
\end_layout

\begin_layout Itemize
Variation around a regression line is called 
\series bold
residual variation
\series default
.
\end_layout

\begin_deeper
\begin_layout Itemize
Residuals are just the difference between the actual data point values and
 those predicted by a regression line.
\end_layout

\end_deeper
\begin_layout Itemize
Model 
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}$
\end_inset

, where 
\begin_inset Formula $\epsilon_{i}\sim N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Observed outcome at predictor value 
\begin_inset Formula $X_{i}$
\end_inset

 is 
\begin_inset Formula $Y_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Predicted outcome is 
\begin_inset Formula $\hat{Y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}X_{i}$
\end_inset


\end_layout

\begin_layout Itemize
The residual is the difference between the observed and predicted outcomes:
 
\begin_inset Formula $e_{i}=Y_{i}-\hat{Y}_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Least squares minimizes 
\begin_inset Formula $\sum_{i=1}^{n}e_{i}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $e_{i}$
\end_inset

 can be thought of as estimates of the 
\begin_inset Formula $\epsilon_{i}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Properties of the residuals
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[e_{i}\right]=0$
\end_inset

.
\end_layout

\begin_layout Itemize
If an intercept is included, 
\begin_inset Formula $\sum_{i=1}^{n}e_{i}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
If a regressor variable, 
\begin_inset Formula $X_{i}$
\end_inset

, is included in the model, 
\begin_inset Formula $\sum_{i=1}^{n}e_{i}X_{i}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
Residuals can be thought of as the outcome 
\begin_inset Formula $Y$
\end_inset

, with the linear association of the predictor 
\begin_inset Formula $X$
\end_inset

 removed.
\end_layout

\begin_layout Itemize
There is a difference between 
\series bold
residual variation
\series default
 (variation after removing the predictor) from 
\series bold
systematic variation
\series default
 (variation explained by the regression model).
\end_layout

\end_deeper
\begin_layout Itemize
To get residuals in R, use 
\family typewriter
resid(fit)
\family default
, where 
\family typewriter
fit
\family default
 is the result of a regression function like 
\family typewriter
lm
\family default
.
\end_layout

\begin_layout Itemize
When you plot residuals, you shouldn't see any type of pattern, they should
 look mostly random.
 If there is a pattern, there may be some systematic error in your analysis
 or the assumptions you have made.
\end_layout

\begin_layout Itemize
Estimating residual variation:
\end_layout

\begin_deeper
\begin_layout Itemize
The maximum likelihood estimate of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}$
\end_inset

, the average squared residual.
\end_layout

\begin_layout Itemize
Most people use 
\begin_inset Formula $\hat{\sigma}^{2}=\frac{1}{n-2}\sum_{i=1}^{n}e_{i}^{2}$
\end_inset

 (you lose two degrees of freedom since you have an intercept and a slope
 in your fit).
\end_layout

\begin_deeper
\begin_layout Itemize
The 
\begin_inset Formula $n-2$
\end_inset

 instead of 
\begin_inset Formula $n$
\end_inset

 is so that 
\begin_inset Formula $E\left[\hat{\sigma}^{2}\right]=\sigma^{2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
\begin_inset Formula $R^{2}=\frac{\sum_{i=1}^{n}\left(\hat{Y}_{i}-\bar{Y}\right)^{2}}{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Percentage of total variability that is explained by the linear relationship
 with the predictor.
\end_layout

\begin_layout Itemize
\begin_inset Formula $0\leq R^{2}\leq1$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $R^{2}$
\end_inset

 is the sample correlation squared.
\end_layout

\begin_layout Itemize
\begin_inset Formula $R^{2}$
\end_inset

 can be a misleading summary of model fit.
\end_layout

\begin_deeper
\begin_layout Itemize
Deleting data can inflate 
\begin_inset Formula $R^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Adding terms to a regression model always increases 
\begin_inset Formula $R^{2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Inference in regression
\end_layout

\begin_layout Itemize
Our model is 
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}$
\end_inset

, where 
\begin_inset Formula $\epsilon\sim N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
We assume that the true model is known.
\end_layout

\end_deeper
\begin_layout Itemize
Statistics like 
\begin_inset Formula $\frac{\hat{\theta}-\theta}{\hat{\sigma}_{\theta}}$
\end_inset

 have the following properties:
\end_layout

\begin_deeper
\begin_layout Itemize
Normally distributed and have a Student's T distribution if the estimated
 variance is replaced with a sample estimate (under normality assumptions).
\end_layout

\begin_layout Itemize
Can be used to test 
\begin_inset Formula $H_{0}:\theta=\theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{a}:\theta>,<,\neq\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Can be used to create a confidence interval for 
\begin_inset Formula $\theta$
\end_inset

 via 
\begin_inset Formula $\hat{\theta}\pm Q_{1-\alpha/2}\hat{\sigma}_{\theta}$
\end_inset

, where 
\begin_inset Formula $Q_{1-\alpha/2}$
\end_inset

 is the relevant quantile from either a normal or a T distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
I think you use 
\begin_inset Formula $n-2$
\end_inset

 degrees of freqedom for a T distribution in this case.
 Not totally sure, though.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
In the case of regression with IID sampling assumption and normal errors,
 our inferences will look similar to what we covered in the statistical
 inference class.
\end_layout

\begin_layout Itemize
Variance of the regression slope:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma_{\hat{\beta}_{1}}^{2}=Var\left(\hat{\beta}_{1}\right)=\sigma^{2}/\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
Variance of the intercept:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma_{\hat{\beta}_{0}}^{2}=Var\left(\beta_{0}\right)=\left(\frac{1}{n}+\frac{\bar{X}^{2}}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}\right)\sigma^{2}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
In practice, 
\begin_inset Formula $\sigma$
\end_inset

 can be replaced by its estimate in the above two equations (
\begin_inset Formula $\frac{\sum_{i=1}^{n}e_{i}^{2}}{n-2}$
\end_inset

).
\end_layout

\begin_layout Itemize
It's probably not surprising that under IID Gaussian errors, 
\begin_inset Formula $\frac{\hat{\beta}_{j}-\beta_{j}}{\hat{\sigma}_{\hat{\beta}_{j}}}$
\end_inset

 follows a T distribution with 
\begin_inset Formula $n-2$
\end_inset

 degrees of freedom and a normal distribution for large 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This can be used to create confidence intervals and perform hypothesis tests.
\end_layout

\end_deeper
\begin_layout Itemize
Prediction of outcomes:
\end_layout

\begin_deeper
\begin_layout Itemize
Consider predicting 
\begin_inset Formula $Y$
\end_inset

 at a value 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Itemize
The obvious estimate for a prediction at point 
\begin_inset Formula $x_{0}$
\end_inset

 is 
\begin_inset Formula $\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
A standard error is needed to create a prediction interval.
\end_layout

\begin_layout Itemize
There is a distinction between intervals for the regression line at a particular
 point 
\begin_inset Formula $x_{0}$
\end_inset

 and intervals for the prediction of a 
\begin_inset Formula $y$
\end_inset

 value at a point 
\begin_inset Formula $x_{0}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Line at 
\begin_inset Formula $x_{0}$
\end_inset

 standard error: 
\begin_inset Formula $\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\left(x_{0}-\bar{X}\right)^{2}}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is usually referred to as a confidence interval.
\end_layout

\begin_layout Itemize
R code: 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cr <- sigma*sqrt(1/n+(x0-mean(x))^2/ssx)
\end_layout

\begin_layout Plain Layout

y0 + c(-1,1)*qt(0.975,df=n-2)*cr
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Prediction interval standard error at 
\begin_inset Formula $x_{0}$
\end_inset

: 
\begin_inset Formula $\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{X}\right)^{2}}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is usually referred to as a prediction interval.
\end_layout

\begin_layout Itemize
R code:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

pr <- sigma*sqrt(1+1/n+(x0-mean(x))^2/ssx)
\end_layout

\begin_layout Plain Layout

y0 + c(-1,1)*qt(0.975,df=n-2)*pr
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Code example
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

library(UsingR); data(diamond)
\end_layout

\begin_layout Plain Layout

y <- diamond$price; x <- diamond$carat; n <- length(y)
\end_layout

\begin_layout Plain Layout

beta1 <- cor(x,y)*sd(y)/sd(x);
\end_layout

\begin_layout Plain Layout

beta0 <- mean(y) - beta1*mean(x);
\end_layout

\begin_layout Plain Layout

e <- y - beta0 - beta1*x;
\end_layout

\begin_layout Plain Layout

sigma <- sqrt(sum(e^2)/(n-2))
\end_layout

\begin_layout Plain Layout

ssx <- sum((x-mean(x))^2) ## this is also equal to var(x)*(n-1)
\end_layout

\begin_layout Plain Layout

seBeta0 <- sqrt(1/n + mean(x)^2/ssx)*sigma
\end_layout

\begin_layout Plain Layout

seBeta1 <- sigma/sqrt(ssx)
\end_layout

\begin_layout Plain Layout

tBeta0 <- beta0/seBeta0;
\end_layout

\begin_layout Plain Layout

tBeta1 <- beta1/seBeta1;
\end_layout

\begin_layout Plain Layout

pBeta0 <- 2*pt(abs(tBeta0), df=n-2, lower.tail=FALSE)
\end_layout

\begin_layout Plain Layout

pBeta1 <- 2*pt(abs(tBeta1), df=n-2, lower.tail=FALSE)
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Multivariable regression
\end_layout

\begin_layout Itemize
Sometimes there are several variables which may affect a predictor, or hidden
 variables affecting the outcome.
\end_layout

\begin_layout Itemize
Multivariable analyses attempt to account for other variables that may explain
 a relationship.
\end_layout

\begin_layout Itemize
Example:
\end_layout

\begin_deeper
\begin_layout Itemize
An insurance company is interested in how last year's claims can predict
 a person's time in the hospital this year.
\end_layout

\begin_layout Itemize
They want to use an enormous amount of data contained in claims to predict
 a single number.
\end_layout

\begin_layout Itemize
Simple linear regression is not equipped to handle more than one predictor.
\end_layout

\begin_layout Itemize
How can one generalize simple linear regression to incorporate lots of regressor
s for the purpose of prediction?
\end_layout

\begin_layout Itemize
What are the consequences of adding lots of regressors?
\end_layout

\begin_deeper
\begin_layout Itemize
There must be consequences to adding variables that are unrelated to the
 outcome.
\end_layout

\begin_layout Itemize
There must be consequences to omitting variables that are related to the
 outcome.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The linear model for multivariate regression:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{1}X_{1i}+\beta_{2}X_{2i}+...+\beta_{p}X_{pi}+\epsilon_{i}=\sum_{k=1}^{p}X_{ki}\beta_{k}+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Here, 
\begin_inset Formula $X_{1i}$
\end_inset

 is typically 1 so that an intercept is included.
\end_layout

\begin_layout Itemize
Least squares (and hence ML estimates, under IID Gaussianity of the errors)
 minimizes 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-\sum_{k=1}^{p}X_{ki}\beta_{k}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Note: the important linearity here is the linearity in the coefficients!
\end_layout

\begin_deeper
\begin_layout Itemize
Example: 
\begin_inset Formula $Y_{i}=\beta_{1}X_{1i}^{2}+...\beta_{p}X_{pi}^{2}+\epsilon_{i}$
\end_inset

 is still a linear model, we've just squared the elements of the predictor
 variables.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
How to get estimates
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that the least squares estimate for regression through the origin,
 
\begin_inset Formula $E\left[Y_{i}\right]=X_{1i}\beta_{1}$
\end_inset

 was 
\begin_inset Formula $\sum_{i}X_{i}Y_{i}/\sum_{i}X_{i}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Let's consider two regressors, 
\begin_inset Formula $E\left[Y_{i}\right]=X_{1i}\beta_{1}+X_{2i}\beta_{2}=\mu_{i}$
\end_inset

 .
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that if 
\begin_inset Formula $\hat{\mu}_{i}$
\end_inset

 satisfies 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-\hat{\mu}_{i}\right)\left(\hat{\mu}_{i}-\mu_{i}\right)=0$
\end_inset

, then we've found the least squares estimates.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}e_{i,Y|X_{2}}e_{i,X_{1}|X_{2}}}{\sum_{i=1}^{n}e_{i,x_{1}|X_{2}}^{2}}$
\end_inset

: the regression esimate for 
\begin_inset Formula $\beta_{1}$
\end_inset

 is the regression through the origin estimate, having regressed 
\begin_inset Formula $X_{2}$
\end_inset

 out of both the response and the predictor.
\end_layout

\begin_layout Itemize
Similarly, the regression estimate for 
\begin_inset Formula $\beta_{2}$
\end_inset

 is the regression through the origin estimate, having regressed 
\begin_inset Formula $X_{1}$
\end_inset

 out of both the response and the predictor.
\end_layout

\end_deeper
\begin_layout Itemize
Generally, multivariate regression estimates are exactly those having removed
 the linear relationship of the other variables from both the regressor
 and the response.
\end_layout

\end_deeper
\begin_layout Itemize
Example with two variables, simple linear regression
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{1}X_{1i}+\beta_{2}X_{2i}$
\end_inset

, where 
\begin_inset Formula $X_{2i}=1$
\end_inset

 is an intercept term.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $\frac{\sum_{j}X_{2j}X_{1j}}{\sum_{j}X_{2j}^{2}}X_{2i}=\frac{\sum_{j}X_{1j}}{n}=\bar{X}_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
Thus, 
\begin_inset Formula $e_{i,X_{1}|X_{2}}=X_{1i}-\bar{X}_{1}$
\end_inset

 and 
\begin_inset Formula $e_{i,Y|X_{2}}=Y_{i}-\bar{Y}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}e_{i,Y|X_{2}}e_{i,X_{1}|X_{2}}}{\sum_{i=1}^{n}e_{i,X_{1}|X_{2}}^{2}}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}=Cor\left(X,Y\right)\frac{Sd\left(Y\right)}{Sd\left(X\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Extending to the general case:
\end_layout

\begin_deeper
\begin_layout Itemize
Least square solution to minimize: 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-X_{1i}\beta_{1}-...-X_{pi}\beta_{p}\right)^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Solving this solution yields the least squares estimates.
\end_layout

\begin_layout Itemize
Obtaining a good, fast, and general solution usually requires linear algebra.
\end_layout

\end_deeper
\begin_layout Itemize
The least squares estimate for the coefficient of a multivariate regression
 model is exactly regression through the origin with the linear relationship
 with the other regressors removed from both the regressor and outcome by
 taking residuals.
\end_layout

\begin_layout Itemize
In this sense, multivariate regression 
\begin_inset Quotes eld
\end_inset

adjusts
\begin_inset Quotes erd
\end_inset

 a coefficient for the linear impact of the other variables.
\end_layout

\end_deeper
\begin_layout Itemize
Interpretation of the coefficients
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[Y|X_{1}=x_{1},...,X_{p}=x_{p}\right]=\sum_{k=1}^{p}x_{k}\beta_{k}$
\end_inset


\end_layout

\begin_layout Itemize
What if one variable 
\begin_inset Formula $x_{1}$
\end_inset

 is incremented by one?
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[Y|X_{1}=x_{1}+1,...,X_{p}=x_{p}\right]-E\left[Y|X_{1}=x_{1},...,X_{p}=x_{p}\right]=\beta_{1}$
\end_inset


\end_layout

\begin_layout Itemize
So the intepretation of a multivariate regression coefficient is the expected
 change in the response per unit change in the regressor, holding all of
 the other regressors fixed.
\end_layout

\end_deeper
\begin_layout Itemize
Fitted values, residuals, and residual variation - all of our simple linear
 regression quantities can be extended to multivariate linear models.
\end_layout

\begin_deeper
\begin_layout Itemize
Model: 
\begin_inset Formula $Y_{i}=\sum_{k=1}^{p}X_{ik}\beta_{k}+\epsilon_{i}$
\end_inset

 where 
\begin_inset Formula $\epsilon_{i}\sim N\left(0,\sigma^{2}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Fitted responses: 
\begin_inset Formula $\hat{Y}_{i}=\sum_{k=1}^{p}X_{ik}\hat{\beta}_{k}$
\end_inset


\end_layout

\begin_layout Itemize
Residuals: 
\begin_inset Formula $e_{i}=Y_{i}-\hat{Y}_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Variance estimate: 
\begin_inset Formula $\hat{\sigma}^{2}=\frac{1}{n-p}\sum_{i=1}^{n}e_{i}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
To get predicted responses at new values 
\begin_inset Formula $x_{1},...,x_{p}$
\end_inset

, simply plug these values into the linear model 
\begin_inset Formula $\sum_{k=1}^{p}x_{k}\hat{\beta}_{k}$
\end_inset

.
\end_layout

\begin_layout Itemize
Our coefficients have standard errors 
\begin_inset Formula $\hat{\sigma}_{\hat{\beta}_{k}}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\hat{\beta}_{k}-\beta_{k}}{\hat{\sigma}_{\hat{\beta}_{k}}}$
\end_inset

 follows a T distribution with 
\begin_inset Formula $n-p$
\end_inset

 degrees of freedom.
\end_layout

\end_deeper
\begin_layout Itemize
Predicted responses have standard errors and we can calculate predicted
 and expected response intervals.
\end_layout

\end_deeper
\begin_layout Itemize
Linear models are the single most important applied statistical and machine
 learning technique 
\series bold
by far
\series default
.
\end_layout

\begin_layout Itemize
Some amazing things that you can accomplish with linear models:
\end_layout

\begin_deeper
\begin_layout Itemize
Decompose a signal into its harmonics.
\end_layout

\begin_layout Itemize
Flexibly fit complicated functions.
\end_layout

\begin_layout Itemize
Fit factor variables as predictors.
\end_layout

\begin_layout Itemize
Uncover complex multivariate relationship with the response.
\end_layout

\begin_layout Itemize
Build accurate prediction models.
\end_layout

\end_deeper
\begin_layout Itemize
Dummy variables
\end_layout

\begin_deeper
\begin_layout Itemize
The equation for representing the relationship between a particular outcome
 and several factors contains binary variables, one for each factor.
 These are called 
\begin_inset Quotes eld
\end_inset

dummy variables
\begin_inset Quotes erd
\end_inset

 and each indicates if a particular outcome is associated with a specific
 factor or category.
\end_layout

\begin_layout Itemize
Consider the linear model 
\begin_inset Formula $Y_{i}=\beta_{0}+X_{i1}\beta_{1}+\epsilon_{i}$
\end_inset

 where each 
\begin_inset Formula $X_{i1}$
\end_inset

 is binary so that it is 1 if measurement 
\begin_inset Formula $i$
\end_inset

 is in a group and 0 otherwise.
\end_layout

\begin_layout Itemize
Then for people in the group, 
\begin_inset Formula $E\left[Y_{i}\right]=\beta_{0}+\beta_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
For people not in the group, 
\begin_inset Formula $E\left[Y_{i}\right]=\beta_{0}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{1}$
\end_inset

 is interpreted as the increase or decrease in the mean comparing those
 in the group to those who are not.
\end_layout

\begin_layout Itemize
Consider a multi-level factor level.
 Let's say a three-level factor based on political party affiliation (Republican
, Democrat, Independent).
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{0}+X_{i1}\beta_{1}+X_{i2}\beta_{2}+\epsilon_{i}$
\end_inset

 where 
\begin_inset Formula $X_{i1}$
\end_inset

 is 1 for Republicans and 0 otherwise and 
\begin_inset Formula $X_{i2}$
\end_inset

 is 1 for Democrats and 0 otherwise.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $i$
\end_inset

 is Republican, Democrat, or Independent, 
\begin_inset Formula $E\left[Y\right]=\beta_{0}+\beta_{1}$
\end_inset

, 
\begin_inset Formula $\beta_{0}+\beta_{2}$
\end_inset

, or 
\begin_inset Formula $\beta_{0}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{1}$
\end_inset

 compares Republicans to Independents, 
\begin_inset Formula $\beta_{2}$
\end_inset

 compares Democrats to Independents, and 
\begin_inset Formula $\beta_{1}-\beta_{2}$
\end_inset

 compares Republicans to Democrats.
\end_layout

\end_deeper
\begin_layout Itemize
The first entry in the Estimate column is labeled as "(Intercept)".
 That is because sprayA is the first in the alphabetical list of the levels
 of the factor, and R by default uses the first level as the reference against
 which the other levels or groups are compared when doing its t-tests (shown
 in the third column).
\end_layout

\begin_layout Itemize
The estimates in this case are the coefficients of the binary or dummy variables.
 The Intercept is the mean of the reference group, and the other Estimates
 are the distances of the other groups' means from the reference mean.
\end_layout

\begin_deeper
\begin_layout Itemize
If we do an lm(y ~ x-1), this will remove the intercept.
 Then sprayA will be shown and all of the Estimates (and other quantities)
 will be absolute, instead of being computed relative to sprayA.
\end_layout

\end_deeper
\begin_layout Itemize
To choose the reference group, you can use the 
\family typewriter
relevel()
\family default
 function to re-order the factor.
\end_layout

\begin_layout Itemize
To get t-values relative to the reference group, take 
\family typewriter
(fit$coef[2]-fit$coef[3])/standard error
\family default
.
 The standard error is usually obtained from the one-on-one regressions.
\end_layout

\end_deeper
\begin_layout Subsection*
Multivariable regression - notes from swirl()
\end_layout

\begin_layout Itemize
In this lesson we'll illustrate that regression in many variables amounts
 to a series of regressions in one.
\end_layout

\begin_layout Itemize
Using regression in one variable, we'll show how to eliminate any chosen
 regressor, thus reducing a regression in N variables, to a regression in
 N-1.
\end_layout

\begin_deeper
\begin_layout Itemize
Hence, if we know how to do a regression in 1 variable, we can do a regression
 in 2.
\end_layout

\begin_layout Itemize
Once we know how to do a regression in 2 variables, we can do a regression
 in 3, and so on.
\end_layout

\end_deeper
\begin_layout Itemize
We begin with the galton data and a review of eliminating the intercept
 by subtracting the means.
\end_layout

\begin_layout Itemize
When we perform a regression in one variable, such as lm(child ~ parent,
 galton), we get two coefficients, a slope and an intercept.
\end_layout

\begin_deeper
\begin_layout Itemize
The intercept is really the coefficient of a special regressor which has
 the same value, 1, at every sample.
 The function, lm, includes this regressor by default.
\end_layout

\begin_layout Itemize
In earlier lessons we demonstrated that the regression line given by lm(child
 ~ parent, galton) goes through the point x=mean(parent), y=mean(child).
\end_layout

\begin_layout Itemize
We also showed that if we subtract the mean from each variable, the regression
 line goes through the origin, x=0, y=0, hence its intercept is zero.
\end_layout

\begin_layout Itemize
Thus, by subtracting the means, we eliminate one of the two regressors,
 the constant, leaving just one, parent.
 The coefficient of the remaining regressor is the slope.
\end_layout

\begin_layout Itemize
Subtracting the means to eliminate the intercept is a special case of a
 general technique which is sometimes called Gaussian Elimination.
\end_layout

\begin_layout Itemize
As it applies here, the general technique is to pick one regressor and to
 
\series bold
replace all other variables by the residuals of their regressions against
 that one
\series default
.
\end_layout

\end_deeper
\begin_layout Itemize
Example with swiss dataset:
\end_layout

\begin_deeper
\begin_layout Itemize
First, use the R function lm to generate the linear model "all" in which
 Fertility is the variable dependent on all the others.
 Use the R shorthand "." to represent the five independent variables in the
 formula passed to lm.
 Remember the data is "swiss".
\end_layout

\begin_layout Itemize
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

all <- lm(Fertility ~ ., data=swiss)
\end_layout

\begin_layout Plain Layout

summary(all)
\end_layout

\begin_layout Plain Layout

Call: lm(formula = Fertility ~ ., data = swiss)
\end_layout

\begin_layout Plain Layout

Residuals:      Min       1Q   Median       3Q      Max  -15.2743  -5.2617
   0.5032   4.1198  15.3213 
\end_layout

\begin_layout Plain Layout

Coefficients:
\end_layout

\begin_layout Plain Layout

				  Estimate  Std.
 Error  t value Pr(>|t|)
\end_layout

\begin_layout Plain Layout

(Intercept)      66.91518   10.70604   6.250  1.91e-07
\end_layout

\begin_layout Plain Layout

Agriculture      -0.17211    0.07030  -2.448  0.01873
\end_layout

\begin_layout Plain Layout

Examination      -0.25801    0.25388  -1.016  0.31546
\end_layout

\begin_layout Plain Layout

Education        -0.87094    0.18303  -4.758 2.43e-05
\end_layout

\begin_layout Plain Layout

Catholic          0.10412    0.03526   2.953  0.00519
\end_layout

\begin_layout Plain Layout

Infant.Mortality  1.07705    0.38172   2.822  0.00734
\end_layout

\begin_layout Plain Layout

--- Signif.
 codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
\end_layout

\begin_layout Plain Layout

Residual standard error: 7.165 on 41 degrees of freedom Multiple R-squared:
  0.7067,	Adjusted R-squared:  0.671  F-statistic: 19.76 on 5 and 41 DF,  p-value:
 5.594e-10
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Recall that the Estimates are the coefficients of the independent variables
 of the linear model (all of which are percentages) and they reflect an
 estimated change in the dependent variable (fertility) when the corresponding
 independent variable changes.
 So, for every 1% increase in percent of males involved in agriculture as
 an occupation we expect a .17% decrease in fertility, holding all the other
 variables constant; for every 1% increase in Catholicism, we expect a .10%
 increase in fertility, holding all other variables constant.
\end_layout

\end_deeper
\begin_layout Subsection*
Interactions
\end_layout

\begin_layout Itemize
Example: hunger data.
\end_layout

\begin_layout Itemize
Linear model: 
\begin_inset Formula $Hu_{i}=b_{0}+b_{1}Y_{i}+e_{i}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Hu_{i}$
\end_inset

: percent of hungry children.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{0}$
\end_inset

: percent hungry at year 0.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{1}$
\end_inset

: decrease in percent hungry per year.
\end_layout

\begin_layout Itemize
\begin_inset Formula $e_{i}$
\end_inset

: everything we didn't measure.
\end_layout

\end_deeper
\begin_layout Itemize
If we consider a model with two lines:
\end_layout

\begin_deeper
\begin_layout Itemize
Females: 
\begin_inset Formula $HuF_{i}=bf_{0}+bf_{1}YF_{i}+ef_{i}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $bf_{0}$
\end_inset

: percent of girls hungry at year 0.
\end_layout

\begin_layout Itemize
\begin_inset Formula $bf_{1}$
\end_inset

: decrease in percent of girls hungry per year.
\end_layout

\begin_layout Itemize
\begin_inset Formula $ef_{i}$
\end_inset

: everything we didn't measure.
\end_layout

\end_deeper
\begin_layout Itemize
Males: 
\begin_inset Formula $HuM_{i}=bm_{0}-bm_{1}YM_{i}+em_{i}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $bm_{0}$
\end_inset

: percent of boys hungry at year 0.
\end_layout

\begin_layout Itemize
\begin_inset Formula $bn_{1}$
\end_inset

: decrease in percent of boys hungry per year.
\end_layout

\begin_layout Itemize
\begin_inset Formula $em_{i}$
\end_inset

: everything we didn't measure.
\end_layout

\end_deeper
\begin_layout Itemize
When we stratify by gender, the slopes and intercepts and residual variances
 are different for each gender.
\end_layout

\end_deeper
\begin_layout Itemize
Two lines, same slope, different intercept: 
\begin_inset Formula $Hu_{i}=b_{0}+b_{1}1\left(Sex_{i}="Male"\right)+b_{2}Y_{i}+e_{i}^{*}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $b_{0}$
\end_inset

: percent hungry at year zero for females.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{0}+b_{1}$
\end_inset

: percent hungry at year zero for males.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{2}$
\end_inset

: change in percent hungry (for either males or females) in one year.
\end_layout

\begin_layout Itemize
\begin_inset Formula $e_{i}^{*}$
\end_inset

: everything we didn't measure.
\end_layout

\begin_layout Itemize
How to do this in R:
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

lmBoth <-lm(Numeric ~ Year + Sex, data=hunger)
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
This says that our slope variable, Year, does not interact with Sex.
 So only the intercepts will be different.
\end_layout

\end_deeper
\begin_layout Itemize
Two lines, different slopes (interactions): 
\begin_inset Formula $Hu_{i}=b_{0}+b_{1}1\left(Sex_{i}="Male"\right)+b_{2}Y_{i}+b_{3}1\left(Sex_{i}="Male"\right)Y_{i}+e_{i}^{+}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $b_{0}$
\end_inset

: percent hungry at year 0 for females.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{0}+b_{1}$
\end_inset

: percent hungry at year zero for males.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{2}$
\end_inset

: change in percent hungry (females) in one year.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{2}+b_{3}$
\end_inset

: change in percent hungry (males) in one year.
\end_layout

\begin_layout Itemize
\begin_inset Formula $e_{i}^{+}$
\end_inset

: everything we didn't measure.
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[H\right]=b_{0}+b_{1}+\left(b_{2}+b_{3}\right)Y$
\end_inset

 for males.
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[H\right]=b_{0}+b_{2}Y$
\end_inset

 for females.
\end_layout

\begin_layout Itemize
How to do this in R: 
\family typewriter
lmInter <- lm(Numeric ~ Year + Sex + Sex*Year, data=hunger)
\end_layout

\end_deeper
\begin_layout Itemize
Interpreting a continuous interaction:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[Y_{i}|X_{1i}=x_{1},X_{2i}=x_{2}\right]=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\beta_{3}x_{1}x_{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The last term here is the interaction term.
\end_layout

\end_deeper
\begin_layout Itemize
Holding 
\begin_inset Formula $X_{2}$
\end_inset

constant, we have: 
\begin_inset Formula $E\left[Y_{i}|X_{1i}=x_{1}+1,X_{2i}=x_{2}\right]-E\left[Y_{i}|X_{1i}=x_{1},X_{2i}=x_{2}\right]=\beta_{1}+\beta_{3}x_{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Thus, the expected change in 
\begin_inset Formula $Y$
\end_inset

 per unit 
\begin_inset Formula $X_{1}$
\end_inset

 holding all else constant is not constant.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{1}$
\end_inset

 is the slope when 
\begin_inset Formula $x_{2}=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
We can do the same kind of thing to see what happens when both variables
 change:
\end_layout

\begin_deeper
\begin_layout Itemize
End result: 
\begin_inset Formula $\beta_{3}$
\end_inset

 is the expected change in 
\begin_inset Formula $Y$
\end_inset

 per unit change in 
\begin_inset Formula $X_{1}$
\end_inset

, per unit change in 
\begin_inset Formula $X_{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
We can also think of it as the change in the slope relating 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 per unit change in 
\begin_inset Formula $X_{2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
To investigate residual relationships:
\end_layout

\begin_deeper
\begin_layout Itemize
Can plot 
\family typewriter
resid(lm(y ~ x2))
\family default
 vs.
 
\family typewriter
resid(lm(y ~ x1))
\family default
.
\end_layout

\end_deeper
\begin_layout Subsection*
Residuals and diagnostics
\end_layout

\begin_layout Itemize
Residuals are defined as 
\begin_inset Formula $e_{i}=Y_{i}-\hat{Y}_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Our estimate of residual variation is 
\begin_inset Formula $\hat{\sigma}^{2}=\frac{\sum_{i=1}^{n}e_{i}^{2}}{n-p}$
\end_inset

 where 
\begin_inset Formula $n-p$
\end_inset

 is chosen such that 
\begin_inset Formula $E\left[\hat{\sigma}^{2}\right]=\sigma^{2}$
\end_inset

, i.e., the estimator is unbiased.
\end_layout

\begin_layout Itemize
Influential, high leverage, and outlying points
\end_layout

\begin_deeper
\begin_layout Itemize
Calling a point an outlier is 
\series bold
vague
\series default
.
\end_layout

\begin_deeper
\begin_layout Itemize
Outliers can be the result of spurious or real processes (example: data
 entry error or a statistical fluctuation).
\end_layout

\begin_layout Itemize
Outliers can have varying degrees of influence.
\end_layout

\begin_layout Itemize
Outliers can conform to the regression relationship (i.e., being marginally
 outlying in 
\begin_inset Formula $X$
\end_inset

 or 
\begin_inset Formula $Y$
\end_inset

, but not outlying given the regression relationship).
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Measures of influence
\end_layout

\begin_deeper
\begin_layout Itemize
Do 
\family typewriter
?influence.measures
\family default
 to see the full suite of influence measures in stats.
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
rstandard
\family default
: standardized residuals (residuals divided by their standard deviations).
\end_layout

\begin_layout Itemize

\family typewriter
rstudent
\family default
: standardized residuals where the 
\begin_inset Formula $i$
\end_inset

th data point was deleted in the calculation of the standard deviation for
 the residual to follow a t-distribution.
\end_layout

\begin_layout Itemize

\family typewriter
hatvalues
\family default
: measures of leverage.
\end_layout

\begin_layout Itemize

\family typewriter
dffits
\family default
: change in the predicted response when the 
\begin_inset Formula $i$
\end_inset

th point is deleted in fitting the model.
\end_layout

\begin_layout Itemize

\family typewriter
dfbetas
\family default
: change in the individual coefficients when the 
\begin_inset Formula $i$
\end_inset

th point is deleted in fitting the model.
\end_layout

\begin_layout Itemize

\family typewriter
cooks.distance
\family default
: overall change in the coefficients when the 
\begin_inset Formula $i$
\end_inset

th point is deleted.
\end_layout

\begin_layout Itemize

\family typewriter
resid
\family default
: returns the ordinary residuals.
\end_layout

\begin_layout Itemize

\family typewriter
resid(fit)/(1-hatvalues(fit))
\family default
: returns the PRESS residuals, i.e., the 
\begin_inset Quotes eld
\end_inset

leave-one-out
\begin_inset Quotes erd
\end_inset

 cross-validation residuals - the difference in the response and the predicted
 response at data point 
\begin_inset Formula $i$
\end_inset

, where it was not included in the model fitting.
 Here 
\family typewriter
fit
\family default
 is the linear model.
\end_layout

\end_deeper
\begin_layout Itemize
How to use these:
\end_layout

\begin_deeper
\begin_layout Itemize
Be wary of simplistic rules for diagnostic plots and measures.
 The use of these tools is context-specific.
\end_layout

\begin_layout Itemize
Not all of the measures have meaningful absolute scales.
 You can look at them relative to the values across the data.
\end_layout

\begin_layout Itemize
They probe your data in different ways to diagnose different problems.
\end_layout

\begin_layout Itemize
Patterns in your residual plots generally indicate some poor aspect of model
 fit, including:
\end_layout

\begin_deeper
\begin_layout Itemize
Heteroskedasticity (non-constant variance).
\end_layout

\begin_layout Itemize
Missing model terms.
\end_layout

\begin_layout Itemize
Temporal patterns (plot residuals vs.
 collection order of data points).
\end_layout

\end_deeper
\begin_layout Itemize
Residual QQ plots investigate normality of the errors.
\end_layout

\begin_layout Itemize
Leverage measures (hat values) can be useful for diagnosing data entry errors.
\end_layout

\begin_layout Itemize
Influence measures get to the bottom line - how does deleting or including
 this point impact a particular aspect of the model?
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Model selection
\end_layout

\begin_layout Itemize
Choosing a model when you have multiple predictors
\end_layout

\begin_deeper
\begin_layout Itemize
Machine learning class focuses on prediction, so we'll focus on modeling.
\end_layout

\end_deeper
\begin_layout Itemize
Prediction has a different set of criteria, needs for interpretability and
 standard for generalizability.
\end_layout

\begin_layout Itemize
In modeling, we want to make a parsimonious, interpretable representation
 of the data that enhances our understanding of the phenomena under study.
\end_layout

\begin_deeper
\begin_layout Itemize
Under this philosophy, what's the right model? Whatever model connects the
 data to a true, parsimonious statement abotu what you're studying.
\end_layout

\begin_layout Itemize
Good modeling decisions are context-dependent.
\end_layout

\begin_layout Itemize
We'll focus on variable inclusion and exclusion.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Rumsfeldian triplet
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Known knowns: regressors that we know we should include in the model and
 have included.
\end_layout

\begin_layout Itemize
Known unknowns: regressors that we would like to include in the model but
 we don't have them.
\end_layout

\begin_layout Itemize
Unknown unknowns: regressors that we don't even know about that we should
 have included in the model.
\end_layout

\end_deeper
\begin_layout Itemize
General rules
\end_layout

\begin_deeper
\begin_layout Itemize
Omitting variables tends to result in bias unless their regressors are uncorrela
ted with the omitted ones.
\end_layout

\begin_deeper
\begin_layout Itemize
This is why we randomize treatments - it attempts to uncorrelate our treatment
 indicator with variables that we don't have and can't include in the model.
\end_layout

\begin_layout Itemize
If there are too many unobserved confounding variables, even randomization
 won't help us.
\end_layout

\end_deeper
\begin_layout Itemize
Including variables that we shouldn't have increases standard errors of
 the regression variables.
\end_layout

\begin_deeper
\begin_layout Itemize
Thus, we don't want to idly throw new variables into the model.
\end_layout

\end_deeper
\begin_layout Itemize
The model must tend toward perfect fit as the number of non-redundant regressors
 approaches the number of measurements.
\end_layout

\begin_layout Itemize
\begin_inset Formula $R^{2}$
\end_inset

 increases monotonically as more regressors are added.
\end_layout

\begin_layout Itemize
The sum of the squared errors decreases monotonically as more regressors
 are included.
\end_layout

\end_deeper
\begin_layout Itemize
Variance inflation: variance of model tends to increase as you add uncorrelated
 predictors (i.e., variables that aren't relevant and you shouldn't add).
\end_layout

\begin_deeper
\begin_layout Itemize
If the regressors you add are correlated to other regressors, then your
 variance will become even more inflated!
\end_layout

\begin_layout Itemize
Note: we don't actually know 
\begin_inset Formula $\sigma$
\end_inset

, so we can only estimate the increase in the actual standard error of the
 coefficients for including a regressor.
\end_layout

\begin_deeper
\begin_layout Itemize
However, 
\begin_inset Formula $\sigma$
\end_inset

 drops out of the relative standard errors.
 If you sequentially add variables, you can check the variance inflation
 for including each one.
\end_layout

\end_deeper
\begin_layout Itemize
When the other regressors are actually orthogonal to the regressor of interest,
 then there is no variance inflation.
\end_layout

\begin_layout Itemize
The variance inflation factor (VIF) is the increase in the variance for
 the 
\begin_inset Formula $i$
\end_inset

th regressor compared to the ideal setting where it is orthogonal to the
 other regressors.
\end_layout

\begin_deeper
\begin_layout Itemize
Use the 
\family typewriter
vif(fit)
\family default
 function in R to get this.
\end_layout

\end_deeper
\begin_layout Itemize
Note: variance inflation is only part of the picture.
 We may still want to include certain variables, even if they dramatically
 inflate our variance.
\end_layout

\end_deeper
\begin_layout Itemize
What about residual variance estimation?
\end_layout

\begin_deeper
\begin_layout Itemize
Assuming that the model is linear with additive IID errors (with finite
 variance), we can mathematically describe the impact of omitting necessary
 variables or including unnecessary ones.
\end_layout

\begin_layout Itemize
If we underfit the model, the variance estimate is biased.
\end_layout

\begin_layout Itemize
If we correctly fit or overfit the model, including all necessary covariates
 and/or uncessary covariates, the variance estimate is biased (
\begin_inset Formula $E\left[\hat{\sigma}^{2}\right]=\sigma^{2}$
\end_inset

).
\end_layout

\begin_layout Itemize
However, the variance of the variance is larger if we include unncessary
 variables (
\begin_inset Formula $Var\left(\hat{\sigma}_{unnecessary}^{2}\right)\geq Var\left(\hat{\sigma}_{necessary}^{2}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Covariate model selection
\end_layout

\begin_deeper
\begin_layout Itemize
Principal components or factor analysis models on covariates are often useful
 for reducing complex covariate spaces.
\end_layout

\begin_layout Itemize
Good design can often eliminate the need for complex model searches.
\end_layout

\begin_layout Itemize
If the models of interest are nested and without lots of parameters differentiat
ing them, it's fairly uncontroversial to use nested likelihood ratio tests.
\end_layout

\begin_layout Itemize
Suggested approach: given a coefficient of interest, use covariate adjustment
 and multiple models to probe that effect to evaluate it for robustness
 and to see what other covariates knock it out.
 This isn't terribly systematic, but it does tend to teach you a lot about
 the data.
\end_layout

\end_deeper
\begin_layout Itemize
How to do nested model testing in R:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

fit1 <- lm(Fertility ~ Agriculture, data=swiss)
\end_layout

\begin_layout Plain Layout

fit2 <- update(fit, Fertility ~ Agriculture + Examination + Education)
\end_layout

\begin_layout Plain Layout

fit5 <- update(fit, Fertility ~ Agriculture + Examination + Education +
 Catholic + Infant.Mortality)
\end_layout

\begin_layout Plain Layout

anova(fit1, fit3, fit5) ## creates a series of likelihood ratio tests.
 Output compares each model to the previous one.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Generalized linear models
\end_layout

\begin_layout Itemize
Linear models are one of the most useful applied statistical techniques,
 but they do have limitations.
\end_layout

\begin_deeper
\begin_layout Itemize
Additive response models don't make much sense if the response is discrete
 or strictly positive.
\end_layout

\begin_layout Itemize
Additive error models often don't make sense.
 Example: if the outcome has to be positive.
\end_layout

\begin_layout Itemize
Transformations are often hard to interpret.
\end_layout

\begin_layout Itemize
There is value in modeling the data on the scale with which it was collected.
\end_layout

\begin_layout Itemize
Particularly interpretable transformations, specifically natural logarithms,
 aren't applicable for negative or zero values.
\end_layout

\end_deeper
\begin_layout Itemize
Generalized linear models (GLMs) involve three components:
\end_layout

\begin_deeper
\begin_layout Itemize
An exponential family model for the response.
\end_layout

\begin_layout Itemize
A systematic component via a linear predictor.
\end_layout

\begin_layout Itemize
A link function that connects the means of the response to the linear predictor.
\end_layout

\end_deeper
\begin_layout Itemize
Example:
\end_layout

\begin_deeper
\begin_layout Itemize
Assume that 
\begin_inset Formula $Y_{i}\sim N\left(\mu_{i},\sigma^{2}\right)$
\end_inset

 (the Gaussian distribution is an exponential family distribution).
\end_layout

\begin_layout Itemize
Define the linear predictor to be 
\begin_inset Formula $\eta_{i}=\sum_{k=1}^{p}X_{ik}\beta_{k}$
\end_inset


\end_layout

\begin_layout Itemize
The link function is 
\begin_inset Formula $g\left(\mu\right)=\eta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
For linear models, 
\begin_inset Formula $g\left(\mu\right)=\mu$
\end_inset

 such that 
\begin_inset Formula $\mu_{i}=\eta_{i}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
This yields the same likelihood model as our additive error Gausisan linear
 model 
\begin_inset Formula $Y_{i}=\sum_{k=1}^{p}X_{ik}\beta_{k}+\epsilon_{i}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
As usual, 
\begin_inset Formula $\epsilon_{i}\sim N\left(0,\sigma^{2}\right)$
\end_inset

 and are assumed to be IID.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example: logistic regression.
\end_layout

\begin_deeper
\begin_layout Itemize
Assume that 
\begin_inset Formula $Y_{i}\sim Bernoulli\left(\mu_{i}\right)$
\end_inset

 such that 
\begin_inset Formula $E\left[Y_{i}\right]=\mu_{i}$
\end_inset

 where 
\begin_inset Formula $0\leq\mu_{i}\leq1$
\end_inset

.
\end_layout

\begin_layout Itemize
Linear predictor: 
\begin_inset Formula $\eta_{i}=\sum_{k=1}^{p}X_{ik}\beta_{k}$
\end_inset


\end_layout

\begin_layout Itemize
Link function: 
\begin_inset Formula $g\left(\mu\right)=\eta=\log\left(\frac{\mu}{1-\mu}\right)$
\end_inset

 is the natural log of the odds, referred to as the logit.
\end_layout

\begin_layout Itemize
Note that we an invert the logit function as 
\begin_inset Formula $\mu_{i}=\frac{\exp\left(\eta_{i}\right)}{1+\exp\left(\eta_{i}\right)}$
\end_inset

 and 
\begin_inset Formula $1-\mu_{i}=\frac{1}{1+\exp\left(\eta_{i}\right)}$
\end_inset


\end_layout

\begin_layout Itemize
Thus, the likelihood is 
\begin_inset Formula $\prod_{i=1}^{n}\mu_{i}^{y_{i}}\left(1-\mu_{i}\right)^{1-y_{i}}=\exp\left(\sum_{i=1}^{n}y_{i}\eta_{i}\right)\prod_{i=1}^{n}\left(1+\eta_{i}\right)^{-1}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Example: Poisson regression.
\end_layout

\begin_deeper
\begin_layout Itemize
Assume that 
\begin_inset Formula $Y_{i}\sim Poisson\left(\mu_{i}\right)$
\end_inset

 such that 
\begin_inset Formula $E\left[Y_{i}\right]=\mu_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Linear predictor: 
\begin_inset Formula $\eta_{i}=\sum_{k=1}^{p}X_{ik}\beta_{k}$
\end_inset


\end_layout

\begin_layout Itemize
Link function: 
\begin_inset Formula $g\left(\mu\right)=\eta=\log\left(\mu\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Recall that 
\begin_inset Formula $e^{x}$
\end_inset

 is the inverse of 
\begin_inset Formula $\log\left(x\right)$
\end_inset

 so that 
\begin_inset Formula $\mu_{i}=e^{\eta_{i}}$
\end_inset

.
\end_layout

\begin_layout Itemize
Thus, the likelihood is 
\begin_inset Formula $\prod_{i=1}^{n}\left(y_{i}!\right)^{-1}\mu_{i}^{y_{i}}e^{-\mu_{i}}\propto\exp\left(\sum_{i=1}^{n}y_{i}\eta_{i}-\sum_{i=1}^{n}\mu_{i}\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
In each case, the only way in which the likelihood depends on the data is
 through 
\begin_inset Formula $\sum_{i=1}^{n}y_{i}\eta_{i}=\sum_{i=1}^{n}y_{i}\sum_{k=1}^{p}X_{ik}\beta_{k}=\sum_{k=1}^{p}\beta_{k}\sum_{i=1}^{n}X_{ik}y_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Thus, we don't really need the full data, only 
\begin_inset Formula $\sum_{i=1}^{n}X_{ik}y_{i}$
\end_inset

.
 This simplification is a consequence of choosing so-called 
\begin_inset Quotes eld
\end_inset

canonical
\begin_inset Quotes erd
\end_inset

 link functions.
\end_layout

\end_deeper
\begin_layout Itemize
All models achieve their maximum at the root of the so-called normal equations:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $0=\sum_{i=1}^{n}\frac{\left(Y_{i}-\mu_{i}\right)}{Var\left(Y_{i}\right)}W_{i}$
\end_inset

, where 
\begin_inset Formula $W_{i}$
\end_inset

 are the derivatives of the inverse of the link function.
\end_layout

\end_deeper
\begin_layout Itemize
Variances
\end_layout

\begin_deeper
\begin_layout Itemize
For the linear model: 
\begin_inset Formula $Var\left(Y_{i}\right)=\sigma^{2}$
\end_inset

 is constant.
\end_layout

\begin_layout Itemize
For the binomial case: 
\begin_inset Formula $Var\left(Y_{i}\right)=\mu_{i}\left(1-\mu\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
For the Poisson case: 
\begin_inset Formula $Var\left(Y_{i}\right)=\mu_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
In the latter cases, it is often relevant to have a more flexible variance
 model, even if it doesn't correspond to an actual likelhiood.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $0=\sum_{i=1}^{n}\frac{\left(Y_{i}-\mu_{i}\right)}{\phi\mu_{i}\left(1-\mu_{i}\right)}W_{i}$
\end_inset

 and 
\begin_inset Formula $0=\sum_{i=1}^{n}\frac{\left(Y_{i}-\mu_{i}\right)}{\phi\mu_{i}}W_{i}$
\end_inset


\end_layout

\begin_layout Itemize
These are called 'quasi-likelihood' normal equations.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\phi$
\end_inset

 is an extra parameter used to calculate the variance (doesn't depend on
 
\begin_inset Formula $i$
\end_inset

).
 It can help when there are complicated variance restrictions.
\end_layout

\begin_layout Itemize
The purpose is to relax the very strict assumptions about the relationships
 between the means and variances in the models.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Odds and ends
\end_layout

\begin_deeper
\begin_layout Itemize
The normal equations have to be solved iteratively.
 Results are 
\begin_inset Formula $\hat{\beta}_{k}$
\end_inset

 and 
\begin_inset Formula $\phi$
\end_inset

 (if included).
\end_layout

\begin_layout Itemize
Predicted linear predictor responses can be obtained as 
\begin_inset Formula $\hat{\eta}=\sum_{k=1}^{p}X_{k}\hat{\beta}_{k}$
\end_inset

.
\end_layout

\begin_layout Itemize
Predicted mean responses are given by 
\begin_inset Formula $\hat{\mu}=g^{-1}\left(\hat{\eta}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Coefficients are interpreted as 
\begin_inset Formula $g\left(E\left[Y|X_{k}=x_{k}+1,X_{\sim k}=x_{\sim k}\right]\right)-g\left(E\left[Y|X_{k}=x_{k},X_{\sim k}=x_{\sim k}\right]\right)=\beta_{k}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Difference in a one-unit increase in a particular coefficient, holding the
 other constant.
\end_layout

\begin_layout Itemize
Done on the link function scale.
\end_layout

\end_deeper
\begin_layout Itemize
Variations on the Newton-Raphson method are used to solve iteratively.
\end_layout

\begin_layout Itemize
Asymptotics are used for inference usually.
\end_layout

\begin_layout Itemize
Many of the ideas from linear models can be brought over to GLMs.
\end_layout

\end_deeper
\begin_layout Subsection*
More on odds
\end_layout

\begin_layout Itemize
Using ANOVA: 
\family typewriter
anova(logRegRavens, test=
\begin_inset Quotes erd
\end_inset

Chisq
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize
Want to do this with nested models.
\end_layout

\begin_layout Itemize
Deviance: measure of model fit compared to the previous model.
\end_layout

\begin_layout Itemize
To compare deviances, take a smaller model and subtract the larger model
 from it.
\end_layout

\begin_layout Itemize
The deviance residuals can be compared to a chi-squared distribution with
 degrees of freedom equal to that of the difference in degrees of freedom
 between the two models.
\end_layout

\end_deeper
\begin_layout Itemize
Interpreting odds ratios
\end_layout

\begin_deeper
\begin_layout Itemize
They are NOT probabilities!
\end_layout

\begin_layout Itemize
Odds ratio of 1 = log odds ratio of 0 = no difference in odds.
\end_layout

\begin_layout Itemize
Odds ratio < 0.5 or > 2 commonly called a 
\begin_inset Quotes eld
\end_inset

moderate effect.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Relative risk 
\begin_inset Formula $\frac{Pr\left(W_{i}|S_{i}=10\right)}{Pr\left(W_{i}|S_{i}=0\right)}$
\end_inset

 is often easier to interpret but harder to estimate.
\end_layout

\begin_layout Itemize
For small probabilities, RR 
\begin_inset Formula $\approx$
\end_inset

 OR, but they are not the same!
\end_layout

\end_deeper
\begin_layout Subsection*
Poisson regression
\end_layout

\begin_layout Itemize
Key ideas:
\end_layout

\begin_deeper
\begin_layout Itemize
May data take the form of counts: calls to a call center, number of flu
 cases in an area, number of cars that cross a bridge, etc.
\end_layout

\begin_layout Itemize
Data may also be in the form of rates: percent of children passing a test,
 percent of hits to a website from a particular country, etc.
\end_layout

\begin_layout Itemize
Linear regression with transformation is an option.
\end_layout

\end_deeper
\begin_layout Itemize
Uses of the Poisson distribution: counts and rates
\end_layout

\begin_deeper
\begin_layout Itemize
Examples: web traffic hits, incidence rates, contingency table data, approximati
ng binomial probabilities with small 
\begin_inset Formula $p$
\end_inset

 and large 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
The Poisson mass function
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $P\left(X=x\right)=\frac{\left(t\lambda\right)^{x}e^{-t\lambda}}{x!}$
\end_inset

 for 
\begin_inset Formula $x=0,1,2,...$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[X\right]=Var\left(X\right)=t\lambda$
\end_inset


\end_layout

\begin_layout Itemize
The Poisson distribution tends to a normal distribution as 
\begin_inset Formula $t\lambda$
\end_inset

 gets large.
\end_layout

\end_deeper
\begin_layout Itemize
Example: number of hits per day on a website
\end_layout

\begin_deeper
\begin_layout Itemize
Can model as linear regression: 
\begin_inset Formula $H_{i}=b_{0}+b_{1}D_{i}+e_{i}$
\end_inset

, 
\family typewriter
lm1 <- lm(visits ~ date)
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $b_{0}$
\end_inset

: number of hits on Julian day 0.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{1}$
\end_inset

: increase in number of hits per day.
\end_layout

\begin_layout Itemize
\begin_inset Formula $H_{i}$
\end_inset

: number of hits.
\end_layout

\begin_layout Itemize
\begin_inset Formula $D_{i}$
\end_inset

: Julian day.
\end_layout

\end_deeper
\begin_layout Itemize
Taking the log of the outcome has a specific interpretation: 
\begin_inset Formula $\log\left(H_{i}\right)=b_{0}+b_{1}D_{i}+e_{i}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $b_{0}$
\end_inset

: log number of hits on Julian day 0.
\end_layout

\begin_layout Itemize
\begin_inset Formula $b_{1}$
\end_inset

: increase in log number of hits per day.
\end_layout

\end_deeper
\begin_layout Itemize
Poisson/log-linear: 
\begin_inset Formula $\log\left(E\left[H_{i}|D_{i},b_{0,}b_{1}\right]\right)=b_{0}+b_{1}D_{i}$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Multiplicative differences: 
\begin_inset Formula $E\left[H_{i}|D_{i},b_{0},b_{1}\right]=\exp\left(b_{0}+b_{1}D_{i}\right)=\exp\left(b_{0}\right)\exp\left(b_{1}D_{i}\right)$
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $D_{i}$
\end_inset

 is increased by one unit, 
\begin_inset Formula $E\left[H_{i}|D_{i},b_{0},b_{1}\right]$
\end_inset

 is multiplied by 
\begin_inset Formula $\exp\left(b_{1}\right)$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Exponentiating coefficients
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\exp\left(E\left[\log\left(Y\right)\right]\right)$
\end_inset

 is the geometric mean of 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Itemize
When you take the natural log of outcomes and fit a regression model, your
 exponentiated coefficients estimate things about geometric means.
\end_layout

\begin_layout Itemize
\begin_inset Formula $e^{\beta_{0}}$
\end_inset

 estimates the geometric mean hits on day 0.
\end_layout

\begin_layout Itemize
\begin_inset Formula $e^{\beta_{1}}$
\end_inset

 estimates relative increase or decrease in geometric mean hits per day.
\end_layout

\begin_layout Itemize
There is a problem with logs when you have zero counts, but you can add
 a constant without affecting your results.
\end_layout

\end_deeper
\begin_layout Itemize
Poisson regression in R
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

glm1 <- glm(visits ~ julianDay, family="poisson")
\end_layout

\begin_layout Plain Layout

## Plotting the fit
\end_layout

\begin_layout Plain Layout

lines(julianDay, glm1$fitted)
\end_layout

\begin_layout Plain Layout

## confidence intervals
\end_layout

\begin_layout Plain Layout

confint(glm1)
\end_layout

\begin_layout Plain Layout

## agnostic confidence intervals
\end_layout

\begin_layout Plain Layout

confint.agnostic(glm1)
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Modeling rates
\end_layout

\begin_deeper
\begin_layout Itemize
In 
\family typewriter
glm()
\family default
, use an offset like 
\family typewriter
offset=log(visits+1)
\end_layout

\end_deeper
\end_body
\end_document
