#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Regression Models - Notes
\end_layout

\begin_layout Author
Tanner Prestegard
\end_layout

\begin_layout Date
Course taken from 6/1/2015 - 6/28/2015
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Subsection*

\series bold
Introduction
\end_layout

\begin_layout Itemize
Questions we may want to answer for this class (considering an example of
 children's heights and their parents' heights):
\end_layout

\begin_deeper
\begin_layout Itemize
Use the parents' heights to predict children's heights.
\end_layout

\begin_layout Itemize
To try to find an easily described mean relationship between parent and
 children heights.
\end_layout

\begin_layout Itemize
To investigate the variation in children's heights that appears unrelated
 to parents' heights (residual variation).
\end_layout

\begin_layout Itemize
To quantify what impact genotype information has beyond parental height
 in explaining child height.
\end_layout

\begin_layout Itemize
To figure out how and what assumptions are needed to generalize findings
 beyond the data in question.
\end_layout

\begin_layout Itemize
Why do children of very tall parents tend to be tall, but a little shorter
 than their parents, and why do children of very short parents tend to be
 short, but a little taller than their parents? (regression to the mean)
\end_layout

\end_deeper
\begin_layout Subsection*
Background and notation
\end_layout

\begin_layout Itemize
Define the empirical (sample) mean as: 
\begin_inset Formula $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
If we subtract the mean from all data points, we are 
\series bold
centering
\series default
 the random variables: 
\begin_inset Formula $\tilde{X}_{i}=X_{i}-\bar{X}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The mean of 
\begin_inset Formula $\tilde{X}_{i}$
\end_inset

 is zero.
\end_layout

\end_deeper
\begin_layout Itemize
The empirical (sample) variance is: 
\begin_inset Formula $S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\frac{1}{n-1}$
\end_inset


\begin_inset Formula $\left(\sum_{i=1}^{n}X_{i}^{2}-n\bar{X}^{2}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The data defined by 
\begin_inset Formula $X_{i}/S$
\end_inset

 have empirical standard deviation 1.
 This is called 
\begin_inset Quotes eld
\end_inset

scaling
\begin_inset Quotes erd
\end_inset

 the data.
\end_layout

\end_deeper
\begin_layout Itemize
Normalization: centering and scaling the data to have empirical mean 0 and
 empirical standard deviation 1.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Z_{i}=\frac{X_{i}-\bar{X}}{S}$
\end_inset


\end_layout

\begin_layout Itemize
Normalized data have units equal to standard deviations of the original
 data.
\end_layout

\end_deeper
\begin_layout Itemize
Empirical covariance
\end_layout

\begin_deeper
\begin_layout Itemize
Consider a pair of data 
\begin_inset Formula $\left(X_{i},Y_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Their covariance is 
\begin_inset Formula $Cov\left(X,Y\right)=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)=\frac{1}{n-1}\left(\sum_{i=1}^{n}X_{i}Y_{i}-n\bar{X}\bar{Y}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Their correlation is defined as 
\begin_inset Formula $Cor\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{S_{x}S_{y}}$
\end_inset

, where 
\begin_inset Formula $S_{i}$
\end_inset

 is the estimate of the standard deviation for variable 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)=Cor\left(Y,X\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $-1\leq Cor\left(X,Y\right)\leq1$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)=1$
\end_inset

 and 
\begin_inset Formula $Cor\left(X,Y\right)=-1$
\end_inset

 only when the observations fall perfectly on a positively or negatively
 sloped line, respectively.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)$
\end_inset

 measures the strength of the linear relationship between the 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 data, with stronger relationships as 
\begin_inset Formula $Cor\left(X,Y\right)$
\end_inset

 goes to -1 or +1.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Cor\left(X,Y\right)$
\end_inset

 implies no linear relationship.
\end_layout

\end_deeper
\begin_layout Subsection*
Basic least squares
\end_layout

\begin_layout Itemize
Example using children's and parents' heights.
\end_layout

\begin_layout Itemize
Consider only the children's heights - how could one describe the 
\begin_inset Quotes eld
\end_inset

middle
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Itemize
One definition: let 
\begin_inset Formula $Y_{i}$
\end_inset

 be the height of child 
\begin_inset Formula $i$
\end_inset

, for 
\begin_inset Formula $i=1,..,n$
\end_inset

 , then define the middle as the value of 
\begin_inset Formula $\mu$
\end_inset

 that minimizes 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-\mu\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
This is the physical 
\begin_inset Quotes eld
\end_inset

center of mass
\begin_inset Quotes erd
\end_inset

 of the histogram.
\end_layout

\begin_layout Itemize
The end result is 
\begin_inset Formula $\mu=\bar{Y}$
\end_inset

, the mean (and we can do a proof of this).
\end_layout

\end_deeper
\begin_layout Itemize
Regression through the origin:
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose the 
\begin_inset Formula $X_{i}$
\end_inset

 are the parents' heights.
\end_layout

\begin_layout Itemize
Consider picking the slope 
\begin_inset Formula $\beta$
\end_inset

 that minimizes 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-X_{i}\beta\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta=\frac{\sum_{i=1}^{n}X_{i}Y_{i}}{\sum_{i=1}^{n}X_{i}^{2}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Linear least squares
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $Y_{i}$
\end_inset

 be the 
\begin_inset Formula $i$
\end_inset

th child's height and 
\begin_inset Formula $X_{i}$
\end_inset

 be the 
\begin_inset Formula $i$
\end_inset

th (average over the pair of) parents' heights.
\end_layout

\begin_layout Itemize
Consider finding the 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 line: child's height = 
\begin_inset Formula $\beta_{0}$
\end_inset

 + (parent's height)*
\begin_inset Formula $\beta_{1}$
\end_inset


\end_layout

\begin_layout Itemize
Use least squares:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\sum_{i=1}^{n}\left[Y_{i}-\left(\beta_{0}+\beta_{1}X_{i}\right)\right]^{2}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}=Cor\left(Y,X\right)\frac{S_{y}}{S_{x}}$
\end_inset

 (hat indicates that it is an estimator)
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}$
\end_inset

 has units of 
\begin_inset Formula $Y/X$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{0}=\bar{Y}-\hat{\beta}_{1}\bar{X}$
\end_inset

 (hat indicates that it is an estimator)
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{0}$
\end_inset

 has units of 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
The line passes through the point 
\begin_inset Formula $\left(\bar{X},\bar{Y}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The slope is the same as what you would get if you centered the data and
 did regression through the origin.
\end_layout

\begin_layout Itemize
If you normalized the data, the slope is 
\begin_inset Formula $Cor\left(Y,X\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection*
Regression to the mean
\end_layout

\begin_layout Itemize
Examples:
\end_layout

\begin_deeper
\begin_layout Itemize
Why are children of tall parents tall, but usually not as tall as their
 parents?
\end_layout

\begin_layout Itemize
Why do the best-performing athletes from last year usually not perform quite
 as well this year?
\end_layout

\end_deeper
\begin_layout Itemize
Imagine simulated pairs of random normal variables.
\end_layout

\begin_deeper
\begin_layout Itemize
The largest first ones would be the largest by chance, and the probability
 that there are smaller ones for the second simulation is high.
\end_layout

\begin_layout Itemize
In other words, 
\begin_inset Formula $P\left(Y<x|X=X\right)$
\end_inset

 gets bigger as 
\begin_inset Formula $x$
\end_inset

 heads into the very large values.
\end_layout

\begin_layout Itemize
Similarly, 
\begin_inset Formula $P\left(Y>x|X=x\right)$
\end_inset

 gets bigger as 
\begin_inset Formula $x$
\end_inset

 heads to very small values.
\end_layout

\end_deeper
\begin_layout Itemize
Suppose that we normalize 
\begin_inset Formula $X$
\end_inset

 (child's height) and 
\begin_inset Formula $Y$
\end_inset

 (parent's height) so that they both have mean 0 and variance 1.
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that our regression line will pass through 
\begin_inset Formula $\left(0,0\right)$
\end_inset

, the mean of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Itemize
The slope of the gression line is 
\begin_inset Formula $Cor\left(Y,X\right)$
\end_inset

, regardless of which variable is the outcome.
\end_layout

\end_deeper
\begin_layout Subsection*
Statistical linear regression models
\end_layout

\begin_layout Itemize
Basic regression model with additive Gaussian errors:
\end_layout

\begin_deeper
\begin_layout Itemize
Least squares is an estimation tool, how do we do inference?
\end_layout

\begin_layout Itemize
Consider developing a probabilistic model for linear regression: 
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\epsilon_{i}$
\end_inset

 are random Gaussian errors, which are assumed to be IID and drawn from
 
\begin_inset Formula $N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Note: 
\begin_inset Formula $E\left[Y_{i}|X_{i}=x_{i}\right]=\mu_{i}=\beta_{0}+\beta_{1}x_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Note: 
\begin_inset Formula $Var\left(Y_{i}|X_{i}=x_{i}\right)=\sigma^{2}$
\end_inset

.
 This is the variance around the regression line.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Interpreting regression coefficients:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\beta_{0}$
\end_inset

 is the expected value of the response when the predictor is 0.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[Y|X=0\right]=\beta_{0}$
\end_inset


\end_layout

\begin_layout Itemize
Note: this isn't always of interest, for example when 
\begin_inset Formula $X=0$
\end_inset

 is far outside the range of the data, or physically impossible (X is height,
 blood pressure, etc.).
\end_layout

\begin_layout Itemize
However, you can shift your 
\begin_inset Formula $X$
\end_inset

 values by some factor 
\begin_inset Formula $a$
\end_inset

; this will change the intercept, but not the slope.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}=\beta_{0}+a\beta_{1}+\beta_{1}\left(X_{i}-a\right)+\epsilon_{i}=\tilde{\beta}_{0}+\beta_{1}\left(X_{i}-a\right)+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Often, 
\begin_inset Formula $a$
\end_inset

 is set to 
\begin_inset Formula $\bar{X}$
\end_inset

 so that the intercept is interpreted as the expected response at the average
 
\begin_inset Formula $X$
\end_inset

 value.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
\begin_inset Formula $\beta_{1}$
\end_inset

 is the expected change in response for a 1 unit change in the predictor.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[Y|X=x+1\right]-E\left[Y|X=x\right]=\beta_{0}+\beta_{1}\left(x+1\right)-\left(\beta_{0}+\beta_{1}x\right)=\beta_{1}$
\end_inset


\end_layout

\begin_layout Itemize
Consider the impact of changing the units of 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}=\beta_{0}+\frac{\beta_{1}}{a}\left(X_{i}a\right)+\epsilon_{i}=\beta_{0}+\tilde{\beta}_{1}\left(X_{i}a\right)+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Multiplication of 
\begin_inset Formula $X$
\end_inset

 by a factor 
\begin_inset Formula $a$
\end_inset

 results in dividing the slope by a factor of 
\begin_inset Formula $a$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Subsection*
R code for regression
\end_layout

\begin_layout Itemize
Linear regression: 
\family typewriter
fit <- lm(outcome ~ predictor, data = name_of_data_frame)
\end_layout

\begin_deeper
\begin_layout Itemize
To just get the coefficients: 
\family typewriter
coef(fit)
\end_layout

\end_deeper
\begin_layout Itemize
Plotting a fit in ggplot: 
\family typewriter
g + geom_smooth(method=
\begin_inset Quotes eld
\end_inset

lm
\begin_inset Quotes erd
\end_inset

, color=
\begin_inset Quotes eld
\end_inset

black
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Itemize
If you want to do arithmetic operations inside the 
\family typewriter
lm
\family default
 function, use the 
\family typewriter
I()
\family default
 function:
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
fit2 <- lm(outcome ~ I(predictor - mean(predictor)), data=name_of_data_frame)
\end_layout

\end_deeper
\begin_layout Itemize
Using a fit to do a prediction: 
\family typewriter
predict(fit, newdata=data.frame(carat=newx))
\end_layout

\begin_layout Subsection*
Residuals
\end_layout

\begin_layout Itemize
Examples here are using the 
\family typewriter
diamond
\family default
 dataset from 
\family typewriter
UsingR
\family default
.
\end_layout

\begin_layout Itemize
Variation around a regression line is called 
\series bold
residual variation
\series default
.
\end_layout

\begin_deeper
\begin_layout Itemize
Residuals are just the difference between the actual data point values and
 those predicted by a regression line.
\end_layout

\end_deeper
\begin_layout Itemize
Model 
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}$
\end_inset

, where 
\begin_inset Formula $\epsilon_{i}\sim N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Observed outcome at predictor value 
\begin_inset Formula $X_{i}$
\end_inset

 is 
\begin_inset Formula $Y_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Predicted outcome is 
\begin_inset Formula $\hat{Y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}X_{i}$
\end_inset


\end_layout

\begin_layout Itemize
The residual is the difference between the observed and predicted outcomes:
 
\begin_inset Formula $e_{i}=Y_{i}-\hat{Y}_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Least squares minimizes 
\begin_inset Formula $\sum_{i=1}^{n}e_{i}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $e_{i}$
\end_inset

 can be thought of as estimates of the 
\begin_inset Formula $\epsilon_{i}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Properties of the residuals
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[e_{i}\right]=0$
\end_inset

.
\end_layout

\begin_layout Itemize
If an intercept is included, 
\begin_inset Formula $\sum_{i=1}^{n}e_{i}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
If a regressor variable, 
\begin_inset Formula $X_{i}$
\end_inset

, is included in the model, 
\begin_inset Formula $\sum_{i=1}^{n}e_{i}X_{i}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
Residuals can be thought of as the outcome 
\begin_inset Formula $Y$
\end_inset

, with the linear association of the predictor 
\begin_inset Formula $X$
\end_inset

 removed.
\end_layout

\begin_layout Itemize
There is a difference between 
\series bold
residual variation
\series default
 (variation after removing the predictor) from 
\series bold
systematic variation
\series default
 (variation explained by the regression model).
\end_layout

\end_deeper
\begin_layout Itemize
To get residuals in R, use 
\family typewriter
resid(fit)
\family default
, where 
\family typewriter
fit
\family default
 is the result of a regression function like 
\family typewriter
lm
\family default
.
\end_layout

\begin_layout Itemize
When you plot residuals, you shouldn't see any type of pattern, they should
 look mostly random.
 If there is a pattern, there may be some systematic error in your analysis
 or the assumptions you have made.
\end_layout

\begin_layout Itemize
Estimating residual variation:
\end_layout

\begin_deeper
\begin_layout Itemize
The maximum likelihood estimate of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}$
\end_inset

, the average squared residual.
\end_layout

\begin_layout Itemize
Most people use 
\begin_inset Formula $\hat{\sigma}^{2}=\frac{1}{n-2}\sum_{i=1}^{n}e_{i}^{2}$
\end_inset

 (you lose two degrees of freedom since you have an intercept and a slope
 in your fit).
\end_layout

\begin_deeper
\begin_layout Itemize
The 
\begin_inset Formula $n-2$
\end_inset

 instead of 
\begin_inset Formula $n$
\end_inset

 is so that 
\begin_inset Formula $E\left[\hat{\sigma}^{2}\right]=\sigma^{2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
\begin_inset Formula $R^{2}=\frac{\sum_{i=1}^{n}\left(\hat{Y}_{i}-\bar{Y}\right)^{2}}{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Percentage of total variability that is explained by the linear relationship
 with the predictor.
\end_layout

\begin_layout Itemize
\begin_inset Formula $0\leq R^{2}\leq1$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $R^{2}$
\end_inset

 is the sample correlation squared.
\end_layout

\begin_layout Itemize
\begin_inset Formula $R^{2}$
\end_inset

 can be a misleading summary of model fit.
\end_layout

\begin_deeper
\begin_layout Itemize
Deleting data can inflate 
\begin_inset Formula $R^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Adding terms to a regression model always increases 
\begin_inset Formula $R^{2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection*
Inference in regression
\end_layout

\begin_layout Itemize
Our model is 
\begin_inset Formula $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}$
\end_inset

, where 
\begin_inset Formula $\epsilon\sim N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
We assume that the true model is known.
\end_layout

\end_deeper
\begin_layout Itemize
Statistics like 
\begin_inset Formula $\frac{\hat{\theta}-\theta}{\hat{\sigma}_{\theta}}$
\end_inset

 have the following properties:
\end_layout

\begin_deeper
\begin_layout Itemize
Normally distributed and have a Student's T distribution if the estimated
 variance is replaced with a sample estimate (under normality assumptions).
\end_layout

\begin_layout Itemize
Can be used to test 
\begin_inset Formula $H_{0}:\theta=\theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{a}:\theta>,<,\neq\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Can be used to create a confidence interval for 
\begin_inset Formula $\theta$
\end_inset

 via 
\begin_inset Formula $\hat{\theta}\pm Q_{1-\alpha/2}\hat{\sigma}_{\theta}$
\end_inset

, where 
\begin_inset Formula $Q_{1-\alpha/2}$
\end_inset

 is the relevant quantile from either a normal or a T distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
I think you use 
\begin_inset Formula $n-2$
\end_inset

 degrees of freqedom for a T distribution in this case.
 Not totally sure, though.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
In the case of regression with IID sampling assumption and normal errors,
 our inferences will look similar to what we covered in the statistical
 inference class.
\end_layout

\begin_layout Itemize
Variance of the regression slope:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma_{\hat{\beta}_{1}}^{2}=Var\left(\hat{\beta}_{1}\right)=\sigma^{2}/\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
Variance of the intercept:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\sigma_{\hat{\beta}_{0}}^{2}=Var\left(\beta_{0}\right)=\left(\frac{1}{n}+\frac{\bar{X}^{2}}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}\right)\sigma^{2}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
In practice, 
\begin_inset Formula $\sigma$
\end_inset

 can be replaced by its estimate in the above two equations (
\begin_inset Formula $\frac{\sum_{i=1}^{n}e_{i}^{2}}{n-2}$
\end_inset

).
\end_layout

\begin_layout Itemize
It's probably not surprising that under IID Gaussian errors, 
\begin_inset Formula $\frac{\hat{\beta}_{j}-\beta_{j}}{\hat{\sigma}_{\hat{\beta}_{j}}}$
\end_inset

 follows a T distribution with 
\begin_inset Formula $n-2$
\end_inset

 degrees of freedom and a normal distribution for large 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This can be used to create confidence intervals and perform hypothesis tests.
\end_layout

\end_deeper
\begin_layout Itemize
Prediction of outcomes:
\end_layout

\begin_deeper
\begin_layout Itemize
Consider predicting 
\begin_inset Formula $Y$
\end_inset

 at a value 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Itemize
The obvious estimate for a prediction at point 
\begin_inset Formula $x_{0}$
\end_inset

 is 
\begin_inset Formula $\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
A standard error is needed to create a prediction interval.
\end_layout

\begin_layout Itemize
There is a distinction between intervals for the regression line at a particular
 point 
\begin_inset Formula $x_{0}$
\end_inset

 and intervals for the prediction of a 
\begin_inset Formula $y$
\end_inset

 value at a point 
\begin_inset Formula $x_{0}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Line at 
\begin_inset Formula $x_{0}$
\end_inset

 standard error: 
\begin_inset Formula $\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\left(x_{0}-\bar{X}\right)^{2}}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}}$
\end_inset


\end_layout

\begin_layout Itemize
Prediction interval standard error at 
\begin_inset Formula $x_{0}$
\end_inset

: 
\begin_inset Formula $\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{X}\right)^{2}}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Code example
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

library(UsingR); data(diamond)
\end_layout

\begin_layout Plain Layout

y <- diamond$price; x <- diamond$carat; n <- length(y)
\end_layout

\begin_layout Plain Layout

beta1 <- cor(x,y)*sd(y)/sd(x);
\end_layout

\begin_layout Plain Layout

beta0 <- mean(y) - beta1*mean(x);
\end_layout

\begin_layout Plain Layout

e <- y - beta0 - beta1*x;
\end_layout

\begin_layout Plain Layout

sigma <- sqrt(sum(e^2)/(n-2))
\end_layout

\begin_layout Plain Layout

ssx <- sum((x-mean(x))^2) ## this is also equal to var(x)*(n-1)
\end_layout

\begin_layout Plain Layout

seBeta0 <- sqrt(1/n + mean(x)^2/ssx)*sigma
\end_layout

\begin_layout Plain Layout

seBeta1 <- sigma/sqrt(ssx)
\end_layout

\begin_layout Plain Layout

tBeta0 <- beta0/seBeta0;
\end_layout

\begin_layout Plain Layout

tBeta1 <- beta1/seBeta1;
\end_layout

\begin_layout Plain Layout

pBeta0 <- 2*pt(abs(tBeta0), df=n-2, lower.tail=FALSE)
\end_layout

\begin_layout Plain Layout

pBeta1 <- 2*pt(abs(tBeta1), df=n-2, lower.tail=FALSE)
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
Multivariable regression
\end_layout

\begin_layout Itemize
Sometimes there are several variables which may affect a predictor, or hidden
 variables affecting the outcome.
\end_layout

\begin_layout Itemize
Multivariable analyses attempt to account for other variables that may explain
 a relationship.
\end_layout

\begin_layout Itemize
Example:
\end_layout

\begin_deeper
\begin_layout Itemize
An insurance company is interested in how last year's claims can predict
 a person's time in the hospital this year.
\end_layout

\begin_layout Itemize
They want to use an enormous amount of data contained in claims to predict
 a single number.
\end_layout

\begin_layout Itemize
Simple linear regression is not equipped to handle more than one predictor.
\end_layout

\begin_layout Itemize
How can one generalize simple linear regression to incorporate lots of regressor
s for the purpose of prediction?
\end_layout

\begin_layout Itemize
What are the consequences of adding lots of regressors?
\end_layout

\begin_deeper
\begin_layout Itemize
There must be consequences to adding variables that are unrelated to the
 outcome.
\end_layout

\begin_layout Itemize
There must be consequences to omitting variables that are related to the
 outcome.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The linear model for multivariate regression:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{1}X_{1i}+\beta_{2}X_{2i}+...+\beta_{p}X_{pi}+\epsilon_{i}=\sum_{k=1}^{p}X_{ki}\beta_{k}+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Here, 
\begin_inset Formula $X_{1i}$
\end_inset

 is typically 1 so that an intercept is included.
\end_layout

\begin_layout Itemize
Least squares (and hence ML estimates, under IID Gaussianity of the errors)
 minimizes 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-\sum_{k=1}^{p}X_{ki}\beta_{k}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Note: the important linearity here is the linearity in the coefficients!
\end_layout

\begin_deeper
\begin_layout Itemize
Example: 
\begin_inset Formula $Y_{i}=\beta_{1}X_{1i}^{2}+...\beta_{p}X_{pi}^{2}+\epsilon_{i}$
\end_inset

 is still a linear model, we've just squared the elements of the predictor
 variables.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
How to get estimates
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that the least squares estimate for regression through the origin,
 
\begin_inset Formula $E\left[Y_{i}\right]=X_{1i}\beta_{1}$
\end_inset

 was 
\begin_inset Formula $\sum_{i}X_{i}Y_{i}/\sum_{i}X_{i}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Let's consider two regressors, 
\begin_inset Formula $E\left[Y_{i}\right]=X_{1i}\beta_{1}+X_{2i}\beta_{2}=\mu_{i}$
\end_inset

 .
\end_layout

\begin_deeper
\begin_layout Itemize
Recall that if 
\begin_inset Formula $\hat{\mu}_{i}$
\end_inset

 satisfies 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-\hat{\mu}_{i}\right)\left(\hat{\mu}_{i}-\mu_{i}\right)=0$
\end_inset

, then we've found the least squares estimates.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}e_{i,Y|X_{2}}e_{i,X_{1}|X_{2}}}{\sum_{i=1}^{n}e_{i,x_{1}|X_{2}}^{2}}$
\end_inset

: the regression esimate for 
\begin_inset Formula $\beta_{1}$
\end_inset

 is the regression through the origin estimate, having regressed 
\begin_inset Formula $X_{2}$
\end_inset

 out of both the response and the predictor.
\end_layout

\begin_layout Itemize
Similarly, the regression estimate for 
\begin_inset Formula $\beta_{2}$
\end_inset

 is the regression through the origin estimate, having regressed 
\begin_inset Formula $X_{1}$
\end_inset

 out of both the response and the predictor.
\end_layout

\end_deeper
\begin_layout Itemize
Generally, multivariate regression estimates are exactly those having removed
 the linear relationship of the other variables from both the regressor
 and the response.
\end_layout

\end_deeper
\begin_layout Itemize
Example with two variables, simple linear regression
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Y_{i}=\beta_{1}X_{1i}+\beta_{2}X_{2i}$
\end_inset

, where 
\begin_inset Formula $X_{2i}=1$
\end_inset

 is an intercept term.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $\frac{\sum_{j}X_{2j}X_{1j}}{\sum_{j}X_{2j}^{2}}X_{2i}=\frac{\sum_{j}X_{1j}}{n}=\bar{X}_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
Thus, 
\begin_inset Formula $e_{i,X_{1}|X_{2}}=X_{1i}-\bar{X}_{1}$
\end_inset

 and 
\begin_inset Formula $e_{i,Y|X_{2}}=Y_{i}-\bar{Y}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}e_{i,Y|X_{2}}e_{i,X_{1}|X_{2}}}{\sum_{i=1}^{n}e_{i,X_{1}|X_{2}}^{2}}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}=Cor\left(X,Y\right)\frac{Sd\left(Y\right)}{Sd\left(X\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Extending to the general case:
\end_layout

\begin_deeper
\begin_layout Itemize
Least square solution to minimize: 
\begin_inset Formula $\sum_{i=1}^{n}\left(Y_{i}-X_{1i}\beta_{1}-...-X_{pi}\beta_{p}\right)^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Solving this solution yields the least squares estimates.
\end_layout

\begin_layout Itemize
Obtaining a good, fast, and general solution usually requires linear algebra.
\end_layout

\end_deeper
\begin_layout Itemize
The least squares estimate for the coefficient of a multivariate regression
 model is exactly regression through the origin with the linear relationship
 with the other regressors removed from both the regressor and outcome by
 taking residuals.
\end_layout

\begin_layout Itemize
In this sense, multivariate regression 
\begin_inset Quotes eld
\end_inset

adjusts
\begin_inset Quotes erd
\end_inset

 a coefficient for the linear impact of the other variables.
\end_layout

\end_deeper
\begin_layout Itemize
Interpretation of the coefficients
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E\left[Y|X_{1}=x_{1},...,X_{p}=x_{p}\right]=\sum_{k=1}^{p}x_{k}\beta_{k}$
\end_inset


\end_layout

\begin_layout Itemize
What if one variable 
\begin_inset Formula $x_{1}$
\end_inset

 is incremented by one?
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[Y|X_{1}=x_{1}+1,...,X_{p}=x_{p}\right]-E\left[Y|X_{1}=x_{1},...,X_{p}=x_{p}\right]=\beta_{1}$
\end_inset


\end_layout

\begin_layout Itemize
So the intepretation of a multivariate regression coefficient is the expected
 change in the response per unit change in the regressor, holding all of
 the other regressors fixed.
\end_layout

\end_deeper
\begin_layout Itemize
Fitted values, residuals, and residual variation - all of our simple linear
 regression quantities can be extended to multivariate linear models.
\end_layout

\begin_deeper
\begin_layout Itemize
Model: 
\begin_inset Formula $Y_{i}=\sum_{k=1}^{p}X_{ik}\beta_{k}+\epsilon_{i}$
\end_inset

 where 
\begin_inset Formula $\epsilon_{i}\sim N\left(0,\sigma^{2}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Fitted responses: 
\begin_inset Formula $\hat{Y}_{i}=\sum_{k=1}^{p}X_{ik}\hat{\beta}_{k}$
\end_inset


\end_layout

\begin_layout Itemize
Residuals: 
\begin_inset Formula $e_{i}=Y_{i}-\hat{Y}_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Variance estimate: 
\begin_inset Formula $\hat{\sigma}^{2}=\frac{1}{n-p}\sum_{i=1}^{n}e_{i}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
To get predicted responses at new values 
\begin_inset Formula $x_{1},...,x_{p}$
\end_inset

, simply plug these values into the linear model 
\begin_inset Formula $\sum_{k=1}^{p}x_{k}\hat{\beta}_{k}$
\end_inset

.
\end_layout

\begin_layout Itemize
Our coefficients have standard errors 
\begin_inset Formula $\hat{\sigma}_{\hat{\beta}_{k}}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{\hat{\beta}_{k}-\beta_{k}}{\hat{\sigma}_{\hat{\beta}_{k}}}$
\end_inset

 follows a T distribution with 
\begin_inset Formula $n-p$
\end_inset

 degrees of freedom.
\end_layout

\end_deeper
\begin_layout Itemize
Predicted responses have standard errors and we can calculate predicted
 and expected response intervals.
\end_layout

\end_deeper
\begin_layout Itemize
Linear models are the single most important applied statistical and machine
 learning technique 
\series bold
by far
\series default
.
\end_layout

\begin_layout Itemize
Some amazing things that you can accomplish with linear models:
\end_layout

\begin_deeper
\begin_layout Itemize
Decompose a signal into its harmonics.
\end_layout

\begin_layout Itemize
Flexibly fit complicated functions.
\end_layout

\begin_layout Itemize
Fit factor variables as predictors.
\end_layout

\begin_layout Itemize
Uncover complex multivariate relationship with the response.
\end_layout

\begin_layout Itemize
Build accurate prediction models.
\end_layout

\end_deeper
\end_body
\end_document
